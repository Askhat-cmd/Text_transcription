–û—Ç–ª–∏—á–Ω–∞—è –∏–¥–µ—è! –î–∞–≤–∞–π—Ç–µ —Å–ø—Ä–æ–µ–∫—Ç–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º—É **semantic memory** –¥–ª—è –±–æ—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª–∏—Ç –µ–º—É –∏–º–µ—Ç—å –æ–±—â–µ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ —Ä–∞–∑–≥–æ–≤–æ—Ä–µ –±–µ–∑ –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

***

# –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Semantic Memory –¥–ª—è Bot Psychologist

## üéØ –¶–µ–ª–∏ –∏ –∑–∞–¥–∞—á–∏

### **–ß—Ç–æ —Ö–æ—Ç–∏–º –¥–æ–±–∏—Ç—å—Å—è:**
1. **–î–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å** ‚Äî –±–æ—Ç –ø–æ–º–Ω–∏—Ç –æ —á–µ–º –≥–æ–≤–æ—Ä–∏–ª–∏ 50-100 —Ö–æ–¥–æ–≤ –Ω–∞–∑–∞–¥
2. **–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫** ‚Äî –Ω–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–æ—à–ª—ã–µ –æ–±–º–µ–Ω—ã –ø–æ —Å–º—ã—Å–ª—É, –∞ –Ω–µ –ø–æ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏–∏
3. **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å** ‚Äî –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–æ–∫–µ–Ω–∞–º–∏
4. **Summary** ‚Äî –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –≤—Å–µ–≥–æ –¥–∏–∞–ª–æ–≥–∞ –¥–ª—è –æ–±—â–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è

***

## üìä –ì–∏–±—Ä–∏–¥–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–∞–º—è—Ç–∏

### **–î–≤—É—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         SEMANTIC MEMORY SYSTEM                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  –£–†–û–í–ï–ù–¨ 1: Short-term Memory          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  (–ü–æ—Å–ª–µ–¥–Ω–∏–µ 3-5 —Ö–æ–¥–æ–≤)                 ‚îÇ    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç                      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ –•—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø–æ—Ä—è–¥–æ–∫              ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ ~2000 —Å–∏–º–≤–æ–ª–æ–≤                       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ –í–°–ï–ì–î–ê –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –≤ –ø—Ä–æ–º–ø—Ç          ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  –£–†–û–í–ï–ù–¨ 2: Long-term Semantic Memory  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  (–í—Å–µ –ø—Ä–æ—à–ª—ã–µ —Ö–æ–¥—ã)                    ‚îÇ    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ –í–µ–∫—Ç–æ—Ä–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏                 ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫                  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ –¢–æ–ª—å–∫–æ –†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –∫ —Ç–µ–∫—É—â–µ–º—É –≤–æ–ø—Ä–æ—Å—É‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ ~1000 —Å–∏–º–≤–æ–ª–æ–≤ (—Ç–æ–ø-3 –Ω–∞—Ö–æ–¥–∫–∏)       ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  –£–†–û–í–ï–ù–¨ 3: Conversation Summary       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  (–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–µ)                 ‚îÇ    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –≤—Å–µ–≥–æ –¥–∏–∞–ª–æ–≥–∞         ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ –û–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –∫–∞–∂–¥—ã–µ 5-10 —Ö–æ–¥–æ–≤        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ ~300-500 —Å–∏–º–≤–æ–ª–æ–≤                    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã, –ø—Ä–æ–≥—Ä–µ—Å—Å, –∏–Ω—Å–∞–π—Ç—ã     ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

–ò–¢–û–ì–û –≤ –ø—Ä–æ–º–ø—Ç–µ: 2000 + 1000 + 500 = 3500 —Å–∏–º–≤–æ–ª–æ–≤
```

***

## üèóÔ∏è –î–µ—Ç–∞–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

### **1. Short-term Memory (—Ç–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)**

**–ß—Ç–æ –µ—Å—Ç—å —Å–µ–π—á–∞—Å:**
```python
memory.get_context_for_llm(n=3, max_chars=2000)
```

**–û—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å** ‚Äî —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ –¥–ª—è –Ω–µ–¥–∞–≤–Ω–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

***

### **2. Long-term Semantic Memory (–ù–û–í–û–ï)**

#### **–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:**

**–ê. –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:**
- –ü—Ä–∏ –∫–∞–∂–¥–æ–º –æ–±–º–µ–Ω–µ —Å–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–∞ –∏ –æ—Ç–≤–µ—Ç–∞
- –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å: `sentence-transformers` (–ª–æ–∫–∞–ª—å–Ω–æ) –∏–ª–∏ OpenAI embeddings
- –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª —Ä—è–¥–æ–º —Å –∏—Å—Ç–æ—Ä–∏–µ–π

**–ë. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫:**
- –ö–æ–≥–¥–∞ –ø—Ä–∏—Ö–æ–¥–∏—Ç –Ω–æ–≤—ã–π –≤–æ–ø—Ä–æ—Å, –Ω–∞—Ö–æ–¥–∏–º —Ç–æ–ø-3 –ø–æ—Ö–æ–∂–∏—Ö –ø—Ä–æ—à–ª—ã—Ö –æ–±–º–µ–Ω–∞
- –ö—Ä–∏—Ç–µ—Ä–∏–π: –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ > 0.7
- –î–æ–±–∞–≤–ª—è–µ–º –≤ –ø—Ä–æ–º–ø—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã

**–í. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è:**
- –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞—é—Ç—Å—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ (–Ω–µ –±–ª–æ–∫–∏—Ä—É—é—Ç –æ—Ç–≤–µ—Ç)
- –ö—ç—à–∏—Ä—É—é—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–æ
- –ù–µ –Ω—É–∂–Ω–∞ –≤–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î ‚Äî –ø—Ä–æ—Å—Ç–æ –º–∞—Å—Å–∏–≤ –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –ø–∞–º—è—Ç–∏

***

### **3. Conversation Summary (–ù–û–í–û–ï)**

#### **–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑—é–º–µ:**

**–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:**
- –ö–∞–∂–¥—ã–µ 5-10 —Ö–æ–¥–æ–≤ LLM –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –¥–∏–∞–ª–æ–≥–∞
- –†–µ–∑—é–º–µ –∑–∞–º–µ–Ω—è–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–µ (–Ω–µ –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ—Ç—Å—è)
- –°–æ–¥–µ—Ä–∂–∏—Ç: –∫–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã, –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –≤–∞–∂–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã

**–ü—Ä–∏–º–µ—Ä —Ä–µ–∑—é–º–µ:**
```
–†–ï–ó–Æ–ú–ï –î–ò–ê–õ–û–ì–ê:
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏–∑—É—á–∞–µ—Ç –ø—Ä–∞–∫—Ç–∏–∫—É –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏—è. –ù–∞—á–∞–ª —Å –±–∞–∑–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –æ –ø—Ä–∏—Ä–æ–¥–µ 
–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç–∏, –∑–∞—Ç–µ–º –ø–µ—Ä–µ—à–µ–ª –∫ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è–º. –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä–µ—Å ‚Äî 
—Ä–∞–±–æ—Ç–∞ —Å–æ —Å—Ç—Ä–µ—Å—Å–æ–º —á–µ—Ä–µ–∑ –¥—ã—Ö–∞–Ω–∏–µ. –ò—Å–ø—ã—Ç—ã–≤–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å —Ä–µ–≥—É–ª—è—Ä–Ω–æ—Å—Ç—å—é –ø—Ä–∞–∫—Ç–∏–∫–∏. 
–ü–æ–ª—É—á–∏–ª –¥–≤–∞ –≤–∞–∂–Ω—ã—Ö –∏–Ω—Å–∞–π—Ç–∞ –æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–∏ –∑–∞ –º—ã—Å–ª—è–º–∏.
```

***

## üíª –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

### **–§–∞–π–ª–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:**

```python
# bot_agent/semantic_memory.py - –ù–û–í–´–ô –§–ê–ô–õ

from typing import List, Dict, Tuple, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
import json
from pathlib import Path

class SemanticMemory:
    """
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–æ—à–ª—ã—Ö –æ–±–º–µ–Ω–æ–≤.
    """
    
    def __init__(self, user_id: str):
        self.user_id = user_id
        self.embeddings: List[np.ndarray] = []
        self.turns_data: List[Dict] = []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–≥–∫—É—é multilingual –º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        self.cache_dir = config.CACHE_DIR / "semantic_memory"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        self.embeddings_file = self.cache_dir / f"{user_id}_embeddings.npz"
        self.metadata_file = self.cache_dir / f"{user_id}_metadata.json"
    
    def add_turn_embedding(self, turn: ConversationTurn, turn_index: int):
        """
        –°–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ö–æ–¥–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å.
        
        Args:
            turn: –•–æ–¥ –¥–∏–∞–ª–æ–≥–∞
            turn_index: –ò–Ω–¥–µ–∫—Å —Ö–æ–¥–∞
        """
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –≤–æ–ø—Ä–æ—Å –∏ –æ—Ç–≤–µ—Ç –¥–ª—è –±–æ–ª–µ–µ –±–æ–≥–∞—Ç–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        text = f"{turn.user_input} {turn.bot_response or ''}"
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
        embedding = self.model.encode(text, show_progress_bar=False)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        self.embeddings.append(embedding)
        self.turns_data.append({
            "turn_index": turn_index,
            "user_input": turn.user_input,
            "bot_response": turn.bot_response[:200] if turn.bot_response else "",
            "user_state": turn.user_state,
            "concepts": turn.concepts,
            "timestamp": turn.timestamp
        })
        
        self._save_to_disk()
    
    def search_similar_turns(
        self, 
        query: str, 
        top_k: int = 3,
        min_similarity: float = 0.7
    ) -> List[Tuple[Dict, float]]:
        """
        –ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ –ø—Ä–æ—à–ª—ã–µ –æ–±–º–µ–Ω—ã.
        
        Args:
            query: –¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            min_similarity: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (0-1)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (turn_data, similarity_score)
        """
        if not self.embeddings:
            return []
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—É—â–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
        query_embedding = self.model.encode(query, show_progress_bar=False)
        
        # –°—á–∏—Ç–∞–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å–æ –≤—Å–µ–º–∏ –ø—Ä–æ—à–ª—ã–º–∏ —Ö–æ–¥–∞–º–∏
        similarities = []
        for i, emb in enumerate(self.embeddings):
            similarity = self._cosine_similarity(query_embedding, emb)
            if similarity >= min_similarity:
                similarities.append((self.turns_data[i], float(similarity)))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        return similarities[:top_k]
    
    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        """–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))
    
    def get_context_for_llm(self, query: str, max_chars: int = 1000) -> str:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–æ—à–ª—ã—Ö –æ–±–º–µ–Ω–æ–≤ –¥–ª—è LLM.
        
        Args:
            query: –¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å
            max_chars: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
        """
        similar = self.search_similar_turns(query, top_k=3, min_similarity=0.7)
        
        if not similar:
            return ""
        
        context = "–†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –ü–†–û–®–õ–´–ï –û–ë–ú–ï–ù–´:\\n\\n"
        current_len = len(context)
        
        for turn_data, score in similar:
            entry = (
                f"[–°—Ö–æ–¥—Å—Ç–≤–æ: {score:.2f}] –û–±–º–µ–Ω #{turn_data['turn_index']}:\\n"
                f"  –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {turn_data['user_input']}\\n"
                f"  –ë–æ—Ç: {turn_data['bot_response']}\\n"
            )
            if turn_data['user_state']:
                entry += f"  –°–æ—Å—Ç–æ—è–Ω–∏–µ: {turn_data['user_state']}\\n"
            entry += "\\n"
            
            if current_len + len(entry) > max_chars:
                break
            
            context += entry
            current_len += len(entry)
        
        return context
    
    def _save_to_disk(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
        if self.embeddings:
            np.savez_compressed(
                self.embeddings_file,
                embeddings=np.array(self.embeddings)
            )
        
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.turns_data, f, ensure_ascii=False, indent=2)
    
    def load_from_disk(self) -> bool:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –¥–∏—Å–∫–∞"""
        if not self.embeddings_file.exists():
            return False
        
        try:
            data = np.load(self.embeddings_file)
            self.embeddings = list(data['embeddings'])
            
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                self.turns_data = json.load(f)
            
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            return False
```

***

### **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ conversation_memory.py:**

```python
# bot_agent/conversation_memory.py - –î–û–ü–û–õ–ù–ï–ù–ò–Ø

from .semantic_memory import SemanticMemory

class ConversationMemory:
    def __init__(self, user_id: str = "default"):
        # ... existing code ...
        
        # –ù–û–í–û–ï: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å
        self.semantic_memory = SemanticMemory(user_id)
        self.semantic_memory.load_from_disk()
        
        # –ù–û–í–û–ï: –†–µ–∑—é–º–µ –¥–∏–∞–ª–æ–≥–∞
        self.summary: Optional[str] = None
        self.summary_updated_at: Optional[int] = None  # turn index
    
    def add_turn(self, ...):
        """–î–æ–±–∞–≤–∏—Ç—å —Ö–æ–¥ –≤ –∏—Å—Ç–æ—Ä–∏—é"""
        turn = ConversationTurn(...)
        self.turns.append(turn)
        
        # ... existing code ...
        
        # –ù–û–í–û–ï: –î–æ–±–∞–≤–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
        turn_index = len(self.turns)
        self.semantic_memory.add_turn_embedding(turn, turn_index)
        
        # –ù–û–í–û–ï: –û–±–Ω–æ–≤–∏—Ç—å —Ä–µ–∑—é–º–µ –∫–∞–∂–¥—ã–µ 5 —Ö–æ–¥–æ–≤
        if turn_index % 5 == 0:
            self._update_summary()
        
        self.save_to_disk()
        return turn
    
    def get_full_context_for_llm(self, current_question: str) -> Dict[str, str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM:
        - Short-term memory (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 3-5 —Ö–æ–¥–æ–≤)
        - Semantic memory (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–æ—à–ª—ã–µ –æ–±–º–µ–Ω—ã)
        - Summary (–æ–±—â–µ–µ —Ä–µ–∑—é–º–µ –¥–∏–∞–ª–æ–≥–∞)
        
        Returns:
            Dict —Å —Ç—Ä–µ–º—è –≤–∏–¥–∞–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        """
        return {
            "short_term": self.get_context_for_llm(n=3, max_chars=2000),
            "semantic": self.semantic_memory.get_context_for_llm(
                current_question, max_chars=1000
            ),
            "summary": self.summary or ""
        }
    
    def _update_summary(self):
        """
        –û–±–Ω–æ–≤–∏—Ç—å —Ä–µ–∑—é–º–µ –¥–∏–∞–ª–æ–≥–∞ —á–µ—Ä–µ–∑ LLM.
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –∫–∞–∂–¥—ã–µ 5 —Ö–æ–¥–æ–≤.
        """
        if len(self.turns) < 5:
            return
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Ö–æ–¥–æ–≤ –¥–ª—è —Ä–µ–∑—é–º–µ
        recent_turns = self.turns[-10:]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM
        turns_text = ""
        for i, turn in enumerate(recent_turns, 1):
            turns_text += f"\n–•–æ–¥ {i}:\n"
            turns_text += f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {turn.user_input}\n"
            turns_text += f"–ë–æ—Ç: {turn.bot_response[:150]}...\n"
        
        summary_prompt = f"""
–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –¥–∏–∞–ª–æ–≥–∞ (–º–∞–∫—Å–∏–º—É–º 500 —Å–∏–º–≤–æ–ª–æ–≤).
–í–∫–ª—é—á–∏:
- –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—Å—É–∂–¥–∞–ª–∏—Å—å
- –ü—Ä–æ–≥—Ä–µ—Å—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏
- –í–∞–∂–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã –∏–ª–∏ –ø—Ä–æ—Ä—ã–≤—ã
- –¢–µ–∫—É—â–∏–π —Ñ–æ–∫—É—Å –¥–∏–∞–ª–æ–≥–∞

–î–ò–ê–õ–û–ì:
{turns_text}

–†–ï–ó–Æ–ú–ï (–∫—Ä–∞—Ç–∫–æ, –ø–æ-—Ä—É—Å—Å–∫–∏):
"""
        
        try:
            from .llm_answerer import get_llm_answerer
            answerer = get_llm_answerer()
            
            # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ –¥–ª—è —Ä–µ–∑—é–º–µ
            response = answerer.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": summary_prompt}],
                temperature=0.3,
                max_tokens=200
            )
            
            self.summary = response.choices[0].message.content.strip()
            self.summary_updated_at = len(self.turns)
            
            logger.info(f"‚úÖ –†–µ–∑—é–º–µ –æ–±–Ω–æ–≤–ª–µ–Ω–æ (—Ö–æ–¥ #{self.summary_updated_at})")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ä–µ–∑—é–º–µ: {e}")
```

***

### **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ answer –º–æ–¥—É–ª–∏:**

```python
# bot_agent/answer_basic.py (–∏ –æ—Å—Ç–∞–ª—å–Ω—ã–µ answer_*.py)

def answer_question(question: str, user_id: str = "default", **kwargs):
    """–û—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å —É—á–µ—Ç–æ–º –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –ø–∞–º—è—Ç–∏"""
    
    # 1. –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–∞–º—è—Ç—å
    memory = get_conversation_memory(user_id)
    
    # 2. –ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
    memory_context = memory.get_full_context_for_llm(question)
    
    # 3. –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –±–ª–æ–∫–æ–≤ (–∫–∞–∫ –æ–±—ã—á–Ω–æ)
    retriever = get_retriever()
    top_blocks = retriever.retrieve(question, top_k=5)
    
    # 4. –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–º–ø—Ç —Å –¢–†–ï–ú–Ø —Ç–∏–ø–∞–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    full_context = ""
    
    # A. Summary (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if memory_context["summary"]:
        full_context += f"""
–ö–†–ê–¢–ö–û–ï –†–ï–ó–Æ–ú–ï –î–ò–ê–õ–û–ì–ê:
{memory_context["summary"]}

---

"""
    
    # B. Semantic memory (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–æ—à–ª—ã–µ –æ–±–º–µ–Ω—ã)
    if memory_context["semantic"]:
        full_context += memory_context["semantic"] + "\n---\n\n"
    
    # C. Short-term memory (–ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ö–æ–¥—ã)
    if memory_context["short_term"]:
        full_context += memory_context["short_term"] + "\n---\n\n"
    
    # D. –ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    full_context += "–ú–ê–¢–ï–†–ò–ê–õ –ò–ó –õ–ï–ö–¶–ò–ô:\n\n"
    for block in top_blocks:
        full_context += format_block(block) + "\n\n"
    
    # 5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    result = answerer.generate_answer(
        user_question=question,
        blocks=top_blocks,
        conversation_history=full_context  # –ü–µ—Ä–µ–¥–∞–µ–º –≤–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
    )
    
    # 6. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ø–∞–º—è—Ç—å
    memory.add_turn(
        user_input=question,
        bot_response=result["answer"],
        blocks_used=len(top_blocks),
        concepts=[b.title for b in top_blocks]
    )
    
    return result
```

***

## üìè –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

### **–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è:**

| –î–ª–∏–Ω–∞ –¥–∏–∞–ª–æ–≥–∞ | Short-term | Semantic | Summary | –ò—Ç–æ–≥–æ |
|---------------|------------|----------|---------|-------|
| 1-5 —Ö–æ–¥–æ–≤ | –í–°–ï —Ö–æ–¥—ã | ‚Äî | ‚Äî | ~1000 |
| 6-20 —Ö–æ–¥–æ–≤ | 3 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö | —Ç–æ–ø-2 | ‚Äî | ~2500 |
| 21-50 —Ö–æ–¥–æ–≤ | 3 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö | —Ç–æ–ø-3 | –î–∞ | ~3500 |
| 50+ —Ö–æ–¥–æ–≤ | 3 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö | —Ç–æ–ø-3 | –î–∞ | ~3500 |

```python
def get_adaptive_context(self, question: str) -> Dict[str, str]:
    """–ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–ª–∏–Ω—ã –¥–∏–∞–ª–æ–≥–∞"""
    total_turns = len(self.turns)
    
    if total_turns <= 5:
        # –ö–æ—Ä–æ—Ç–∫–∏–π –¥–∏–∞–ª–æ–≥ ‚Äî –±–µ—Ä–µ–º –≤—Å–µ —Ö–æ–¥—ã
        return {
            "short_term": self.get_context_for_llm(n=total_turns),
            "semantic": "",
            "summary": ""
        }
    
    elif total_turns <= 20:
        # –°—Ä–µ–¥–Ω–∏–π –¥–∏–∞–ª–æ–≥ ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º semantic
        return {
            "short_term": self.get_context_for_llm(n=3),
            "semantic": self.semantic_memory.get_context_for_llm(
                question, max_chars=800
            ),
            "summary": ""
        }
    
    else:
        # –î–ª–∏–Ω–Ω—ã–π –¥–∏–∞–ª–æ–≥ ‚Äî full stack
        return {
            "short_term": self.get_context_for_llm(n=3),
            "semantic": self.semantic_memory.get_context_for_llm(
                question, max_chars=1000
            ),
            "summary": self.summary or ""
        }
```

***

## ‚ö° –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

### **1. Lazy Loading —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:**

```python
class SemanticMemory:
    def __init__(self, user_id: str):
        self.model = None  # –ù–µ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å—Ä–∞–∑—É
        self._model_loaded = False
    
    def _ensure_model_loaded(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏"""
        if not self._model_loaded:
            self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            self._model_loaded = True
```

### **2. Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞:**

```python
def rebuild_all_embeddings(self):
    """–ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ batch'–µ–º (–±—ã—Å—Ç—Ä–µ–µ)"""
    if not self.turns:
        return
    
    texts = [
        f"{turn.user_input} {turn.bot_response or ''}"
        for turn in self.turns
    ]
    
    # Batch encoding –±—ã—Å—Ç—Ä–µ–µ —á–µ–º –ø–æ –æ–¥–Ω–æ–º—É
    self.embeddings = self.model.encode(
        texts,
        batch_size=32,
        show_progress_bar=True
    )
```

### **3. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏:**

```python
# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Å–∏–Ω–≥–ª—Ç–æ–Ω –º–æ–¥–µ–ª–∏
_embedding_model = None

def get_embedding_model():
    global _embedding_model
    if _embedding_model is None:
        _embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    return _embedding_model
```

***

## üéØ –ü—Ä–∏–º–µ—Ä –∏—Ç–æ–≥–æ–≤–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞

```
–ö–†–ê–¢–ö–û–ï –†–ï–ó–Æ–ú–ï –î–ò–ê–õ–û–ì–ê:
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏–∑—É—á–∞–µ—Ç –ø—Ä–∞–∫—Ç–∏–∫—É –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏—è. –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä–µ—Å ‚Äî —Ä–∞–±–æ—Ç–∞ —Å–æ —Å—Ç—Ä–µ—Å—Å–æ–º 
—á–µ—Ä–µ–∑ –¥—ã—Ö–∞–Ω–∏–µ. –ò—Å–ø—ã—Ç—ã–≤–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å —Ä–µ–≥—É–ª—è—Ä–Ω–æ—Å—Ç—å—é. –ü–æ–ª—É—á–∏–ª –∏–Ω—Å–∞–π—Ç –æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–∏ 
–∑–∞ –º—ã—Å–ª—è–º–∏.

---

–†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –ü–†–û–®–õ–´–ï –û–ë–ú–ï–ù–´:

[–°—Ö–æ–¥—Å—Ç–≤–æ: 0.85] –û–±–º–µ–Ω #8:
  –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: –ö–∞–∫ —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å–æ —Å—Ç—Ä–µ—Å—Å–æ–º —á–µ—Ä–µ–∑ –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ?
  –ë–æ—Ç: –°—Ç—Ä–µ—Å—Å –º–æ–∂–Ω–æ –Ω–∞–±–ª—é–¥–∞—Ç—å –∫–∞–∫ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–µ –æ—â—É—â–µ–Ω–∏–µ –≤ —Ç–µ–ª–µ...
  –°–æ—Å—Ç–æ—è–Ω–∏–µ: stressed

[–°—Ö–æ–¥—Å—Ç–≤–æ: 0.78] –û–±–º–µ–Ω #12:
  –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: –ü–æ—á–µ–º—É —è –Ω–µ –º–æ–≥—É –ø—Ä–∞–∫—Ç–∏–∫–æ–≤–∞—Ç—å —Ä–µ–≥—É–ª—è—Ä–Ω–æ?
  –ë–æ—Ç: –†–µ–≥—É–ª—è—Ä–Ω–æ—Å—Ç—å –ø—Ä–∞–∫—Ç–∏–∫–∏ ‚Äî —á–∞—Å—Ç–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ...
  –°–æ—Å—Ç–æ—è–Ω–∏–µ: frustrated

---

–ò–°–¢–û–†–ò–Ø –î–ò–ê–õ–û–ì–ê (–ø–æ—Å–ª–µ–¥–Ω–∏–µ –æ–±–æ—Ä–æ—Ç—ã):

–û–±–º–µ–Ω #14:
  –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: –ß—Ç–æ –¥–µ–ª–∞—Ç—å —Å –Ω–∞–≤—è–∑—á–∏–≤—ã–º–∏ –º—ã—Å–ª—è–º–∏?
  –ë–æ—Ç: –ù–∞–≤—è–∑—á–∏–≤—ã–µ –º—ã—Å–ª–∏ –º–æ–∂–Ω–æ –Ω–∞–±–ª—é–¥–∞—Ç—å –±–µ–∑ –≤–æ–≤–ª–µ—á–µ–Ω–∏—è...
  –°–æ—Å—Ç–æ—è–Ω–∏–µ: curiosity

–û–±–º–µ–Ω #15:
  –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: –ê –∫–∞–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –Ω–∞–±–ª—é–¥–∞—Ç—å?
  –ë–æ—Ç: –¢–µ—Ö–Ω–∏–∫–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç—Ä–µ—Ö —à–∞–≥–æ–≤...
  –°–æ—Å—Ç–æ—è–Ω–∏–µ: seeking_practical

---

–ú–ê–¢–ï–†–ò–ê–õ –ò–ó –õ–ï–ö–¶–ò–ô:
[–±–ª–æ–∫–∏ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π]

---

–¢–ï–ö–£–©–ò–ô –í–û–ü–†–û–°:
–°–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç –≤ –¥–µ–Ω—å –Ω—É–∂–Ω–æ –ø—Ä–∞–∫—Ç–∏–∫–æ–≤–∞—Ç—å?
```

***

## üîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤ .env

```env
# === Semantic Memory ===
ENABLE_SEMANTIC_MEMORY=true
SEMANTIC_SEARCH_TOP_K=3
SEMANTIC_MIN_SIMILARITY=0.7
SEMANTIC_MAX_CHARS=1000

# === Summary ===
ENABLE_CONVERSATION_SUMMARY=true
SUMMARY_UPDATE_INTERVAL=5  # –∫–∞–∂–¥—ã–µ 5 —Ö–æ–¥–æ–≤
SUMMARY_MAX_CHARS=500

# === Embedding Model ===
EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2
# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã:
# - all-MiniLM-L6-v2 (–±—ã—Å—Ç—Ä–µ–µ, –Ω–æ —Ö—É–∂–µ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ)
# - openai (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å OpenAI embeddings API)
```

***

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤ –∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º

| –ü–æ–¥—Ö–æ–¥ | –°–∫–æ—Ä–æ—Å—Ç—å | –ö–∞—á–µ—Å—Ç–≤–æ (RU) | –°—Ç–æ–∏–º–æ—Å—Ç—å | –û—Ñ—Ñ–ª–∞–π–Ω |
|--------|----------|---------------|-----------|---------|
| **sentence-transformers (local)** | –°—Ä–µ–¥–Ω–µ | –•–æ—Ä–æ—à–æ | –ë–µ—Å–ø–ª–∞—Ç–Ω–æ | ‚úÖ –î–∞ |
| **OpenAI text-embedding-3-small** | –ë—ã—Å—Ç—Ä–æ | –û—Ç–ª–∏—á–Ω–æ | $0.02/1M —Ç–æ–∫–µ–Ω–æ–≤ | ‚ùå –ù–µ—Ç |
| **OpenAI text-embedding-ada-002** | –ë—ã—Å—Ç—Ä–æ | –•–æ—Ä–æ—à–æ | $0.10/1M —Ç–æ–∫–µ–Ω–æ–≤ | ‚ùå –ù–µ—Ç |

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –ù–∞—á–∞—Ç—å —Å `sentence-transformers` –ª–æ–∫–∞–ª—å–Ω–æ, –∑–∞—Ç–µ–º –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å OpenAI –¥–ª—è production.

***

## üöÄ –ü–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã)

### **Phase 1: MVP (—Ä–µ–∫–æ–º–µ–Ω–¥—É—é –Ω–∞—á–∞—Ç—å —Å —ç—Ç–æ–≥–æ)**
1. ‚úÖ –î–æ–±–∞–≤–∏—Ç—å `semantic_memory.py` —Å –±–∞–∑–æ–≤—ã–º –ø–æ–∏—Å–∫–æ–º
2. ‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –≤ `conversation_memory.py`
3. ‚úÖ –û–±–Ω–æ–≤–∏—Ç—å `answer_basic.py` –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ semantic –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
4. ‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –¥–∏–∞–ª–æ–≥–∞—Ö

### **Phase 2: Summary**
5. ‚úÖ –î–æ–±–∞–≤–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ä–µ–∑—é–º–µ
6. ‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—é–º–µ –∫–∞–∂–¥—ã–µ 5 —Ö–æ–¥–æ–≤
7. ‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ä–µ–∑—é–º–µ –≤ –ø—Ä–æ–º–ø—Ç

### **Phase 3: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è**
8. ‚úÖ Lazy loading –º–æ–¥–µ–ª–µ–π
9. ‚úÖ Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
10. ‚úÖ –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

### **Phase 4: Advanced**
11. ‚¨ú –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ OpenAI embeddings
12. ‚¨ú UI –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ semantic search —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
13. ‚¨ú –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–∞–º—è—Ç–∏

***

## üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è

**–ù–∞—á–Ω–∏—Ç–µ —Å Phase 1 (Semantic Memory –±–µ–∑ Summary):**

1. –≠—Ç–æ –¥–∞—Å—Ç —Å—Ä–∞–∑—É 80% –ø–æ–ª—å–∑—ã
2. –ù–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö LLM –≤—ã–∑–æ–≤–æ–≤ (—ç–∫–æ–Ω–æ–º–∏—è)
3. –õ–µ–≥–∫–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –¥–µ–±–∞–∂–∏—Ç—å
4. Summary –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ—Ç–æ–º, –∫–æ–≥–¥–∞ —É–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ semantic search —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ

_____
____
–û—Ç–ª–∏—á–Ω–æ! –°–µ–π—á–∞—Å —Å–æ–∑–¥–∞–º –ø–æ–ª–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é Semantic Memory –¥–ª—è –≤–∞—à–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞.

***

# –ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Semantic Memory System

## üìÅ –§–∞–π–ª 1: `bot_agent/semantic_memory.py` (–ù–û–í–´–ô)

```python
# bot_agent/semantic_memory.py
"""
Semantic Memory Module
======================

–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
–ü–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–æ—à–ª—ã–µ –æ–±–º–µ–Ω—ã –ø–æ —Å–º—ã—Å–ª—É, –∞ –Ω–µ –ø–æ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏–∏.
"""

import logging
import json
import numpy as np
from typing import List, Dict, Tuple, Optional
from pathlib import Path
from dataclasses import dataclass

from .config import config

logger = logging.getLogger(__name__)


@dataclass
class TurnEmbedding:
    """–≠–º–±–µ–¥–¥–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ —Ö–æ–¥–∞ –¥–∏–∞–ª–æ–≥–∞"""
    turn_index: int
    user_input: str
    bot_response_preview: str
    user_state: Optional[str]
    concepts: List[str]
    timestamp: str
    embedding: np.ndarray


class SemanticMemory:
    """
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç sentence-transformers –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π
    –ø—Ä–æ—à–ª—ã—Ö –æ–±–º–µ–Ω–æ–≤ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞.
    
    Usage:
        >>> semantic_mem = SemanticMemory(user_id="test_user")
        >>> semantic_mem.load_from_disk()
        >>> similar = semantic_mem.search_similar_turns("–ö–∞–∫ —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è —Å–æ —Å—Ç—Ä–µ—Å—Å–æ–º?")
    """
    
    def __init__(self, user_id: str = "default"):
        self.user_id = user_id
        self.turn_embeddings: List[TurnEmbedding] = []
        
        # –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –ª–µ–Ω–∏–≤–æ (–ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏)
        self._model = None
        self._model_loaded = False
        
        # –ü—É—Ç–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è
        self.cache_dir = config.CACHE_DIR / "semantic_memory"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        self.embeddings_file = self.cache_dir / f"{user_id}_embeddings.npz"
        self.metadata_file = self.cache_dir / f"{user_id}_metadata.json"
        
        logger.debug(f"üì¶ SemanticMemory —Å–æ–∑–¥–∞–Ω –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_id}")
    
    @property
    def model(self):
        """Lazy loading –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        if not self._model_loaded:
            self._load_model()
        return self._model
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å sentence-transformers"""
        try:
            from sentence_transformers import SentenceTransformer
            
            model_name = config.EMBEDDING_MODEL
            logger.info(f"ü§ñ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {model_name}")
            
            self._model = SentenceTransformer(model_name)
            self._model_loaded = True
            
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
        except ImportError:
            logger.error(
                "‚ùå sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. "
                "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sentence-transformers"
            )
            raise
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    def add_turn_embedding(
        self,
        turn_index: int,
        user_input: str,
        bot_response: Optional[str],
        user_state: Optional[str],
        concepts: List[str],
        timestamp: str
    ) -> None:
        """
        –°–æ–∑–¥–∞—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ö–æ–¥–∞.
        
        Args:
            turn_index: –ò–Ω–¥–µ–∫—Å —Ö–æ–¥–∞ (–Ω–∞—á–∏–Ω–∞—è —Å 1)
            user_input: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            bot_response: –û—Ç–≤–µ—Ç –±–æ—Ç–∞
            user_state: –°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            concepts: –°–ø–∏—Å–æ–∫ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
            timestamp: –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞
        """
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –≤–æ–ø—Ä–æ—Å –∏ –æ—Ç–≤–µ—Ç –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        # –û—Ç–≤–µ—Ç –æ–±—Ä–µ–∑–∞–µ–º –¥–æ 200 —Å–∏–º–≤–æ–ª–æ–≤ —á—Ç–æ–±—ã –Ω–µ –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞–ª –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–µ
        response_preview = (
            bot_response[:200] if bot_response else ""
        )
        
        text_to_embed = f"{user_input} {response_preview}"
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
        try:
            embedding = self.model.encode(
                text_to_embed,
                show_progress_bar=False,
                convert_to_numpy=True
            )
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç TurnEmbedding
            turn_emb = TurnEmbedding(
                turn_index=turn_index,
                user_input=user_input,
                bot_response_preview=response_preview,
                user_state=user_state,
                concepts=concepts,
                timestamp=timestamp,
                embedding=embedding
            )
            
            self.turn_embeddings.append(turn_emb)
            logger.debug(f"‚ûï –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–æ–±–∞–≤–ª–µ–Ω –¥–ª—è —Ö–æ–¥–∞ #{turn_index}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
    
    def search_similar_turns(
        self,
        query: str,
        top_k: int = 3,
        min_similarity: float = 0.7,
        exclude_last_n: int = 5
    ) -> List[Tuple[TurnEmbedding, float]]:
        """
        –ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ –ø—Ä–æ—à–ª—ã–µ –æ–±–º–µ–Ω—ã –ø–æ —Å–µ–º–∞–Ω—Ç–∏–∫–µ.
        
        Args:
            query: –¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            min_similarity: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (0-1)
            exclude_last_n: –ò—Å–∫–ª—é—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Ö–æ–¥–æ–≤ (–æ–Ω–∏ —É–∂–µ –≤ short-term)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (TurnEmbedding, similarity_score)
        """
        if not self.turn_embeddings:
            logger.debug("üîç –ù–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞")
            return []
        
        try:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—É—â–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
            query_embedding = self.model.encode(
                query,
                show_progress_bar=False,
                convert_to_numpy=True
            )
            
            # –°—á–∏—Ç–∞–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å–æ –≤—Å–µ–º–∏ –ø—Ä–æ—à–ª—ã–º–∏ —Ö–æ–¥–∞–º–∏
            similarities = []
            
            # –ò—Å–∫–ª—é—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Ö–æ–¥–æ–≤ (–æ–Ω–∏ —É–∂–µ –≤ short-term memory)
            search_pool = self.turn_embeddings[:-exclude_last_n] if exclude_last_n > 0 else self.turn_embeddings
            
            for turn_emb in search_pool:
                similarity = self._cosine_similarity(query_embedding, turn_emb.embedding)
                
                if similarity >= min_similarity:
                    similarities.append((turn_emb, float(similarity)))
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
            similarities.sort(key=lambda x: x[1], reverse=True)
            
            result = similarities[:top_k]
            
            if result:
                logger.debug(
                    f"üîç –ù–∞–π–¥–µ–Ω–æ {len(result)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–æ—à–ª—ã—Ö —Ö–æ–¥–æ–≤ "
                    f"(—Å—Ö–æ–¥—Å—Ç–≤–æ >= {min_similarity:.2f})"
                )
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def get_context_for_llm(
        self,
        query: str,
        max_chars: int = 1000,
        top_k: int = 3,
        min_similarity: float = 0.7
    ) -> str:
        """
        –ü–æ–ª—É—á–∏—Ç—å –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–æ—à–ª—ã—Ö –æ–±–º–µ–Ω–æ–≤ –¥–ª—è LLM.
        
        Args:
            query: –¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            max_chars: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            min_similarity: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ –ø—Ä–æ—à–ª—ã–º–∏ –æ–±–º–µ–Ω–∞–º–∏
        """
        similar = self.search_similar_turns(
            query,
            top_k=top_k,
            min_similarity=min_similarity
        )
        
        if not similar:
            return ""
        
        context = "–†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –ü–†–û–®–õ–´–ï –û–ë–ú–ï–ù–´:\n\n"
        current_len = len(context)
        
        for turn_emb, score in similar:
            entry = (
                f"[–°—Ö–æ–¥—Å—Ç–≤–æ: {score:.2f}] –û–±–º–µ–Ω #{turn_emb.turn_index}:\n"
                f"  –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {turn_emb.user_input}\n"
                f"  –ë–æ—Ç: {turn_emb.bot_response_preview}"
            )
            
            if len(turn_emb.bot_response_preview) == 200:
                entry += "..."
            
            entry += "\n"
            
            if turn_emb.user_state:
                entry += f"  –°–æ—Å—Ç–æ—è–Ω–∏–µ: {turn_emb.user_state}\n"
            
            if turn_emb.concepts:
                entry += f"  –ö–æ–Ω—Ü–µ–ø—Ç—ã: {', '.join(turn_emb.concepts[:3])}\n"
            
            entry += "\n"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç —Å–∏–º–≤–æ–ª–æ–≤
            if current_len + len(entry) > max_chars:
                if len(context) > len("–†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –ü–†–û–®–õ–´–ï –û–ë–ú–ï–ù–´:\n\n"):
                    # –•–æ—Ç—è –±—ã –æ–¥–∏–Ω –æ–±–º–µ–Ω —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω
                    break
                else:
                    # –î–∞–∂–µ –æ–¥–∏–Ω –æ–±–º–µ–Ω —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π - –æ–±—Ä–µ–∑–∞–µ–º
                    allowed = max_chars - current_len
                    entry = entry[:max(0, allowed - 3)] + "..."
                    if entry:
                        context += entry
                    break
            
            context += entry
            current_len += len(entry)
        
        return context
    
    @staticmethod
    def _cosine_similarity(vec_a: np.ndarray, vec_b: np.ndarray) -> float:
        """
        –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏.
        
        Args:
            vec_a: –ü–µ—Ä–≤—ã–π –≤–µ–∫—Ç–æ—Ä
            vec_b: –í—Ç–æ—Ä–æ–π –≤–µ–∫—Ç–æ—Ä
            
        Returns:
            –°—Ö–æ–¥—Å—Ç–≤–æ –æ—Ç 0 –¥–æ 1
        """
        dot_product = np.dot(vec_a, vec_b)
        norm_a = np.linalg.norm(vec_a)
        norm_b = np.linalg.norm(vec_b)
        
        if norm_a == 0 or norm_b == 0:
            return 0.0
        
        return float(dot_product / (norm_a * norm_b))
    
    def save_to_disk(self) -> None:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–∞ –¥–∏—Å–∫"""
        if not self.turn_embeddings:
            logger.debug("‚ö†Ô∏è –ù–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return
        
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∫–∞–∫ numpy array
            embeddings_array = np.array([
                turn_emb.embedding for turn_emb in self.turn_embeddings
            ])
            
            np.savez_compressed(
                self.embeddings_file,
                embeddings=embeddings_array
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = [
                {
                    "turn_index": turn_emb.turn_index,
                    "user_input": turn_emb.user_input,
                    "bot_response_preview": turn_emb.bot_response_preview,
                    "user_state": turn_emb.user_state,
                    "concepts": turn_emb.concepts,
                    "timestamp": turn_emb.timestamp
                }
                for turn_emb in self.turn_embeddings
            ]
            
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            logger.debug(
                f"üíæ Semantic memory —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {len(self.turn_embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"
            )
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è semantic memory: {e}")
    
    def load_from_disk(self) -> bool:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –¥–∏—Å–∫–∞.
        
        Returns:
            True –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞, False –µ—Å–ª–∏ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã
        """
        if not self.embeddings_file.exists() or not self.metadata_file.exists():
            logger.debug(f"üìã –ù–æ–≤–∞—è semantic memory –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {self.user_id}")
            return False
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            data = np.load(self.embeddings_file)
            embeddings_array = data['embeddings']
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                metadata_list = json.load(f)
            
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º TurnEmbedding –æ–±—ä–µ–∫—Ç—ã
            self.turn_embeddings = []
            
            for i, meta in enumerate(metadata_list):
                turn_emb = TurnEmbedding(
                    turn_index=meta["turn_index"],
                    user_input=meta["user_input"],
                    bot_response_preview=meta["bot_response_preview"],
                    user_state=meta.get("user_state"),
                    concepts=meta.get("concepts", []),
                    timestamp=meta["timestamp"],
                    embedding=embeddings_array[i]
                )
                self.turn_embeddings.append(turn_emb)
            
            logger.info(
                f"‚úÖ Semantic memory –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(self.turn_embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"
            )
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ semantic memory: {e}")
            return False
    
    def rebuild_all_embeddings(self, turns_data: List[Dict]) -> None:
        """
        –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ batch'–µ–º (–¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏/–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è).
        
        Args:
            turns_data: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –æ —Ö–æ–¥–∞—Ö
        """
        if not turns_data:
            return
        
        logger.info(f"üî® –ü–µ—Ä–µ—Å–æ–∑–¥–∞—é {len(turns_data)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è batch encoding
            texts = []
            for turn in turns_data:
                response_preview = (
                    turn.get("bot_response", "")[:200]
                    if turn.get("bot_response")
                    else ""
                )
                text = f"{turn['user_input']} {response_preview}"
                texts.append(text)
            
            # Batch encoding (–±—ã—Å—Ç—Ä–µ–µ —á–µ–º –ø–æ –æ–¥–Ω–æ–º—É)
            embeddings = self.model.encode(
                texts,
                batch_size=32,
                show_progress_bar=True,
                convert_to_numpy=True
            )
            
            # –°–æ–∑–¥–∞–µ–º TurnEmbedding –æ–±—ä–µ–∫—Ç—ã
            self.turn_embeddings = []
            
            for i, turn in enumerate(turns_data):
                response_preview = (
                    turn.get("bot_response", "")[:200]
                    if turn.get("bot_response")
                    else ""
                )
                
                turn_emb = TurnEmbedding(
                    turn_index=i + 1,
                    user_input=turn["user_input"],
                    bot_response_preview=response_preview,
                    user_state=turn.get("user_state"),
                    concepts=turn.get("concepts", []),
                    timestamp=turn.get("timestamp", ""),
                    embedding=embeddings[i]
                )
                
                self.turn_embeddings.append(turn_emb)
            
            self.save_to_disk()
            
            logger.info(f"‚úÖ –í—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω—ã: {len(self.turn_embeddings)}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
    
    def clear(self) -> None:
        """–û—á–∏—Å—Ç–∏—Ç—å semantic memory"""
        self.turn_embeddings = []
        
        # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã
        if self.embeddings_file.exists():
            self.embeddings_file.unlink()
        if self.metadata_file.exists():
            self.metadata_file.unlink()
        
        logger.info("üóëÔ∏è Semantic memory –æ—á–∏—â–µ–Ω–∞")
    
    def get_stats(self) -> Dict:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É semantic memory.
        
        Returns:
            Dict —Å –∫–ª—é—á–µ–≤—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        return {
            "total_embeddings": len(self.turn_embeddings),
            "model_loaded": self._model_loaded,
            "model_name": config.EMBEDDING_MODEL,
            "cache_dir": str(self.cache_dir),
            "embeddings_size_mb": (
                self.embeddings_file.stat().st_size / (1024 * 1024)
                if self.embeddings_file.exists()
                else 0
            )
        }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤ semantic memory
_semantic_memory_instances: Dict[str, SemanticMemory] = {}


def get_semantic_memory(user_id: str = "default") -> SemanticMemory:
    """
    –ü–æ–ª—É—á–∏—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä semantic memory –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (—Å–∏–Ω–≥–ª—Ç–æ–Ω).
    
    Args:
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
    Returns:
        SemanticMemory –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    """
    if user_id not in _semantic_memory_instances:
        semantic_mem = SemanticMemory(user_id)
        semantic_mem.load_from_disk()
        _semantic_memory_instances[user_id] = semantic_mem
    
    return _semantic_memory_instances[user_id]
```

***

## üìÅ –§–∞–π–ª 2: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ `bot_agent/config.py`

```python
# bot_agent/config.py - –î–û–ë–ê–í–ò–¢–¨ –í –ö–û–ù–ï–¶

class Config:
    # ... existing code ...
    
    # === Semantic Memory (NEW) ===
    ENABLE_SEMANTIC_MEMORY = os.getenv("ENABLE_SEMANTIC_MEMORY", "True").lower() == "true"
    SEMANTIC_SEARCH_TOP_K = int(os.getenv("SEMANTIC_SEARCH_TOP_K", "3"))
    SEMANTIC_MIN_SIMILARITY = float(os.getenv("SEMANTIC_MIN_SIMILARITY", "0.7"))
    SEMANTIC_MAX_CHARS = int(os.getenv("SEMANTIC_MAX_CHARS", "1000"))
    
    # –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    EMBEDDING_MODEL = os.getenv(
        "EMBEDDING_MODEL",
        "paraphrase-multilingual-MiniLM-L12-v2"
    )
    
    # === Conversation Summary (NEW) ===
    ENABLE_CONVERSATION_SUMMARY = os.getenv("ENABLE_CONVERSATION_SUMMARY", "True").lower() == "true"
    SUMMARY_UPDATE_INTERVAL = int(os.getenv("SUMMARY_UPDATE_INTERVAL", "5"))
    SUMMARY_MAX_CHARS = int(os.getenv("SUMMARY_MAX_CHARS", "500"))
```

***

## üìÅ –§–∞–π–ª 3: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ `bot_agent/conversation_memory.py`

```python
# bot_agent/conversation_memory.py - –î–û–ü–û–õ–ù–ï–ù–ò–Ø

# –î–æ–±–∞–≤–∏—Ç—å –∏–º–ø–æ—Ä—Ç –≤ –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞
from .semantic_memory import get_semantic_memory, SemanticMemory

class ConversationMemory:
    """
    –•—Ä–∞–Ω–∏—Ç –∏ —É–ø—Ä–∞–≤–ª—è–µ—Ç –∏—Å—Ç–æ—Ä–∏–µ–π –¥–∏–∞–ª–æ–≥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ + semantic search.
    """
    
    def __init__(self, user_id: str = "default"):
        self.user_id = user_id
        self.turns: List[ConversationTurn] = []
        self.metadata: Dict = {
            "created": datetime.now().isoformat(),
            "last_updated": datetime.now().isoformat(),
            "total_turns": 0,
            "user_level": "beginner",
            "primary_interests": [],
            "challenges": [],
            "breakthroughs": []
        }
        self.memory_dir = config.CACHE_DIR / "conversations"
        self.memory_dir.mkdir(parents=True, exist_ok=True)
        
        # === –ù–û–í–û–ï: Semantic Memory ===
        self.semantic_memory: Optional[SemanticMemory] = None
        if config.ENABLE_SEMANTIC_MEMORY:
            self.semantic_memory = get_semantic_memory(user_id)
        
        # === –ù–û–í–û–ï: Conversation Summary ===
        self.summary: Optional[str] = None
        self.summary_updated_at: Optional[int] = None  # turn index
    
    def load_from_disk(self) -> bool:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ —Å –¥–∏—Å–∫–∞.
        
        Returns:
            True –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞, False –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω
        """
        filepath = self.memory_dir / f"{self.user_id}.json"
        
        if not filepath.exists():
            logger.debug(f"üìã –ù–æ–≤–∞—è –∏—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {self.user_id}")
            return False
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.metadata = data.get("metadata", self.metadata)
            self.turns = [
                ConversationTurn(**turn_data)
                for turn_data in data.get("turns", [])
            ]
            
            # === –ù–û–í–û–ï: –ó–∞–≥—Ä—É–∑–∏—Ç—å summary ===
            self.summary = data.get("summary")
            self.summary_updated_at = data.get("summary_updated_at")
            
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∏—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞: {len(self.turns)} –æ–±–æ—Ä–æ—Ç–æ–≤")
            return True
        
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏: {e}")
            return False
    
    def save_to_disk(self) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ –Ω–∞ –¥–∏—Å–∫.
        """
        filepath = self.memory_dir / f"{self.user_id}.json"
        
        self.metadata["last_updated"] = datetime.now().isoformat()
        self.metadata["total_turns"] = len(self.turns)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump({
                    "metadata": self.metadata,
                    "turns": [asdict(turn) for turn in self.turns],
                    # === –ù–û–í–û–ï: –°–æ—Ö—Ä–∞–Ω–∏—Ç—å summary ===
                    "summary": self.summary,
                    "summary_updated_at": self.summary_updated_at
                }, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"üíæ –ò—Å—Ç–æ—Ä–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ ({len(self.turns)} –æ–±–æ—Ä–æ—Ç–æ–≤)")
        
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏: {e}")
    
    def add_turn(
        self,
        user_input: str,
        bot_response: str,
        user_state: Optional[str] = None,
        blocks_used: int = 0,
        concepts: Optional[List[str]] = None
    ) -> ConversationTurn:
        """
        –î–æ–±–∞–≤–∏—Ç—å —Ö–æ–¥ –≤ –∏—Å—Ç–æ—Ä–∏—é.
        
        Args:
            user_input: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            bot_response: –û—Ç–≤–µ—Ç –±–æ—Ç–∞
            user_state: –°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–∏–∑ StateClassifier)
            blocks_used: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤
            concepts: –°–ø–∏—Å–æ–∫ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
            
        Returns:
            –°–æ–∑–¥–∞–Ω–Ω—ã–π ConversationTurn
        """
        turn = ConversationTurn(
            timestamp=datetime.now().isoformat(),
            user_input=user_input,
            user_state=user_state,
            bot_response=bot_response,
            blocks_used=blocks_used,
            concepts=concepts or []
        )
        
        self.turns.append(turn)
        turn_index = len(self.turns)
        logger.debug(f"‚ûï –î–æ–±–∞–≤–ª–µ–Ω —Ö–æ–¥ #{turn_index}")
        
        # === –ù–û–í–û–ï: –î–æ–±–∞–≤–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤ semantic memory ===
        if self.semantic_memory and config.ENABLE_SEMANTIC_MEMORY:
            try:
                self.semantic_memory.add_turn_embedding(
                    turn_index=turn_index,
                    user_input=user_input,
                    bot_response=bot_response,
                    user_state=user_state,
                    concepts=concepts or [],
                    timestamp=turn.timestamp
                )
                self.semantic_memory.save_to_disk()
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
        
        # === –ù–û–í–û–ï: –û–±–Ω–æ–≤–∏—Ç—å summary –∫–∞–∂–¥—ã–µ N —Ö–æ–¥–æ–≤ ===
        if config.ENABLE_CONVERSATION_SUMMARY and turn_index % config.SUMMARY_UPDATE_INTERVAL == 0:
            self._update_summary()
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â–µ–µ —á–∏—Å–ª–æ —Ö–æ–¥–æ–≤ (–∞–≤—Ç–æ—Ä–æ—Ç–∞—Ü–∏—è)
        max_turns = config.MAX_CONVERSATION_TURNS
        if max_turns and len(self.turns) > max_turns:
            overflow = len(self.turns) - max_turns
            self.turns = self.turns[overflow:]
        
        self.save_to_disk()
        return turn
    
    # === –ù–û–í–´–ô –ú–ï–¢–û–î: –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM ===
    def get_full_context_for_llm(
        self,
        current_question: str,
        include_semantic: bool = True,
        include_summary: bool = True
    ) -> Dict[str, str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM —Å–æ –≤—Å–µ–º–∏ —Ç–∏–ø–∞–º–∏ –ø–∞–º—è—Ç–∏.
        
        Args:
            current_question: –¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            include_semantic: –í–∫–ª—é—á–∞—Ç—å semantic memory
            include_summary: –í–∫–ª—é—á–∞—Ç—å summary
            
        Returns:
            Dict —Å —Ç—Ä–µ–º—è –≤–∏–¥–∞–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:
            - short_term: –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Ö–æ–¥–æ–≤
            - semantic: —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–æ—à–ª—ã–µ –æ–±–º–µ–Ω—ã
            - summary: –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –¥–∏–∞–ª–æ–≥–∞
        """
        context = {
            "short_term": "",
            "semantic": "",
            "summary": ""
        }
        
        # 1. Short-term memory (–≤—Å–µ–≥–¥–∞)
        context["short_term"] = self.get_context_for_llm(
            n=config.CONVERSATION_HISTORY_DEPTH,
            max_chars=config.MAX_CONTEXT_SIZE
        )
        
        # 2. Semantic memory (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
        if include_semantic and self.semantic_memory and config.ENABLE_SEMANTIC_MEMORY:
            try:
                context["semantic"] = self.semantic_memory.get_context_for_llm(
                    query=current_question,
                    max_chars=config.SEMANTIC_MAX_CHARS,
                    top_k=config.SEMANTIC_SEARCH_TOP_K,
                    min_similarity=config.SEMANTIC_MIN_SIMILARITY
                )
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ semantic search: {e}")
        
        # 3. Summary (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
        if include_summary and config.ENABLE_CONVERSATION_SUMMARY and self.summary:
            context["summary"] = self.summary
        
        return context
    
    # === –ù–û–í–´–ô –ú–ï–¢–û–î: –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ===
    def get_adaptive_context_for_llm(self, current_question: str) -> Dict[str, str]:
        """
        –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–ª–∏–Ω—ã –¥–∏–∞–ª–æ–≥–∞.
        
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è:
        - 1-5 —Ö–æ–¥–æ–≤: —Ç–æ–ª—å–∫–æ short-term (–≤—Å–µ —Ö–æ–¥—ã)
        - 6-20 —Ö–æ–¥–æ–≤: short-term + semantic
        - 21+ —Ö–æ–¥–æ–≤: short-term + semantic + summary
        
        Args:
            current_question: –¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            Dict —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–ª—è —Ç–µ–∫—É—â–µ–π –¥–ª–∏–Ω—ã –¥–∏–∞–ª–æ–≥–∞
        """
        total_turns = len(self.turns)
        
        if total_turns <= 5:
            # –ö–æ—Ä–æ—Ç–∫–∏–π –¥–∏–∞–ª–æ–≥ ‚Äî –±–µ—Ä–µ–º –≤—Å–µ —Ö–æ–¥—ã, –æ—Å—Ç–∞–ª—å–Ω–æ–µ –Ω–µ –Ω—É–∂–Ω–æ
            return {
                "short_term": self.get_context_for_llm(n=total_turns),
                "semantic": "",
                "summary": ""
            }
        
        elif total_turns <= 20:
            # –°—Ä–µ–¥–Ω–∏–π –¥–∏–∞–ª–æ–≥ ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º semantic search
            return self.get_full_context_for_llm(
                current_question,
                include_semantic=True,
                include_summary=False
            )
        
        else:
            # –î–ª–∏–Ω–Ω—ã–π –¥–∏–∞–ª–æ–≥ ‚Äî full stack
            return self.get_full_context_for_llm(
                current_question,
                include_semantic=True,
                include_summary=True
            )
    
    # === –ù–û–í–´–ô –ú–ï–¢–û–î: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ summary ===
    def _update_summary(self) -> None:
        """
        –û–±–Ω–æ–≤–∏—Ç—å —Ä–µ–∑—é–º–µ –¥–∏–∞–ª–æ–≥–∞ —á–µ—Ä–µ–∑ LLM.
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–∞–∂–¥—ã–µ N —Ö–æ–¥–æ–≤.
        """
        if len(self.turns) < 5:
            return
        
        logger.info(f"üìù –û–±–Ω–æ–≤–ª—è—é —Ä–µ–∑—é–º–µ –¥–∏–∞–ª–æ–≥–∞ (—Ö–æ–¥ #{len(self.turns)})...")
        
        try:
            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Ö–æ–¥–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑—é–º–µ
            recent_turns = self.turns[-10:]
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞
            turns_text = ""
            for i, turn in enumerate(recent_turns, 1):
                turns_text += f"\n–•–æ–¥ {i}:\n"
                turns_text += f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {turn.user_input}\n"
                
                # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
                response = turn.bot_response or ""
                if len(response) > 200:
                    response = response[:200] + "..."
                turns_text += f"–ë–æ—Ç: {response}\n"
                
                if turn.user_state:
                    turns_text += f"–°–æ—Å—Ç–æ—è–Ω–∏–µ: {turn.user_state}\n"
            
            # –ü—Ä–æ–º–ø—Ç –¥–ª—è LLM
            summary_prompt = f"""–°–æ–∑–¥–∞–π –ö–†–ê–¢–ö–û–ï —Ä–µ–∑—é–º–µ –¥–∏–∞–ª–æ–≥–∞ (–º–∞–∫—Å–∏–º—É–º 500 —Å–∏–º–≤–æ–ª–æ–≤, –ø–æ-—Ä—É—Å—Å–∫–∏).

–í–∫–ª—é—á–∏:
- –ö–ª—é—á–µ–≤—ã–µ —Ç–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—Å—É–∂–¥–∞–ª–∏—Å—å
- –ü—Ä–æ–≥—Ä–µ—Å—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–∏
- –í–∞–∂–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã –∏–ª–∏ –ø—Ä–æ—Ä—ã–≤—ã (–µ—Å–ª–∏ –±—ã–ª–∏)
- –¢–µ–∫—É—â–∏–π —Ñ–æ–∫—É—Å –¥–∏–∞–ª–æ–≥–∞

–î–ò–ê–õ–û–ì (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Ö–æ–¥–æ–≤):
{turns_text}

–†–ï–ó–Æ–ú–ï (–∫—Ä–∞—Ç–∫–æ, –æ–¥–Ω–∏–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–º, –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤):"""
            
            # –í—ã–∑—ã–≤–∞–µ–º LLM –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑—é–º–µ
            from .llm_answerer import get_llm_answerer
            answerer = get_llm_answerer()
            
            response = answerer.client.chat.completions.create(
                model=config.LLM_MODEL,
                messages=[{"role": "user", "content": summary_prompt}],
                temperature=0.3,
                max_tokens=200
            )
            
            self.summary = response.choices[0].message.content.strip()
            self.summary_updated_at = len(self.turns)
            
            logger.info(f"‚úÖ –†–µ–∑—é–º–µ –æ–±–Ω–æ–≤–ª–µ–Ω–æ: {len(self.summary)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ –¥–∏—Å–∫
            self.save_to_disk()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ä–µ–∑—é–º–µ: {e}")
    
    def clear(self) -> None:
        """–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ –∏ semantic memory"""
        self.turns = []
        self.metadata["last_updated"] = datetime.now().isoformat()
        self.metadata["total_turns"] = 0
        
        # === –ù–û–í–û–ï: –û—á–∏—Å—Ç–∏—Ç—å summary ===
        self.summary = None
        self.summary_updated_at = None
        
        # === –ù–û–í–û–ï: –û—á–∏—Å—Ç–∏—Ç—å semantic memory ===
        if self.semantic_memory:
            self.semantic_memory.clear()
        
        self.save_to_disk()
    
    def rebuild_semantic_memory(self) -> None:
        """
        –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å semantic memory –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–π –∏—Å—Ç–æ—Ä–∏–∏.
        –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏ –∏–ª–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è.
        """
        if not self.semantic_memory:
            logger.warning("‚ö†Ô∏è Semantic memory –Ω–µ –≤–∫–ª—é—á–µ–Ω–∞")
            return
        
        if not self.turns:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç —Ö–æ–¥–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
            return
        
        logger.info(f"üî® –ü–µ—Ä–µ—Å–æ–∑–¥–∞—é semantic memory –¥–ª—è {len(self.turns)} —Ö–æ–¥–æ–≤...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        turns_data = [
            {
                "user_input": turn.user_input,
                "bot_response": turn.bot_response,
                "user_state": turn.user_state,
                "concepts": turn.concepts,
                "timestamp": turn.timestamp
            }
            for turn in self.turns
        ]
        
        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ batch'–µ–º
        self.semantic_memory.rebuild_all_embeddings(turns_data)
        
        logger.info("‚úÖ Semantic memory –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∞")
    
    def get_summary(self) -> Dict:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞ —Å semantic stats.
        
        Returns:
            Dict —Å –∫–ª—é—á–µ–≤—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        interests = self.get_primary_interests()
        challenges = self.get_challenges()
        breakthroughs = self.get_breakthroughs()
        
        # –°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥
        avg_rating = 0.0
        if self.turns:
            ratings = [t.user_rating for t in self.turns if t.user_rating]
            avg_rating = sum(ratings) / len(ratings) if ratings else 0.0
        
        result = {
            "total_turns": len(self.turns),
            "primary_interests": interests,
            "num_challenges": len(challenges),
            "num_breakthroughs": len(breakthroughs),
            "average_rating": round(avg_rating, 2),
            "user_level": self.metadata.get("user_level", "beginner"),
            "last_interaction": self.turns[-1].timestamp if self.turns else None,
            # === –ù–û–í–û–ï: –î–æ–±–∞–≤–∏—Ç—å semantic memory stats ===
            "conversation_summary": self.summary,
            "summary_updated_at_turn": self.summary_updated_at
        }
        
        if self.semantic_memory:
            result["semantic_memory"] = self.semantic_memory.get_stats()
        
        return result
```

***

## üìÅ –§–∞–π–ª 4: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ `bot_agent/answer_basic.py`

```python
# bot_agent/answer_basic.py - –ò–ó–ú–ï–ù–ï–ù–ò–Ø

def answer_question(
    question: str,
    user_id: str = "default",
    top_k: int = None,
    user_level: str = "beginner",
    use_semantic_memory: bool = True  # –ù–û–í–´–ô –ü–ê–†–ê–ú–ï–¢–†
) -> Dict:
    """
    –û—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (Phase 1: Basic QA + Memory).
    
    Args:
        question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –ø–∞–º—è—Ç–∏ –¥–∏–∞–ª–æ–≥–∞
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –±–ª–æ–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ config)
        user_level: –£—Ä–æ–≤–µ–Ω—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (beginner/intermediate/advanced)
        use_semantic_memory: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å semantic memory (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
        
    Returns:
        Dict —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    """
    if top_k is None:
        top_k = config.TOP_K_BLOCKS
    
    logger.info(f"üí¨ –í–æ–ø—Ä–æ—Å –æ—Ç {user_id}: {question[:50]}...")
    
    # === 1. –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–∞–º—è—Ç—å –¥–∏–∞–ª–æ–≥–∞ ===
    memory = get_conversation_memory(user_id)
    
    # === 2. –ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–∞–º—è—Ç–∏ (SHORT + SEMANTIC + SUMMARY) ===
    if use_semantic_memory and config.ENABLE_SEMANTIC_MEMORY:
        memory_context = memory.get_adaptive_context_for_llm(question)
    else:
        # –¢–æ–ª—å–∫–æ short-term memory
        memory_context = {
            "short_term": memory.get_context_for_llm(n=config.CONVERSATION_HISTORY_DEPTH),
            "semantic": "",
            "summary": ""
        }
    
    # === 3. –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –±–ª–æ–∫–æ–≤ (–∫–∞–∫ –æ–±—ã—á–Ω–æ) ===
    retriever = get_retriever()
    retriever.build_index()
    
    top_blocks = retriever.retrieve(question, top_k=top_k)
    
    if not top_blocks:
        logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –±–ª–æ–∫–æ–≤!")
        return {
            "answer": "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –¥–ª—è –≤–∞—à–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞.",
            "blocks": [],
            "error": "No relevant blocks found"
        }
    
    logger.info(f"‚úì –ù–∞–π–¥–µ–Ω–æ {len(top_blocks)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –±–ª–æ–∫–æ–≤")
    
    # === 4. –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ–º–ø—Ç —Å –ü–û–õ–ù–´–ú –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º ===
    full_context = _build_full_context_prompt(
        memory_context=memory_context,
        blocks=top_blocks,
        question=question
    )
    
    # === 5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ LLM ===
    answerer = get_llm_answerer()
    
    result = answerer.generate_answer(
        user_question=question,
        blocks=top_blocks,
        conversation_history=full_context,  # –ü–µ—Ä–µ–¥–∞–µ–º –≤–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
        model=config.LLM_MODEL,
        temperature=config.LLM_TEMPERATURE,
        max_tokens=config.LLM_MAX_TOKENS
    )
    
    # === 6. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ø–∞–º—è—Ç—å ===
    memory.add_turn(
        user_input=question,
        bot_response=result["answer"],
        user_state=None,  # –í Phase 1 –Ω–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π
        blocks_used=len(top_blocks),
        concepts=[b.title for b in top_blocks]
    )
    
    # === 7. –î–æ–±–∞–≤–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ –æ—Ç–≤–µ—Ç ===
    result["blocks"] = [
        {
            "block_id": block.block_id,
            "title": block.title,
            "summary": block.summary,
            "document_title": block.document_title,
            "youtube_link": block.youtube_link,
            "start": block.start,
            "end": block.end,
            "relevance_score": float(score)
        }
        for block, score in top_blocks
    ]
    
    result["memory_context_used"] = {
        "short_term_chars": len(memory_context["short_term"]),
        "semantic_chars": len(memory_context["semantic"]),
        "summary_chars": len(memory_context["summary"]),
        "semantic_enabled": use_semantic_memory and config.ENABLE_SEMANTIC_MEMORY
    }
    
    logger.info(f"‚úÖ –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {len(result['answer'])} —Å–∏–º–≤–æ–ª–æ–≤")
    
    return result


def _build_full_context_prompt(
    memory_context: Dict[str, str],
    blocks: List[Tuple[Any, float]],
    question: str
) -> str:
    """
    –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –ø–∞–º—è—Ç—å—é –∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏.
    
    Args:
        memory_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç—ã –ø–∞–º—è—Ç–∏ (short_term, semantic, summary)
        blocks: –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –±–ª–æ–∫–∏ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        question: –¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å
        
    Returns:
        –ü–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM
    """
    parts = []
    
    # 1. Summary (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if memory_context["summary"]:
        parts.append(f"""–ö–†–ê–¢–ö–û–ï –†–ï–ó–Æ–ú–ï –î–ò–ê–õ–û–ì–ê:
{memory_context["summary"]}

---
""")
    
    # 2. Semantic memory (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–æ—à–ª—ã–µ –æ–±–º–µ–Ω—ã)
    if memory_context["semantic"]:
        parts.append(memory_context["semantic"] + "\n---\n\n")
    
    # 3. Short-term memory (–ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ö–æ–¥—ã)
    if memory_context["short_term"]:
        parts.append(memory_context["short_term"] + "\n---\n\n")
    
    # 4. –ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    parts.append("–ú–ê–¢–ï–†–ò–ê–õ –ò–ó –õ–ï–ö–¶–ò–ô:\n\n")
    
    for block, score in blocks:
        block_text = f"""--- –ë–õ–û–ö ---
–õ–µ–∫—Ü–∏—è: {block.document_title}
–¢–µ–º–∞: {block.title}
–¢–∞–π–º–∫–æ–¥: {block.start} ‚Äî {block.end}
–°—Å—ã–ª–∫–∞: {block.youtube_link}

–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ: {block.summary}

–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç:
{block.content}

"""
        parts.append(block_text)
    
    # 5. –¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å
    parts.append(f"""
--- –¢–ï–ö–£–©–ò–ô –í–û–ü–†–û–° ---

–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {question}

–°—Ñ–æ—Ä–º–∏—Ä—É–π –æ—Ç–≤–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É—è –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ –∏ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –≤—ã—à–µ.
""")
    
    return "".join(parts)
```

***

## üìÅ –§–∞–π–ª 5: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ `.env.example`

```env
# bot_psychologist/.env.example - –î–û–ë–ê–í–ò–¢–¨ –í –ö–û–ù–ï–¶

# ===== Semantic Memory =====
ENABLE_SEMANTIC_MEMORY=true
SEMANTIC_SEARCH_TOP_K=3
SEMANTIC_MIN_SIMILARITY=0.7
SEMANTIC_MAX_CHARS=1000

# Embedding Model Options:
# - paraphrase-multilingual-MiniLM-L12-v2 (default, —Ö–æ—Ä–æ—à–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ)
# - all-MiniLM-L6-v2 (–±—ã—Å—Ç—Ä–µ–µ, —Ö—É–∂–µ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ)
# - all-mpnet-base-v2 (–ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ, –º–µ–¥–ª–µ–Ω–Ω–µ–µ)
EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2

# ===== Conversation Summary =====
ENABLE_CONVERSATION_SUMMARY=true
SUMMARY_UPDATE_INTERVAL=5
SUMMARY_MAX_CHARS=500
```

***

## üìÅ –§–∞–π–ª 6: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ `requirements_bot.txt`

```txt
# bot_psychologist/requirements_bot.txt - –î–û–ë–ê–í–ò–¢–¨

# Existing dependencies
openai>=1.0.0
python-dotenv>=1.0.0
scikit-learn>=1.3.0
numpy>=1.24.0

# NEW: Sentence Transformers –¥–ª—è semantic memory
sentence-transformers>=2.2.0
torch>=2.0.0  # –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–ª—è sentence-transformers
```

***

## üìÅ –§–∞–π–ª 7: –¢–µ—Å—Ç `test_semantic_memory.py` (–ù–û–í–´–ô)

```python
# bot_psychologist/test_semantic_memory.py
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Semantic Memory
"""

import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.insert(0, str(Path(__file__).parent))

from bot_agent.conversation_memory import get_conversation_memory
from bot_agent.semantic_memory import get_semantic_memory
from bot_agent.config import config

def test_semantic_memory():
    """–¢–µ—Å—Ç semantic memory"""
    
    print("=" * 60)
    print("–¢–ï–°–¢ SEMANTIC MEMORY")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    user_id = "test_semantic_user"
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–∞–º—è—Ç—å
    memory = get_conversation_memory(user_id)
    memory.clear()  # –û—á–∏—â–∞–µ–º –¥–ª—è —á–∏—Å—Ç–æ–≥–æ —Ç–µ—Å—Ç–∞
    
    print(f"\n‚úì –ü–∞–º—è—Ç—å —Å–æ–∑–¥–∞–Ω–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_id}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ö–æ–¥–æ–≤ —Å —Ä–∞–∑–Ω—ã–º–∏ —Ç–µ–º–∞–º–∏
    test_turns = [
        {
            "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ?",
            "answer": "–û—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ ‚Äî —ç—Ç–æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –Ω–∞–±–ª—é–¥–∞—Ç—å –∑–∞ —Å–≤–æ–∏–º–∏ –º—ã—Å–ª—è–º–∏, —ç–º–æ—Ü–∏—è–º–∏ –∏ –æ—â—É—â–µ–Ω–∏—è–º–∏ –±–µ–∑ –≤–æ–≤–ª–µ—á–µ–Ω–∏—è –≤ –Ω–∏—Ö.",
            "state": "curiosity"
        },
        {
            "question": "–ö–∞–∫ –º–µ–¥–∏—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ?",
            "answer": "–ú–µ–¥–∏—Ç–∞—Ü–∏—è –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —É–¥–æ–±–Ω–æ–π –ø–æ–∑—ã. –°–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Ç–µ—Å—å –Ω–∞ –¥—ã—Ö–∞–Ω–∏–∏ –∏ –Ω–∞–±–ª—é–¥–∞–π—Ç–µ –∑–∞ –ø–æ—Ç–æ–∫–æ–º –º—ã—Å–ª–µ–π.",
            "state": "seeking_practical"
        },
        {
            "question": "–ö–∞–∫ —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è —Å–æ —Å—Ç—Ä–µ—Å—Å–æ–º?",
            "answer": "–°—Ç—Ä–µ—Å—Å –º–æ–∂–Ω–æ –Ω–∞–±–ª—é–¥–∞—Ç—å –∫–∞–∫ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–µ –æ—â—É—â–µ–Ω–∏–µ. –û—Å–æ–∑–Ω–∞–π—Ç–µ –≥–¥–µ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ –≤ —Ç–µ–ª–µ –∏ –Ω–∞–ø—Ä–∞–≤—å—Ç–µ —Ç—É–¥–∞ –≤–Ω–∏–º–∞–Ω–∏–µ.",
            "state": "stressed"
        },
        {
            "question": "–ß—Ç–æ –¥–µ–ª–∞—Ç—å —Å –Ω–∞–≤—è–∑—á–∏–≤—ã–º–∏ –º—ã—Å–ª—è–º–∏?",
            "answer": "–ù–∞–≤—è–∑—á–∏–≤—ã–µ –º—ã—Å–ª–∏ –º–æ–∂–Ω–æ –Ω–∞–±–ª—é–¥–∞—Ç—å –±–µ–∑ –≤–æ–≤–ª–µ—á–µ–Ω–∏—è. –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ –∏—Ö –∫–∞–∫ –æ–±–ª–∞–∫–∞ –Ω–∞ –Ω–µ–±–µ.",
            "state": "frustrated"
        },
        {
            "question": "–°–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω—É–∂–Ω–æ –º–µ–¥–∏—Ç–∏—Ä–æ–≤–∞—Ç—å?",
            "answer": "–î–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ 10-15 –º–∏–Ω—É—Ç –≤ –¥–µ–Ω—å. –ì–ª–∞–≤–Ω–æ–µ ‚Äî —Ä–µ–≥—É–ª—è—Ä–Ω–æ—Å—Ç—å, –∞ –Ω–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.",
            "state": "planning"
        }
    ]
    
    print(f"\nüìù –î–æ–±–∞–≤–ª—è—é {len(test_turns)} —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ö–æ–¥–æ–≤...")
    
    for turn in test_turns:
        memory.add_turn(
            user_input=turn["question"],
            bot_response=turn["answer"],
            user_state=turn["state"],
            blocks_used=2,
            concepts=["—Ç–µ—Å—Ç"]
        )
        print(f"  ‚úì –î–æ–±–∞–≤–ª–µ–Ω: {turn['question'][:40]}...")
    
    print(f"\n‚úÖ –í—Å–µ–≥–æ —Ö–æ–¥–æ–≤ –≤ –ø–∞–º—è—Ç–∏: {len(memory.turns)}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º semantic memory
    if memory.semantic_memory:
        stats = memory.semantic_memory.get_stats()
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Semantic Memory:")
        print(f"  ‚Ä¢ –≠–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {stats['total_embeddings']}")
        print(f"  ‚Ä¢ –ú–æ–¥–µ–ª—å: {stats['model_name']}")
        print(f"  ‚Ä¢ –†–∞–∑–º–µ—Ä –Ω–∞ –¥–∏—Å–∫–µ: {stats['embeddings_size_mb']:.2f} MB")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º semantic search
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï SEMANTIC SEARCH")
    print("=" * 60)
    
    test_queries = [
        "–ö–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø—Ä–∞–∫—Ç–∏–∫–æ–≤–∞—Ç—å –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å?",  # –ü–æ—Ö–æ–∂–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã 1, 2
        "–£ –º–µ–Ω—è —Å—Ç—Ä–µ—Å—Å, —á—Ç–æ –¥–µ–ª–∞—Ç—å?",  # –ü–æ—Ö–æ–∂–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å 3
        "–ü–æ—á–µ–º—É —è –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –¥—É–º–∞—é –æ –ø–ª–æ—Ö–æ–º?",  # –ü–æ—Ö–æ–∂–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å 4
        "–°–∫–æ–ª—å–∫–æ —Ä–∞–∑ –≤ –Ω–µ–¥–µ–ª—é –Ω—É–∂–Ω–æ –∑–∞–Ω–∏–º–∞—Ç—å—Å—è?"  # –ü–æ—Ö–æ–∂–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å 5
    ]
    
    for query in test_queries:
        print(f"\nüîç –ó–∞–ø—Ä–æ—Å: \"{query}\"")
        print("-" * 60)
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ adaptive –º–µ—Ç–æ–¥
        context = memory.get_adaptive_context_for_llm(query)
        
        if context["semantic"]:
            print("\n‚úÖ –ù–∞–π–¥–µ–Ω—ã —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–æ—à–ª—ã–µ –æ–±–º–µ–Ω—ã:")
            print(context["semantic"])
        else:
            print("\n‚ö†Ô∏è –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ–±–º–µ–Ω–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä—è–º–æ–π –ø–æ–∏—Å–∫
        if memory.semantic_memory:
            similar = memory.semantic_memory.search_similar_turns(
                query=query,
                top_k=2,
                min_similarity=0.5  # –°–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            )
            
            print(f"\nüìà –ü–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (—Ç–æ–ø-{len(similar)}):")
            for turn_emb, score in similar:
                print(f"  [{score:.3f}] –û–±–º–µ–Ω #{turn_emb.turn_index}: {turn_emb.user_input[:50]}...")
    
    # –¢–µ—Å—Ç –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    print("\n" + "=" * 60)
    print("–ü–û–õ–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢ –î–õ–Ø LLM")
    print("=" * 60)
    
    test_question = "–ö–∞–∫ –Ω–∞—á–∞—Ç—å –ø—Ä–∞–∫—Ç–∏–∫–æ–≤–∞—Ç—å –º–µ–¥–∏—Ç–∞—Ü–∏—é –µ—Å–ª–∏ —É –º–µ–Ω—è —Å—Ç—Ä–µ—Å—Å?"
    
    full_context = memory.get_adaptive_context_for_llm(test_question)
    
    print(f"\nüéØ –í–æ–ø—Ä–æ—Å: {test_question}")
    print("\nüì¶ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:")
    print(f"  ‚Ä¢ Short-term: {len(full_context['short_term'])} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"  ‚Ä¢ Semantic: {len(full_context['semantic'])} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"  ‚Ä¢ Summary: {len(full_context['summary'])} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"  ‚Ä¢ –ò–¢–û–ì–û: {sum(len(v) for v in full_context.values())} —Å–∏–º–≤–æ–ª–æ–≤")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–≤—å—é –∫–∞–∂–¥–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
    if full_context["summary"]:
        print("\n--- SUMMARY ---")
        print(full_context["summary"][:200] + "..." if len(full_context["summary"]) > 200 else full_context["summary"])
    
    if full_context["semantic"]:
        print("\n--- SEMANTIC (–ø–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤–æ–ª–æ–≤) ---")
        print(full_context["semantic"][:300] + "...")
    
    if full_context["short_term"]:
        print("\n--- SHORT-TERM (–ø–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤–æ–ª–æ–≤) ---")
        print(full_context["short_term"][:300] + "...")
    
    # –û—á–∏—Å—Ç–∫–∞ –ø–æ—Å–ª–µ —Ç–µ—Å—Ç–∞
    print("\n" + "=" * 60)
    print("üóëÔ∏è –û—á–∏—Å—Ç–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    memory.clear()
    print("‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")


if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    print(f"‚öôÔ∏è Semantic Memory: {'‚úì –í–∫–ª—é—á–µ–Ω–∞' if config.ENABLE_SEMANTIC_MEMORY else '‚úó –í—ã–∫–ª—é—á–µ–Ω–∞'}")
    print(f"‚öôÔ∏è Conversation Summary: {'‚úì –í–∫–ª—é—á–µ–Ω–∞' if config.ENABLE_CONVERSATION_SUMMARY else '‚úó –í—ã–∫–ª—é—á–µ–Ω–∞'}")
    print(f"‚öôÔ∏è Embedding Model: {config.EMBEDDING_MODEL}")
    print()
    
    try:
        test_semantic_memory()
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
```

***

## üìÅ –§–∞–π–ª 8: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ API routes `api/routes.py`

```python
# api/routes.py - –î–û–ë–ê–í–ò–¢–¨ –ù–û–í–´–ï ENDPOINTS

from fastapi import APIRouter, HTTPException
from bot_agent.conversation_memory import get_conversation_memory

# ... existing code ...

@router.post("/questions/basic-with-semantic")
async def answer_basic_with_semantic(request: QuestionRequest):
    """
    –û—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º semantic memory (Phase 1 Enhanced).
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
    - Short-term memory (–ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Ö–æ–¥–æ–≤)
    - Semantic memory (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–æ—à–ª—ã–µ –æ–±–º–µ–Ω—ã)
    - Conversation summary (–∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –≤—Å–µ–≥–æ –¥–∏–∞–ª–æ–≥–∞)
    """
    try:
        from bot_agent.answer_basic import answer_question
        
        result = answer_question(
            question=request.question,
            user_id=request.user_id,
            top_k=request.top_k,
            user_level=request.user_level,
            use_semantic_memory=True  # –í–∫–ª—é—á–∏—Ç—å semantic memory
        )
        
        return result
        
    except Exception as e:
        logger.error(f"Error in basic semantic answering: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/users/{user_id}/semantic-stats")
async def get_semantic_stats(user_id: str):
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É semantic memory –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    """
    try:
        memory = get_conversation_memory(user_id)
        
        if not memory.semantic_memory:
            return {
                "enabled": False,
                "message": "Semantic memory –Ω–µ –≤–∫–ª—é—á–µ–Ω–∞"
            }
        
        stats = memory.semantic_memory.get_stats()
        
        return {
            "enabled": True,
            "user_id": user_id,
            **stats
        }
        
    except Exception as e:
        logger.error(f"Error getting semantic stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/users/{user_id}/rebuild-semantic-memory")
async def rebuild_semantic_memory(user_id: str):
    """
    –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å semantic memory –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–π –∏—Å—Ç–æ—Ä–∏–∏.
    –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏ –∏–ª–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ –æ—à–∏–±–æ–∫.
    """
    try:
        memory = get_conversation_memory(user_id)
        
        if not memory.semantic_memory:
            raise HTTPException(
                status_code=400,
                detail="Semantic memory –Ω–µ –≤–∫–ª—é—á–µ–Ω–∞"
            )
        
        memory.rebuild_semantic_memory()
        
        stats = memory.semantic_memory.get_stats()
        
        return {
            "success": True,
            "message": f"Semantic memory –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∞ –¥–ª—è {stats['total_embeddings']} —Ö–æ–¥–æ–≤",
            "stats": stats
        }
        
    except Exception as e:
        logger.error(f"Error rebuilding semantic memory: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/users/{user_id}/update-summary")
async def force_update_summary(user_id: str):
    """
    –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å —Ä–µ–∑—é–º–µ –¥–∏–∞–ª–æ–≥–∞.
    """
    try:
        memory = get_conversation_memory(user_id)
        
        if len(memory.turns) < 5:
            raise HTTPException(
                status_code=400,
                detail="–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ö–æ–¥–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑—é–º–µ (–º–∏–Ω–∏–º—É–º 5)"
            )
        
        memory._update_summary()
        
        return {
            "success": True,
            "summary": memory.summary,
            "updated_at_turn": memory.summary_updated_at,
            "summary_length": len(memory.summary) if memory.summary else 0
        }
        
    except Exception as e:
        logger.error(f"Error updating summary: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

***

## üöÄ –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –∏ –∑–∞–ø—É—Å–∫—É

### **–®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏**

```bash
cd bot_psychologist

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å sentence-transformers
pip install sentence-transformers torch

# –ò–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install -r requirements_bot.txt
```

### **–®–∞–≥ 2: –ù–∞—Å—Ç—Ä–æ–∏—Ç—å .env**

```bash
# –°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä
cp .env.example .env

# –û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å .env
nano .env
```

–î–æ–±–∞–≤–∏—Ç—å/–ø—Ä–æ–≤–µ—Ä–∏—Ç—å:
```env
# Semantic Memory
ENABLE_SEMANTIC_MEMORY=true
SEMANTIC_SEARCH_TOP_K=3
SEMANTIC_MIN_SIMILARITY=0.7
SEMANTIC_MAX_CHARS=1000
EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2

# Conversation Summary
ENABLE_CONVERSATION_SUMMARY=true
SUMMARY_UPDATE_INTERVAL=5
SUMMARY_MAX_CHARS=500
```

### **–®–∞–≥ 3: –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã**

```bash
# –¢–µ—Å—Ç semantic memory
python test_semantic_memory.py
```

–û–∂–∏–¥–∞–µ–º—ã–π –≤—ã–≤–æ–¥:
```
‚öôÔ∏è Semantic Memory: ‚úì –í–∫–ª—é—á–µ–Ω–∞
‚öôÔ∏è Conversation Summary: ‚úì –í–∫–ª—é—á–µ–Ω–∞
‚öôÔ∏è Embedding Model: paraphrase-multilingual-MiniLM-L12-v2

============================================================
–¢–ï–°–¢ SEMANTIC MEMORY
============================================================

‚úì –ü–∞–º—è—Ç—å —Å–æ–∑–¥–∞–Ω–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: test_semantic_user

üìù –î–æ–±–∞–≤–ª—è—é 5 —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ö–æ–¥–æ–≤...
  ‚úì –î–æ–±–∞–≤–ª–µ–Ω: –ß—Ç–æ —Ç–∞–∫–æ–µ –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ?...
  ‚úì –î–æ–±–∞–≤–ª–µ–Ω: –ö–∞–∫ –º–µ–¥–∏—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ?...
  ...

‚úÖ –í—Å–µ–≥–æ —Ö–æ–¥–æ–≤ –≤ –ø–∞–º—è—Ç–∏: 5

üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Semantic Memory:
  ‚Ä¢ –≠–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: 5
  ‚Ä¢ –ú–æ–¥–µ–ª—å: paraphrase-multilingual-MiniLM-L12-v2
  ‚Ä¢ –†–∞–∑–º–µ—Ä –Ω–∞ –¥–∏—Å–∫–µ: 0.01 MB

============================================================
–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï SEMANTIC SEARCH
============================================================

üîç –ó–∞–ø—Ä–æ—Å: "–ö–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø—Ä–∞–∫—Ç–∏–∫–æ–≤–∞—Ç—å –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å?"
------------------------------------------------------------

‚úÖ –ù–∞–π–¥–µ–Ω—ã —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–æ—à–ª—ã–µ –æ–±–º–µ–Ω—ã:
–†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –ü–†–û–®–õ–´–ï –û–ë–ú–ï–ù–´:

[–°—Ö–æ–¥—Å—Ç–≤–æ: 0.85] –û–±–º–µ–Ω #1:
  –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: –ß—Ç–æ —Ç–∞–∫–æ–µ –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ?
  –ë–æ—Ç: –û—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ ‚Äî —ç—Ç–æ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –Ω–∞–±–ª—é–¥–∞—Ç—å –∑–∞ —Å–≤–æ–∏–º–∏ –º—ã—Å–ª—è–º–∏...
  ...
```

### **–®–∞–≥ 4: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ API**

```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç—å API —Å–µ—Ä–≤–µ—Ä
cd api
uvicorn main:app --reload --port 8000
```

–¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã:
```bash
# 1. –ó–∞–¥–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏
curl -X POST "http://localhost:8000/api/v1/questions/basic-with-semantic" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ?",
    "user_id": "demo_user",
    "top_k": 5
  }'

curl -X POST "http://localhost:8000/api/v1/questions/basic-with-semantic" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "–ö–∞–∫ –º–µ–¥–∏—Ç–∏—Ä–æ–≤–∞—Ç—å?",
    "user_id": "demo_user",
    "top_k": 5
  }'

# 2. –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –ø–æ—Ö–æ–∂–∏–π –Ω–∞ –ø–µ—Ä–≤—ã–π (–¥–æ–ª–∂–µ–Ω –Ω–∞–π—Ç–∏ —á–µ—Ä–µ–∑ semantic search)
curl -X POST "http://localhost:8000/api/v1/questions/basic-with-semantic" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "–†–∞—Å—Å–∫–∞–∂–∏ –ø–æ–¥—Ä–æ–±–Ω–µ–µ –ø—Ä–æ –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å",
    "user_id": "demo_user",
    "top_k": 5
  }'

# 3. –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É semantic memory
curl "http://localhost:8000/api/v1/users/demo_user/semantic-stats"

# 4. –ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é
curl "http://localhost:8000/api/v1/users/demo_user/history?last_n_turns=10"
```

***

## üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
bot_psychologist/
‚îú‚îÄ‚îÄ bot_agent/
‚îÇ   ‚îú‚îÄ‚îÄ config.py                    # ‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω (–¥–æ–±–∞–≤–ª–µ–Ω—ã –Ω–∞—Å—Ç—Ä–æ–π–∫–∏)
‚îÇ   ‚îú‚îÄ‚îÄ conversation_memory.py       # ‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω (semantic + summary)
‚îÇ   ‚îú‚îÄ‚îÄ semantic_memory.py           # üÜï –ù–û–í–´–ô –§–ê–ô–õ
‚îÇ   ‚îú‚îÄ‚îÄ answer_basic.py              # ‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω (–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è semantic)
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ routes.py                    # ‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω (–Ω–æ–≤—ã–µ endpoints)
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ .cache_bot_agent/
‚îÇ   ‚îú‚îÄ‚îÄ conversations/               # –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–æ–≤
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ {user_id}.json
‚îÇ   ‚îî‚îÄ‚îÄ semantic_memory/             # üÜï –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
‚îÇ       ‚îú‚îÄ‚îÄ {user_id}_embeddings.npz
‚îÇ       ‚îî‚îÄ‚îÄ {user_id}_metadata.json
‚îÇ
‚îú‚îÄ‚îÄ test_semantic_memory.py          # üÜï –ù–û–í–´–ô –¢–ï–°–¢
‚îú‚îÄ‚îÄ requirements_bot.txt             # ‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω (sentence-transformers)
‚îî‚îÄ‚îÄ .env.example                     # ‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω (semantic –Ω–∞—Å—Ç—Ä–æ–π–∫–∏)
```

***

## ‚úÖ –ß—Ç–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ

### **1. Semantic Memory:**
- ‚úÖ –í–µ–∫—Ç–æ—Ä–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —Ö–æ–¥–æ–≤
- ‚úÖ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É
- ‚úÖ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–∞ –¥–∏—Å–∫–µ
- ‚úÖ Lazy loading –º–æ–¥–µ–ª–∏
- ‚úÖ Batch –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

### **2. Conversation Summary:**
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∑—é–º–µ –∫–∞–∂–¥—ã–µ N —Ö–æ–¥–æ–≤
- ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—é–º–µ –≤ –∏—Å—Ç–æ—Ä–∏–∏
- ‚úÖ –í–∫–ª—é—á–µ–Ω–∏–µ —Ä–µ–∑—é–º–µ –≤ –ø—Ä–æ–º–ø—Ç

### **3. –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞:**
- ‚úÖ –ö–æ—Ä–æ—Ç–∫–∏–µ –¥–∏–∞–ª–æ–≥–∏ (1-5): —Ç–æ–ª—å–∫–æ short-term
- ‚úÖ –°—Ä–µ–¥–Ω–∏–µ (6-20): short-term + semantic
- ‚úÖ –î–ª–∏–Ω–Ω—ã–µ (21+): short-term + semantic + summary

### **4. API endpoints:**
- ‚úÖ `/questions/basic-with-semantic` ‚Äî –≤–æ–ø—Ä–æ—Å—ã —Å semantic memory
- ‚úÖ `/users/{user_id}/semantic-stats` ‚Äî —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
- ‚úÖ `/users/{user_id}/rebuild-semantic-memory` ‚Äî –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ
- ‚úÖ `/users/{user_id}/update-summary` ‚Äî –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—é–º–µ

### **5. –¢–µ—Å—Ç—ã:**
- ‚úÖ `test_semantic_memory.py` ‚Äî –ø–æ–ª–Ω—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç

***

## üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç

–¢–µ–ø–µ—Ä—å –≤–∞—à –±–æ—Ç:

1. **–ü–æ–º–Ω–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç** ‚Äî –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3-5 —Ö–æ–¥–æ–≤ –≤—Å–µ–≥–¥–∞ –≤ –ø–∞–º—è—Ç–∏
2. **–ù–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–µ –∏–∑ –ø—Ä–æ—à–ª–æ–≥–æ** ‚Äî semantic search –ø–æ –≤—Å–µ–π –∏—Å—Ç–æ—Ä–∏–∏
3. **–ü–æ–Ω–∏–º–∞–µ—Ç –æ–±—â—É—é –∫–∞—Ä—Ç–∏–Ω—É** ‚Äî –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –¥–∏–∞–ª–æ–≥–∞
4. **–ù–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã** ‚Äî –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (3500 —Å–∏–º–≤–æ–ª–æ–≤ –º–∞–∫—Å)
5. **–†–∞–±–æ—Ç–∞–µ—Ç –æ—Ñ—Ñ–ª–∞–π–Ω** ‚Äî sentence-transformers –ª–æ–∫–∞–ª—å–Ω–æ

–í—Å—ë –≥–æ—Ç–æ–≤–æ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é! üöÄ