# bot_agent/llm_answerer.py
"""
LLM Answerer Module
===================

–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ —á–µ—Ä–µ–∑ OpenAI API —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º –±–æ—Ç–∞-–ø—Å–∏—Ö–æ–ª–æ–≥–∞.
"""

import logging
from typing import List, Dict, Optional
from pathlib import Path

from .data_loader import Block
from .config import config

logger = logging.getLogger(__name__)

_PROMPT_BASE_PATH = Path(__file__).resolve().parent / "prompt_system_base.md"


def _read_prompt_text(path: Path) -> str:
    """
    Read UTF-8 prompt text from disk.
    We allow BOM so the file can be edited in Windows tools comfortably.
    """
    try:
        text = path.read_text(encoding="utf-8")
    except UnicodeDecodeError:
        text = path.read_text(encoding="utf-8-sig")
    return text.lstrip("\ufeff").strip()


class LLMAnswerer:
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è OpenAI API.
    
    Usage:
        >>> answerer = LLMAnswerer()
        >>> result = answerer.generate_answer("–ß—Ç–æ —Ç–∞–∫–æ–µ –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ?", blocks)
        >>> print(result["answer"])
    """
    
    def __init__(self):
        self.api_key = config.OPENAI_API_KEY
        self.client = None
        
        if not self.api_key:
            logger.warning("‚ö†Ô∏è OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. LLM –æ—Ç–≤–µ—Ç—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.")
        else:
            self._init_client()
    
    def _init_client(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI –∫–ª–∏–µ–Ω—Ç–∞"""
        try:
            import openai
            self.client = openai.OpenAI(api_key=self.api_key)
            logger.info("‚úì OpenAI –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except ImportError:
            logger.error("‚ùå openai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai")
            raise
    
    def build_system_prompt(self) -> str:
        """
        –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –±–æ—Ç–∞-–ø—Å–∏—Ö–æ–ª–æ–≥–∞.
        
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ, —Ç–æ–Ω –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –±–æ—Ç–∞.
        """
        try:
            return _read_prompt_text(_PROMPT_BASE_PATH)
        except FileNotFoundError:
            logger.warning(f"‚ö†Ô∏è System prompt file not found: {_PROMPT_BASE_PATH}. Falling back to –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–º—É –ø—Ä–æ–º–ø—Ç—É.")
            return (
                "–¢—ã ‚Äî —Å–ø–æ–∫–æ–π–Ω—ã–π –∏ —Ç–æ—á–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫.\n"
                "–û—Ç–≤–µ—á–∞–π –ø–æ-—Ä—É—Å—Å–∫–∏. –û–ø–∏—Ä–∞–π—Å—è –Ω–∞ –º–∞—Ç–µ—Ä–∏–∞–ª—ã, –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. "
                "–ï—Å–ª–∏ –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞ ‚Äî —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º –∏ –ø–æ–ø—Ä–æ—Å–∏ —É—Ç–æ—á–Ω–µ–Ω–∏–µ."
            )
    
    def build_context_prompt(
        self,
        blocks: List[Block],
        user_question: str,
        conversation_history: Optional[str] = None
    ) -> str:
        """
        –§–æ—Ä–º–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM: –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –±–ª–æ–∫–∏ + –≤–æ–ø—Ä–æ—Å.
        
        Args:
            blocks: –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –±–ª–æ–∫–æ–≤
            user_question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            conversation_history: –ò—Å—Ç–æ—Ä–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö N –æ–±–º–µ–Ω–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
        """
        context = ""

        if conversation_history:
            context += conversation_history.strip() + "\n\n"

        context += "–ú–ê–¢–ï–†–ò–ê–õ –ò–ó –õ–ï–ö–¶–ò–ô:\n\n"
        
        for i, block in enumerate(blocks, 1):
            context += f"--- –ë–õ–û–ö {i} ---\n"
            context += f"–õ–µ–∫—Ü–∏—è: {block.document_title}\n"
            context += f"–¢–µ–º–∞: {block.title}\n"
            context += f"–¢–∞–π–º–∫–æ–¥: {block.start} ‚Äî {block.end}\n"
            context += f"–°—Å—ã–ª–∫–∞: {block.youtube_link}\n"
            context += f"–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ: {block.summary}\n"
            context += f"–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç:\n{block.content}\n\n"
        
        context += f"–í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:\n{user_question}\n\n"
        context += "–°—Ñ–æ—Ä–º–∏—Ä—É–π –æ—Ç–≤–µ—Ç, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –º–∞—Ç–µ—Ä–∏–∞–ª –≤—ã—à–µ."
        
        return context
    
    def generate_answer(
        self,
        user_question: str,
        blocks: List[Block],
        conversation_history: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> Dict:
        """
        –§–æ—Ä–º–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ OpenAI API.
        
        Args:
            user_question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            blocks: –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –±–ª–æ–∫–æ–≤
            model: –ú–æ–¥–µ–ª—å LLM (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ config)
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
            
        Returns:
            Dict —Å –∫–ª—é—á–∞–º–∏:
                - answer: str ‚Äî –≥–æ—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç
                - model_used: str ‚Äî –∫–∞–∫—É—é –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏
                - tokens_used: int ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
                - error: Optional[str] ‚Äî –µ—Å–ª–∏ –±—ã–ª–∞ –æ—à–∏–±–∫–∞
        """
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª—É—á–∞—è –±–µ–∑ –±–ª–æ–∫–æ–≤
        if not blocks:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç –±–ª–æ–∫–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞!")
            return {
                "answer": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à—ë–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞ –¥–ª—è —ç—Ç–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å.",
                "model_used": None,
                "tokens_used": 0,
                "error": "no_blocks"
            }
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª–∏–µ–Ω—Ç–∞
        if not self.client:
            return {
                "answer": "‚ùå OpenAI API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ OPENAI_API_KEY –≤ .env",
                "model_used": None,
                "tokens_used": 0,
                "error": "no_api_key"
            }
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
        model = model or config.LLM_MODEL
        temperature = temperature if temperature is not None else config.LLM_TEMPERATURE
        max_tokens = max_tokens or config.LLM_MAX_TOKENS
        
        # –ü—Ä–æ–º–ø—Ç—ã
        system_prompt = self.build_system_prompt()
        context = self.build_context_prompt(
            blocks,
            user_question,
            conversation_history=conversation_history
        )
        
        logger.debug(f"üì§ –û—Ç–ø—Ä–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å –∫ {model}...")
        
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": context}
                ],
                temperature=temperature,
                max_tokens=max_tokens,
            )
            
            answer = response.choices[0].message.content
            tokens = response.usage.total_tokens if response.usage else 0
            
            logger.debug(f"‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω ({tokens} —Ç–æ–∫–µ–Ω–æ–≤)")
            
            return {
                "answer": answer,
                "model_used": model,
                "tokens_used": tokens,
                "error": None
            }
        
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ OpenAI API: {e}")
            return {
                "answer": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}",
                "model_used": model,
                "tokens_used": 0,
                "error": str(e)
            }



