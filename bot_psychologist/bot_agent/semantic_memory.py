# bot_agent/semantic_memory.py
"""
Semantic Memory Module
======================

–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
–ü–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–æ—à–ª—ã–µ –æ–±–º–µ–Ω—ã –ø–æ —Å–º—ã—Å–ª—É, –∞ –Ω–µ –ø–æ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏–∏.
"""

import logging
import json
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np

from .config import config

logger = logging.getLogger(__name__)


@dataclass
class TurnEmbedding:
    """–≠–º–±–µ–¥–¥–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ —Ö–æ–¥–∞ –¥–∏–∞–ª–æ–≥–∞."""

    turn_index: int
    user_input: str
    bot_response_preview: str
    user_state: Optional[str]
    concepts: List[str]
    timestamp: str
    embedding: np.ndarray


class SemanticMemory:
    """
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç sentence-transformers –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π
    –ø—Ä–æ—à–ª—ã—Ö –æ–±–º–µ–Ω–æ–≤ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞.
    """

    def __init__(self, user_id: str = "default"):
        self.user_id = user_id
        self.turn_embeddings: List[TurnEmbedding] = []

        self._model = None
        self._model_loaded = False

        self.cache_dir = config.CACHE_DIR / "semantic_memory"
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        self.embeddings_file = self.cache_dir / f"{user_id}_embeddings.npz"
        self.metadata_file = self.cache_dir / f"{user_id}_metadata.json"

        logger.debug(f"üì¶ SemanticMemory —Å–æ–∑–¥–∞–Ω –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_id}")

    @property
    def model(self):
        """Lazy loading –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."""
        if not self._model_loaded:
            self._load_model()
        return self._model

    def _load_model(self) -> None:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å sentence-transformers."""
        try:
            from sentence_transformers import SentenceTransformer

            model_name = config.EMBEDDING_MODEL
            logger.info(f"ü§ñ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {model_name}")

            self._model = SentenceTransformer(model_name)
            self._model_loaded = True

            logger.info("‚úÖ –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except ImportError as exc:
            logger.error(
                "‚ùå sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. "
                "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sentence-transformers"
            )
            raise exc
        except Exception as exc:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {exc}")
            raise

    def add_turn_embedding(
        self,
        turn_index: int,
        user_input: str,
        bot_response: Optional[str],
        user_state: Optional[str],
        concepts: List[str],
        timestamp: str,
    ) -> None:
        """
        –°–æ–∑–¥–∞—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ö–æ–¥–∞.

        Args:
            turn_index: –ò–Ω–¥–µ–∫—Å —Ö–æ–¥–∞ (–Ω–∞—á–∏–Ω–∞—è —Å 1)
            user_input: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            bot_response: –û—Ç–≤–µ—Ç –±–æ—Ç–∞
            user_state: –°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            concepts: –°–ø–∏—Å–æ–∫ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
            timestamp: –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞
        """
        response_preview = bot_response[:200] if bot_response else ""
        text_to_embed = f"{user_input} {response_preview}"

        try:
            embedding = self.model.encode(
                text_to_embed,
                show_progress_bar=False,
                convert_to_numpy=True,
            )
        except Exception as exc:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {exc}")
            return

        turn_emb = TurnEmbedding(
            turn_index=turn_index,
            user_input=user_input,
            bot_response_preview=response_preview,
            user_state=user_state,
            concepts=concepts,
            timestamp=timestamp,
            embedding=embedding,
        )
        self.turn_embeddings.append(turn_emb)
        logger.debug(f"‚ûï –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–æ–±–∞–≤–ª–µ–Ω –¥–ª—è —Ö–æ–¥–∞ #{turn_index}")

    def search_similar_turns(
        self,
        query: str,
        top_k: int = 3,
        min_similarity: float = 0.7,
        exclude_last_n: int = 5,
    ) -> List[Tuple[TurnEmbedding, float]]:
        """
        –ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ –ø—Ä–æ—à–ª—ã–µ –æ–±–º–µ–Ω—ã –ø–æ —Å–µ–º–∞–Ω—Ç–∏–∫–µ.

        Args:
            query: –¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            min_similarity: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (0-1)
            exclude_last_n: –ò—Å–∫–ª—é—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Ö–æ–¥–æ–≤ (–æ–Ω–∏ —É–∂–µ –≤ short-term)
        """
        if not self.turn_embeddings:
            logger.debug("üîç –ù–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞")
            return []

        try:
            query_embedding = self.model.encode(
                query,
                show_progress_bar=False,
                convert_to_numpy=True,
            )
        except Exception as exc:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞: {exc}")
            return []

        search_pool = (
            self.turn_embeddings[:-exclude_last_n]
            if exclude_last_n > 0
            else self.turn_embeddings
        )

        similarities: List[Tuple[TurnEmbedding, float]] = []
        for turn_emb in search_pool:
            similarity = self._cosine_similarity(query_embedding, turn_emb.embedding)
            if similarity >= min_similarity:
                similarities.append((turn_emb, float(similarity)))

        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]

    def get_context_for_llm(
        self,
        query: str,
        max_chars: int = 1000,
        top_k: int = 3,
        min_similarity: float = 0.7,
    ) -> str:
        """
        –ü–æ–ª—É—á–∏—Ç—å –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–æ—à–ª—ã—Ö –æ–±–º–µ–Ω–æ–≤.
        """
        similar = self.search_similar_turns(
            query=query,
            top_k=top_k,
            min_similarity=min_similarity,
        )

        if not similar:
            return ""

        context = "–†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –ü–†–û–®–õ–´–ï –û–ë–ú–ï–ù–´:\n\n"
        current_len = len(context)

        for turn_emb, score in similar:
            entry = (
                f"[–°—Ö–æ–¥—Å—Ç–≤–æ: {score:.2f}] –û–±–º–µ–Ω #{turn_emb.turn_index}:\n"
                f"  –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {turn_emb.user_input}\n"
                f"  –ë–æ—Ç: {turn_emb.bot_response_preview}"
            )
            if len(turn_emb.bot_response_preview) == 200:
                entry += "..."
            entry += "\n"

            if turn_emb.user_state:
                entry += f"  –°–æ—Å—Ç–æ—è–Ω–∏–µ: {turn_emb.user_state}\n"
            if turn_emb.concepts:
                entry += f"  –ö–æ–Ω—Ü–µ–ø—Ç—ã: {', '.join(turn_emb.concepts[:3])}\n"
            entry += "\n"

            if current_len + len(entry) > max_chars:
                if current_len > len("–†–ï–õ–ï–í–ê–ù–¢–ù–´–ï –ü–†–û–®–õ–´–ï –û–ë–ú–ï–ù–´:\n\n"):
                    break
                allowed = max_chars - current_len
                entry = entry[:max(0, allowed - 3)] + "..."
                if entry:
                    context += entry
                break

            context += entry
            current_len += len(entry)

        return context

    @staticmethod
    def _cosine_similarity(vec_a: np.ndarray, vec_b: np.ndarray) -> float:
        """–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏."""
        dot_product = np.dot(vec_a, vec_b)
        norm_a = np.linalg.norm(vec_a)
        norm_b = np.linalg.norm(vec_b)
        if norm_a == 0 or norm_b == 0:
            return 0.0
        return float(dot_product / (norm_a * norm_b))

    def save_to_disk(self) -> None:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–∞ –¥–∏—Å–∫."""
        if not self.turn_embeddings:
            logger.debug("‚ö†Ô∏è –ù–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return

        try:
            embeddings_array = np.array(
                [turn_emb.embedding for turn_emb in self.turn_embeddings]
            )
            np.savez_compressed(self.embeddings_file, embeddings=embeddings_array)

            metadata = [
                {
                    "turn_index": turn_emb.turn_index,
                    "user_input": turn_emb.user_input,
                    "bot_response_preview": turn_emb.bot_response_preview,
                    "user_state": turn_emb.user_state,
                    "concepts": turn_emb.concepts,
                    "timestamp": turn_emb.timestamp,
                }
                for turn_emb in self.turn_embeddings
            ]

            with open(self.metadata_file, "w", encoding="utf-8") as file:
                json.dump(metadata, file, ensure_ascii=False, indent=2)

            logger.debug(
                f"üíæ Semantic memory —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {len(self.turn_embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"
            )
        except Exception as exc:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è semantic memory: {exc}")

    def load_from_disk(self) -> bool:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –¥–∏—Å–∫–∞.

        Returns:
            True –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞, False –µ—Å–ª–∏ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã
        """
        if not self.embeddings_file.exists() or not self.metadata_file.exists():
            logger.debug(f"üìã –ù–æ–≤–∞—è semantic memory –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {self.user_id}")
            return False

        try:
            data = np.load(self.embeddings_file)
            embeddings_array = data["embeddings"]

            with open(self.metadata_file, "r", encoding="utf-8") as file:
                metadata_list = json.load(file)

            self.turn_embeddings = []
            for i, meta in enumerate(metadata_list):
                self.turn_embeddings.append(
                    TurnEmbedding(
                        turn_index=meta["turn_index"],
                        user_input=meta["user_input"],
                        bot_response_preview=meta["bot_response_preview"],
                        user_state=meta.get("user_state"),
                        concepts=meta.get("concepts", []),
                        timestamp=meta["timestamp"],
                        embedding=embeddings_array[i],
                    )
                )

            logger.info(
                f"‚úÖ Semantic memory –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(self.turn_embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"
            )
            return True
        except Exception as exc:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ semantic memory: {exc}")
            return False

    def rebuild_all_embeddings(self, turns_data: List[Dict]) -> None:
        """
        –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ batch'–µ–º.
        """
        if not turns_data:
            return

        logger.info(f"üî® –ü–µ—Ä–µ—Å–æ–∑–¥–∞—é {len(turns_data)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")

        try:
            texts: List[str] = []
            for turn in turns_data:
                response_preview = (
                    turn.get("bot_response", "")[:200] if turn.get("bot_response") else ""
                )
                texts.append(f"{turn['user_input']} {response_preview}")

            embeddings = self.model.encode(
                texts,
                batch_size=32,
                show_progress_bar=True,
                convert_to_numpy=True,
            )

            self.turn_embeddings = []
            for i, turn in enumerate(turns_data):
                response_preview = (
                    turn.get("bot_response", "")[:200] if turn.get("bot_response") else ""
                )
                self.turn_embeddings.append(
                    TurnEmbedding(
                        turn_index=i + 1,
                        user_input=turn["user_input"],
                        bot_response_preview=response_preview,
                        user_state=turn.get("user_state"),
                        concepts=turn.get("concepts", []),
                        timestamp=turn.get("timestamp", ""),
                        embedding=embeddings[i],
                    )
                )

            self.save_to_disk()
            logger.info(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω—ã: {len(self.turn_embeddings)}")
        except Exception as exc:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {exc}")

    def clear(self) -> None:
        """–û—á–∏—Å—Ç–∏—Ç—å semantic memory."""
        self.turn_embeddings = []
        if self.embeddings_file.exists():
            self.embeddings_file.unlink()
        if self.metadata_file.exists():
            self.metadata_file.unlink()
        logger.info("üóëÔ∏è Semantic memory –æ—á–∏—â–µ–Ω–∞")

    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É semantic memory."""
        return {
            "total_embeddings": len(self.turn_embeddings),
            "model_loaded": self._model_loaded,
            "model_name": config.EMBEDDING_MODEL,
            "cache_dir": str(self.cache_dir),
            "embeddings_size_mb": (
                self.embeddings_file.stat().st_size / (1024 * 1024)
                if self.embeddings_file.exists()
                else 0
            ),
        }


_semantic_memory_instances: Dict[str, SemanticMemory] = {}


def get_semantic_memory(user_id: str = "default") -> SemanticMemory:
    """
    –ü–æ–ª—É—á–∏—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä semantic memory –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (singleton).
    """
    if user_id not in _semantic_memory_instances:
        semantic_mem = SemanticMemory(user_id)
        semantic_mem.load_from_disk()
        _semantic_memory_instances[user_id] = semantic_mem
    return _semantic_memory_instances[user_id]

