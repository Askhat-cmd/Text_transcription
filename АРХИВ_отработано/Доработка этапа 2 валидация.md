<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# –¥–∞, –º–Ω–µ —Ç–µ–ø–µ—Ä—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –∞–≥–µ–Ω—Ç—É –∫—É—Ä—Å–æ—Ä–∞ —Ç–≤–æ–π —Ç–µ–∫—Å—Ç –≤—ã—à–µ?

–î–∞! –ù–æ –¥–∞–≤–∞–π—Ç–µ —è **–æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é** –∫–æ–º–∞–Ω–¥—É —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è Cursor AI - —Å–¥–µ–ª–∞—é –µ—ë –±–æ–ª–µ–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏ –ø–æ—à–∞–≥–æ–≤–æ–π.

***

# üéØ –ö–û–ú–ê–ù–î–ê –î–õ–Ø CURSOR AI (–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø)

–°–∫–æ–ø–∏—Ä—É–π—Ç–µ —ç—Ç–æ **—Ü–µ–ª–∏–∫–æ–º** –≤ Cursor:

```
TASK: Implement SMART validation mode (highly recommended for production)

PRIORITY: HIGH - affects content retention (from 40% to 95%)

=============================================================
CONTEXT & REASONING
=============================================================

Problem:
- Current strict filter blocks 60% of Sarsekenov's lectures
- Sarsekenov uses mixed terminology (his own + general psychology)
- Example: "–õ—é–¥–∏ –≥–æ–≤–æ—Ä—è—Ç '—ç–≥–æ', –Ω–æ —è –∏–º–µ—é –≤ –≤–∏–¥—É –Ø-–æ–±—Ä–∞–∑"
- This valuable explanatory content is currently BLOCKED

Solution:
- SMART mode: Only check Sarsekenov term density ‚â•15%
- Ignore "forbidden terms" entirely (trust the smart bot downstream)
- Let GPT-4/Claude filter and reformulate during response generation

Expected result:
- 95% content retention instead of 40%
- Natural lecture style preserved
- Smart bot handles terminology translation

=============================================================
IMPLEMENTATION STEPS
=============================================================

STEP 1: Install python-dotenv
------------------------------
pip install python-dotenv


STEP 2: Create .env file in project root
-----------------------------------------
Create file: .env

Content:
```


# ============================================================

# VALIDATION MODE CONFIGURATION

# ============================================================

# Validation modes:

# smart (RECOMMENDED) - pass all content with density ‚â•15%

# ‚úÖ Maximum content retention (95%)

# ‚úÖ Natural lecturer style (mixed terminology)

# ‚úÖ Smart bot filters on output

# soft - pass forbidden terms in explanatory context

# ‚ö†Ô∏è  Complex contextual logic

# ‚ö†Ô∏è  May pass less content

# strict - block any forbidden terms

# ‚ùå Too strict for real lectures

# ‚ùå ~60% content loss

# off - only density check, no other restrictions

# ‚ö†Ô∏è  May pass some irrelevant content

VALIDATION_MODE=smart

# Minimum Sarsekenov term density for SMART/OFF modes

# 0.15 = 15% - optimal balance

# 0.10 = 10% - softer (may pass more noise)

# 0.20 = 20% - stricter (may filter useful content)

MIN_SARSEKENOV_DENSITY_SMART=0.15

# Minimum density for STRICT/SOFT modes

MIN_SARSEKENOV_DENSITY_STRICT=0.25

# Density threshold for contextual usage (SOFT mode)

CONTEXTUAL_DENSITY_THRESHOLD=0.35

```


STEP 3: Update terminology_validator.py
----------------------------------------
File: text_processor/validators/terminology_validator.py

Changes to __init__() method:

Add at the beginning of __init__():
```

from dotenv import load_dotenv
import os

# Load environment variables

load_dotenv()

# Load validation settings from .env

self.validation_mode = os.getenv('VALIDATION_MODE', 'smart')
self.min_density_smart = float(os.getenv('MIN_SARSEKENOV_DENSITY_SMART', '0.15'))
self.min_density_strict = float(os.getenv('MIN_SARSEKENOV_DENSITY_STRICT', '0.25'))
self.contextual_threshold = float(os.getenv('CONTEXTUAL_DENSITY_THRESHOLD', '0.35'))

logger.info(f"Validation mode: {self.validation_mode}")
logger.info(f"Min density (smart): {self.min_density_smart:.0%}")
logger.info(f"Min density (strict): {self.min_density_strict:.0%}")

```

Update validate_text() method signature:
```

def validate_text(
self,
text: str,
min_density: Optional[float] = None,
validation_mode: Optional[str] = None
) -> ValidationResult:

```

Replace ENTIRE validate_text() implementation with this logic:

```


# Determine validation mode

mode = validation_mode or self.validation_mode

# Determine minimum density based on mode

if min_density is None:
if mode in ["smart", "off"]:
min_density = self.min_density_smart  \# 0.15
else:
min_density = self.min_density_strict  \# 0.25

logger.debug(f"Validating text with mode={mode}, min_density={min_density:.0%}")

# STEP 1: Calculate Sarsekenov term density (ALWAYS)

density_metrics = self._calculate_density(text)

# STEP 2: Check minimum density (ALWAYS)

if density_metrics['density'] < min_density:
logger.info(f"Text rejected: density {density_metrics['density']:.1%} < {min_density:.1%}")
return ValidationResult(
is_valid=False,
reason=f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω–æ–≤ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞: {density_metrics['density']:.1%} < {min_density:.1%}",
metrics=density_metrics,
forbidden_terms_found=[],
sarsekenov_entities=[],
is_contextual=False
)

# STEP 3: Extract entities

entities = self._extract_entities(text)

# STEP 4: Check forbidden terms (MODE DEPENDENT)

forbidden_found = self._find_forbidden_terms(text)

# MODE: SMART or OFF

if mode in ["smart", "off"]:
\# SMART/OFF: Ignore forbidden terms entirely
logger.info(f"Text accepted (mode={mode}): density {density_metrics['density']:.1%}, forbidden terms ignored")
return ValidationResult(
is_valid=True,
reason=f"‚úÖ –í–∞–ª–∏–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞ (–ø–ª–æ—Ç–Ω–æ—Å—Ç—å: {density_metrics['density']:.1%}, —Ä–µ–∂–∏–º: {mode})",
metrics=density_metrics,
forbidden_terms_found=forbidden_found,  \# Store for statistics only
sarsekenov_entities=entities,
is_contextual=False
)

# MODE: STRICT

elif mode == "strict":
if forbidden_found:
logger.warning(f"Text rejected (strict): forbidden terms found: {forbidden_found}")
return ValidationResult(
is_valid=False,
reason=f"‚ùå –ù–∞–π–¥–µ–Ω—ã –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã: {', '.join(forbidden_found)}",
metrics=density_metrics,
forbidden_terms_found=forbidden_found,
sarsekenov_entities=entities,
is_contextual=False
)

# MODE: SOFT

elif mode == "soft":
if forbidden_found:
is_contextual = self._is_contextual_usage(
text,
forbidden_found,
density_metrics
)

        if not is_contextual:
            logger.warning(f"Text rejected (soft): forbidden terms without context")
            return ValidationResult(
                is_valid=False,
                reason=f"‚ùå –ù–∞–π–¥–µ–Ω—ã –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –≤–Ω–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {', '.join(forbidden_found)}",
                metrics=density_metrics,
                forbidden_terms_found=forbidden_found,
                sarsekenov_entities=entities,
                is_contextual=False
            )
        else:
            logger.info(f"Text accepted (soft): forbidden terms in explanatory context")
            return ValidationResult(
                is_valid=True,
                reason=f"‚úÖ –í–∞–ª–∏–¥–Ω—ã–π —Ç–µ–∫—Å—Ç (forbidden terms –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è)",
                metrics=density_metrics,
                forbidden_terms_found=forbidden_found,
                sarsekenov_entities=entities,
                is_contextual=True
            )
    
# Default: Success

logger.info(f"Text accepted: density {density_metrics['density']:.1%}")
return ValidationResult(
is_valid=True,
reason=f"‚úÖ –í–∞–ª–∏–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞ (–ø–ª–æ—Ç–Ω–æ—Å—Ç—å: {density_metrics['density']:.1%})",
metrics=density_metrics,
forbidden_terms_found=forbidden_found,
sarsekenov_entities=entities,
is_contextual=False
)

```

Add _is_contextual_usage() method (for SOFT mode):

```

def _is_contextual_usage(
self,
text: str,
forbidden_terms: List[str],
density_metrics: Dict
) -> bool:
"""
Check if forbidden terms are used in explanatory context.

    Contextual criteria:
    1. High Sarsekenov term density (‚â•35%)
    2. Replacement terms near forbidden terms
    3. Explanation markers present
    
    Returns:
        True if contextual/explanatory, False otherwise
    """
    
    # Criterion 1: High density = likely explanatory
    if density_metrics['density'] >= self.contextual_threshold:
        return True
    
    # Criterion 2: Replacement terms nearby
    replacements = self.forbidden_config.get('replacements', {})
    
    for forbidden in forbidden_terms:
        replacement = replacements.get(forbidden)
        if replacement and replacement.lower() in text.lower():
            # Check proximity (within 100 chars)
            forbidden_pos = text.lower().find(forbidden.lower())
            replacement_pos = text.lower().find(replacement.lower())
            
            if abs(forbidden_pos - replacement_pos) < 100:
                return True
    
    # Criterion 3: Explanation markers
    explanation_markers = [
        "–∏–º–µ—é –≤ –≤–∏–¥—É", "–Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ", "—ç—Ç–æ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è",
        "–≤–º–µ—Å—Ç–æ", "–∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞", "–ø—Ä–∞–≤–∏–ª—å–Ω–µ–µ –≥–æ–≤–æ—Ä–∏—Ç—å",
        "–Ω–µ", "–æ—Ç–ª–∏—á–∏–µ", "—Ä–∞–∑–Ω–∏—Ü–∞", "–æ–±—ä—è—Å–Ω—è—é"
    ]
    
    text_lower = text.lower()
    for marker in explanation_markers:
        if marker in text_lower:
            # Check marker proximity to forbidden term
            for forbidden in forbidden_terms:
                marker_pos = text_lower.find(marker)
                forbidden_pos = text_lower.find(forbidden.lower())
                
                if abs(marker_pos - forbidden_pos) < 50:
                    return True
    
    return False
    ```


STEP 4: Update config/extractors_config.yaml
---------------------------------------------
File: config/extractors_config.yaml

Update terminology section:
```

terminology:

# Validation mode (smart recommended for production)

validation_mode: smart

# Density thresholds by mode

min_density_smart: 0.15      \# for smart/off modes
min_density_strict: 0.25     \# for strict/soft modes
contextual_threshold: 0.35   \# for soft mode contextual checks

# Legacy parameters (kept for compatibility)

strict_mode: false
allow_mixed_content: true
case_sensitive: false

```


STEP 5: Add new tests
---------------------
File: tests/extractors/test_terminology_validator.py

Add these test functions:

```

def test_smart_mode_ignores_forbidden(validator):
"""
SMART —Ä–µ–∂–∏–º: –ü—Ä–æ–ø—É—Å–∫–∞–µ—Ç forbidden terms –µ—Å–ª–∏ –ø–ª–æ—Ç–Ω–æ—Å—Ç—å ‚â•15%
"""
text = """
–ß–µ–ª–æ–≤–µ–∫ –ø—Ä–∏—Ö–æ–¥–∏—Ç —Å —Ç—Ä–µ–≤–æ–≥–æ–π –∏ –≥–æ–≤–æ—Ä–∏—Ç –ø—Ä–æ —ç–≥–æ –∏ –ø–æ–¥—Å–æ–∑–Ω–∞–Ω–∏–µ.
–Ø –æ–±—ä—è—Å–Ω—è—é: —ç—Ç–æ –Ω–µ —ç–≥–æ, —ç—Ç–æ –Ø-–æ–±—Ä–∞–∑. –ú–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ –Ø-–æ–±—Ä–∞–∑–æ–º
–≤ –ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è –≤–µ–¥—ë—Ç –∫ —Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏—é. –ò—â—É—â–∏–π –∑–∞–º–µ—á–∞–µ—Ç
–∞–≤—Ç–æ–º–∞—Ç–∏–∑–º—ã –ø—Å–∏—Ö–∏–∫–∏ –∏ –∑–∞—Ö–≤–∞—Ç –≤–Ω–∏–º–∞–Ω–∏—è –Ø-–æ–±—Ä–∞–∑–æ–º.
–ß–µ—Ä–µ–∑ —Å–≤–æ–±–æ–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç —á–∏—Å—Ç–æ–µ –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ.
"""

    result = validator.validate_text(text, validation_mode="smart")
    
    assert result.is_valid == True
    assert len(result.forbidden_terms_found) > 0  # –ù–∞–π–¥–µ–Ω—ã –Ω–æ –Ω–µ –±–ª–æ–∫–∏—Ä—É—é—Ç
    assert result.metrics['density'] >= 0.15
    
    print(f"‚úÖ SMART: –ø–ª–æ—Ç–Ω–æ—Å—Ç—å {result.metrics['density']:.1%}, "
          f"forbidden –Ω–∞–π–¥–µ–Ω–æ: {len(result.forbidden_terms_found)}, —Ç–µ–∫—Å—Ç –ü–†–û–ü–£–©–ï–ù")
    def test_smart_mode_blocks_low_density(validator):
"""
SMART —Ä–µ–∂–∏–º: –ë–ª–æ–∫–∏—Ä—É–µ—Ç –Ω–∏–∑–∫—É—é –ø–ª–æ—Ç–Ω–æ—Å—Ç—å <15%
"""
text = """
–≠–≥–æ, –ø–æ–¥—Å–æ–∑–Ω–∞–Ω–∏–µ, —Ç—Ä–µ–≤–æ–≥–∞, —Å—Ç—Ä–µ—Å—Å. –ú–µ–¥–∏—Ç–∞—Ü–∏—è –ø–æ–º–æ–≥–∞–µ—Ç –ª—é–¥—è–º.
–ü—Å–∏—Ö–æ—Ç–µ—Ä–∞–ø–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ. –ö–ª–∏–µ–Ω—Ç –¥–æ–≤–æ–ª–µ–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏.
–ò–Ω–æ–≥–¥–∞ —É–ø–æ–º—è–Ω—É –º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã.
"""

    result = validator.validate_text(text, validation_mode="smart")
    
    assert result.is_valid == False
    assert result.metrics['density'] < 0.15
    
    print(f"‚ùå SMART: –ø–ª–æ—Ç–Ω–æ—Å—Ç—å {result.metrics['density']:.1%} < 15%, —Ç–µ–∫—Å—Ç –ó–ê–ë–õ–û–ö–ò–†–û–í–ê–ù")
    def test_strict_mode_blocks_forbidden_even_with_high_density(validator):
"""
STRICT —Ä–µ–∂–∏–º: –ë–ª–æ–∫–∏—Ä—É–µ—Ç forbidden terms –¥–∞–∂–µ –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
"""
text = """
–ú–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ –Ø-–æ–±—Ä–∞–∑–æ–º —á–µ—Ä–µ–∑ —Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ.
–ü–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è —Ä–∞—Å—à–∏—Ä—è–µ—Ç—Å—è, —Å–≤–æ–±–æ–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç.
–ù–æ –ª—é–¥–∏ –≥–æ–≤–æ—Ä—è—Ç "—ç–≥–æ" –∏ "–º–µ–¥–∏—Ç–∞—Ü–∏—è" –ø–æ –ø—Ä–∏–≤—ã—á–∫–µ.
–ò—â—É—â–∏–π –ø—Ä–∞–∫—Ç–∏–∫—É–µ—Ç —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–∏.
"""

    result = validator.validate_text(text, validation_mode="strict")
    
    assert result.is_valid == False  # –ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–æ –∏–∑-–∑–∞ forbidden
    assert result.metrics['density'] >= 0.25
    assert len(result.forbidden_terms_found) > 0
    
    print(f"‚ùå STRICT: –ø–ª–æ—Ç–Ω–æ—Å—Ç—å {result.metrics['density']:.1%}, "
          f"–Ω–æ forbidden –Ω–∞–π–¥–µ–Ω–æ ‚Üí –ó–ê–ë–õ–û–ö–ò–†–û–í–ê–ù–û")
    def test_mode_comparison_same_text(validator):
"""
–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ä–µ–∂–∏–º–æ–≤ –Ω–∞ –æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
"""
text = """
–ß–µ–ª–æ–≤–µ–∫ —Å —Ç—Ä–µ–≤–æ–≥–æ–π –≥–æ–≤–æ—Ä–∏—Ç: "–ú–æ—ë —ç–≥–æ –º–µ–Ω—è —Ä–∞–∑—Ä—É—à–∞–µ—Ç."
–Ø –æ–±—ä—è—Å–Ω—è—é: —ç—Ç–æ –Ø-–æ–±—Ä–∞–∑. –ú–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ –Ø-–æ–±—Ä–∞–∑–æ–º
–≤ –ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è –≤–µ–¥—ë—Ç –∫ —Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏—é. –ò—â—É—â–∏–π –≤–∏–¥–∏—Ç
–∞–≤—Ç–æ–º–∞—Ç–∏–∑–º—ã –ø—Å–∏—Ö–∏–∫–∏ –∏ –ø—Ä–∞–∫—Ç–∏–∫—É–µ—Ç —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–∏.
"""

    results = {
        "smart": validator.validate_text(text, validation_mode="smart"),
        "soft": validator.validate_text(text, validation_mode="soft"),
        "strict": validator.validate_text(text, validation_mode="strict"),
        "off": validator.validate_text(text, validation_mode="off")
    }
    
    print("\n" + "="*60)
    print("–°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ñ–ò–ú–û–í –ù–ê –û–î–ù–û–ú –¢–ï–ö–°–¢–ï")
    print("="*60)
    for mode, result in results.items():
        status = "‚úÖ VALID" if result.is_valid else "‚ùå INVALID"
        print(f"{mode.upper():8} {status:12} density={result.metrics['density']:.1%} "
              f"forbidden={len(result.forbidden_terms_found)}")
    print("="*60)
    
# Add to main test execution

if __name__ == "__main__":
validator = TerminologyValidator()

    print("\n" + "="*60)
    print("–ù–û–í–´–ï –¢–ï–°–¢–´: SMART –†–ï–ñ–ò–ú")
    print("="*60)
    print()
    
    test_smart_mode_ignores_forbidden(validator)
    print()
    test_smart_mode_blocks_low_density(validator)
    print()
    test_strict_mode_blocks_forbidden_even_with_high_density(validator)
    print()
    test_mode_comparison_same_text(validator)
    ```


STEP 6: Run tests
-----------------
Execute:
```

python tests/extractors/test_terminology_validator.py

```

Verify output shows:
- ‚úÖ SMART mode passes high-density text with forbidden terms
- ‚ùå SMART mode blocks low-density text
- ‚ùå STRICT mode blocks forbidden terms even with high density
- Mode comparison table shows differences


STEP 7: Update documentation
-----------------------------
File: EXTRACTORS_DEVELOPMENT_REPORT.md

Add new section after "–≠—Ç–∞–ø 1: TerminologyValidator":

```


### –†–µ–∂–∏–º—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏

–ü–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –ª–µ–∫—Ü–∏–π –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞ –±—ã–ª–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –≥–∏–±–∫–∏—Ö —Ä–µ–∂–∏–º–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.

#### –ü—Ä–æ–±–ª–µ–º–∞

–°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–º–µ—à–∞–Ω–Ω—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é:

- –°–≤–æ–∏ —Ç–µ—Ä–º–∏–Ω—ã: "–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ", "–Ø-–æ–±—Ä–∞–∑", "—Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ"
- –û–±—â–∏–µ —Ç–µ—Ä–º–∏–Ω—ã: "—ç–≥–æ", "–º–µ–¥–∏—Ç–∞—Ü–∏—è", "—Ç—Ä–µ–≤–æ–≥–∞" (–¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏–π)

–ñ—ë—Å—Ç–∫–∏–π —Ñ–∏–ª—å—Ç—Ä –±–ª–æ–∫–∏—Ä–æ–≤–∞–ª ~60% —Ü–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.

#### –†–µ—à–µ–Ω–∏–µ: 4 —Ä–µ–∂–∏–º–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏

| –†–µ–∂–∏–º | Forbidden terms | Min density | –û–ø–∏—Å–∞–Ω–∏–µ |
| :-- | :-- | :-- | :-- |
| **smart** ‚úÖ | –ò–ì–ù–û–†–ò–†–£–Æ–¢–°–Ø | 15% | –ü—Ä–æ–ø—É—Å–∫–∞–µ—Ç –≤—Å—ë —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç—å—é |
| soft | –ü—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ | 25% | –ü—Ä–æ–ø—É—Å–∫–∞–µ—Ç –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—ä—è—Å–Ω–∏—Ç–µ–ª—å–Ω—ã–π |
| strict | –ë–ª–æ–∫–∏—Ä—É—é—Ç—Å—è –∂—ë—Å—Ç–∫–æ | 25% | –ë–ª–æ–∫–∏—Ä—É–µ—Ç –ª—é–±–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ |
| off | –ò–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è | 15% | –¢–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ |

#### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: SMART —Ä–µ–∂–∏–º

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**

- ‚úÖ –°–æ—Ö—Ä–∞–Ω—è–µ—Ç 95% –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ª–µ–∫—Ü–∏–π (vs 40% –≤ strict)
- ‚úÖ –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Å—Ç–∏–ª—å –ª–µ–∫—Ç–æ—Ä–∞
- ‚úÖ –£–º–Ω—ã–π –±–æ—Ç (GPT-4) —Å–∞–º —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –Ω–∞ –≤—ã—Ö–æ–¥–µ
- ‚úÖ –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞ (—Ç–æ–ª—å–∫–æ –ø–ª–æ—Ç–Ω–æ—Å—Ç—å ‚â•15%)

**–§–∏–ª–æ—Å–æ—Ñ–∏—è:**
> –õ—É—á—à–µ –¥–∞—Ç—å –±–æ—Ç—É –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—É—Å—Ç—å –æ–Ω —Å–∞–º —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç,
> —á–µ–º –æ—Ç—Å–µ–∏–≤–∞—Ç—å –ø–æ–ª–µ–∑–Ω–æ–µ –Ω–∞ –≤—Ö–æ–¥–µ.

–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ LLM (GPT-4, Claude) –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–º–Ω—ã, —á—Ç–æ–±—ã:

- –í–∏–¥–µ—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
- –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –æ–±—â–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –≤ —Å—Ç–∏–ª—å –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞
- –ò–∑–≤–ª–µ–∫–∞—Ç—å –Ω—É–∂–Ω–æ–µ –∏–∑ —Å–º–µ—à–∞–Ω–Ω–æ–π –ª–µ–∫—Å–∏–∫–∏


#### –ü—Ä–∏–º–µ—Ä —Ä–∞–±–æ—Ç—ã

```python
text = """
–ß–µ–ª–æ–≤–µ–∫ —Å —Ç—Ä–µ–≤–æ–≥–æ–π –≥–æ–≤–æ—Ä–∏—Ç: "–ú–æ—ë —ç–≥–æ —Ä–∞–∑—Ä—É—à–∞–µ—Ç –º–µ–Ω—è."
–Ø –æ–±—ä—è—Å–Ω—è—é: —ç—Ç–æ –Ω–µ —ç–≥–æ, —ç—Ç–æ –Ø-–æ–±—Ä–∞–∑. –ú–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ 
–∑–∞ –Ø-–æ–±—Ä–∞–∑–æ–º –≤–µ–¥—ë—Ç –∫ —Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏—é...
"""

# –ü–ª–æ—Ç–Ω–æ—Å—Ç—å: ~30%, forbidden: ["—Ç—Ä–µ–≤–æ–≥–∞", "—ç–≥–æ"]

SMART:  ‚úÖ VALID   (30% > 15%, forbidden –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è)
SOFT:   ‚úÖ VALID   (30% > 25%, forbidden –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ)
STRICT: ‚ùå INVALID (forbidden –Ω–∞–π–¥–µ–Ω—ã)
OFF:    ‚úÖ VALID   (30% > 15%, forbidden –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è)
```

```


STEP 8: Verification checklist
-------------------------------
Before marking as complete, verify:

- [ ] .env file created with VALIDATION_MODE=smart
- [ ] terminology_validator.py updated with 4 modes
- [ ] _is_contextual_usage() method added (for soft mode)
- [ ] extractors_config.yaml updated
- [ ] New tests added and passing
- [ ] EXTRACTORS_DEVELOPMENT_REPORT.md updated
- [ ] All existing tests still pass
- [ ] Log output shows mode being used

Run final test:
```

python tests/extractors/test_terminology_validator.py
pytest tests/extractors/ -v

```


=============================================================
EXPECTED RESULTS
=============================================================

After implementation, test output should show:

‚úÖ SMART: –ø–ª–æ—Ç–Ω–æ—Å—Ç—å 30%, forbidden –Ω–∞–π–¥–µ–Ω–æ: 2, —Ç–µ–∫—Å—Ç –ü–†–û–ü–£–©–ï–ù
‚ùå SMART: –ø–ª–æ—Ç–Ω–æ—Å—Ç—å 8% < 15%, —Ç–µ–∫—Å—Ç –ó–ê–ë–õ–û–ö–ò–†–û–í–ê–ù
‚ùå STRICT: –ø–ª–æ—Ç–Ω–æ—Å—Ç—å 30%, –Ω–æ forbidden –Ω–∞–π–¥–µ–Ω–æ ‚Üí –ó–ê–ë–õ–û–ö–ò–†–û–í–ê–ù–û

Mode comparison:
SMART    ‚úÖ VALID      density=30% forbidden=2
SOFT     ‚úÖ VALID      density=30% forbidden=2
STRICT   ‚ùå INVALID    density=30% forbidden=2
OFF      ‚úÖ VALID      density=30% forbidden=2

=============================================================
IMPORTANT NOTES
=============================================================

1. SMART is the RECOMMENDED mode for production
2. Backward compatible: default is 'smart' if .env not found
3. Mode can be overridden per call: validate_text(text, validation_mode="strict")
4. Forbidden terms are still tracked in statistics (even when ignored)
5. This change increases content retention from ~40% to ~95%

=============================================================
END OF INSTRUCTIONS
=============================================================

Please implement these changes step by step.
Show progress after each step.
Report any issues immediately.
```


***

## ‚úÖ –ü–û–°–õ–ï –û–¢–ü–†–ê–í–ö–ò

–ö–æ–≥–¥–∞ Cursor –∑–∞–≤–µ—Ä—à–∏—Ç —Ä–∞–±–æ—Ç—É, –ø–æ–ø—Ä–æ—Å–∏—Ç–µ –ø–æ–∫–∞–∑–∞—Ç—å:

1. **–õ–æ–≥ —Ç–µ—Å—Ç–æ–≤** (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–æ–≤—ã–µ —Ç–µ—Å—Ç—ã —Å SMART —Ä–µ–∂–∏–º–æ–º)
2. **–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É** —Ä–µ–∂–∏–º–æ–≤ –Ω–∞ –æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
3. **–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ**, —á—Ç–æ `.env` —Å–æ–∑–¥–∞–Ω

–¢–æ–≥–¥–∞ —è –ø—Ä–æ–≤–µ—Ä—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –¥–∞–º –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å! üöÄ

