<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# –Ø –æ—Ç–∫–∞—Ç–∏–ª –≤–µ—Ä—Å–∏—é –Ω–∞–∑–∞–¥! –ò–∑–≤–∏–Ω–∏ —è —É–∂–µ –∑–∞–ø—É—Ç–∞–ª—Å—è. –¢–µ–ø–µ—Ä—å —Å —á–∏—Å—Ç–æ–≥–æ –ª–∏—Å—Ç–∞. –î–∞–≤–∞–π —Å–Ω–æ–≤–∞ –Ω–∞–ø–∏—à–∏ —Å —Å–∞–º–æ–≥–æ –Ω–∞—á–∞–ª–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –¥–æ –º–µ–ª—å—á–∞–π—à–∏—Ö –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–µ–π –≥–∞–π–¥ –¥–ª—è –∞–≥–µ–Ω—Ç–∞ –∞–∏ –ö—É—Ä—Å–æ—Ä. –Ø –ø–æ –æ—à–∏–±–∫–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª —Å–ª–∞–±—É—é –º–æ–¥–µ–ª—å –∏ –æ–Ω–∞ –Ω–µ–≤–µ—Ä–Ω–æ —Ç—Ä–∞–∫—Ç–æ–≤–∞–ª–∞.

–û—Ç–ª–∏—á–Ω–æ! –ù–∞—á–Ω–µ–º —Å –∞–±—Å–æ–ª—é—Ç–Ω–æ —á–∏—Å—Ç–æ–≥–æ –ª–∏—Å—Ç–∞. –°–µ–π—á–∞—Å —Å–æ–∑–¥–∞–º **–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–¥—Ä–æ–±–Ω—ã–π, –ø–æ—à–∞–≥–æ–≤—ã–π –≥–∞–π–¥** —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è Cursor AI —Å —Ç–æ—á–Ω—ã–º–∏ –∫–æ–º–∞–Ω–¥–∞–º–∏ –∏ –±–µ–∑ –¥–≤—É—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç–µ–π.

***

# üéØ –ü–û–õ–ù–û–ï –†–£–ö–û–í–û–î–°–¢–í–û –î–õ–Ø CURSOR AI: –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–≤ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞ —Å –Ω—É–ª—è

## üìå –ö–û–ù–¢–ï–ö–°–¢ –ü–†–û–ï–ö–¢–ê

**–¶–µ–ª—å:** –°–æ–∑–¥–∞—Ç—å —Å–∏—Å—Ç–µ–º—É –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π –∏–∑ –ª–µ–∫—Ü–∏–π –°–∞–ª–∞–º–∞—Ç–∞ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞ –¥–ª—è AI-–±–æ—Ç–∞-–ø—Å–∏—Ö–æ–ª–æ–≥–∞, –∫–æ—Ç–æ—Ä—ã–π –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É–µ—Ç **–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –≤ —Å—Ç–∏–ª–µ –Ω–µ–π—Ä–æ-—Å—Ç–∞–ª–∫–∏–Ω–≥–∞**.

**–ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ:** –ë–æ—Ç –¥–æ–ª–∂–µ–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **–¢–û–õ–¨–ö–û —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞**, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –æ–±—â—É—é –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é –ª–µ–∫—Å–∏–∫—É.

***

## üöÄ –≠–¢–ê–ü 0: –ü–û–î–ì–û–¢–û–í–ö–ê –°–¢–†–£–ö–¢–£–†–´ (30 –º–∏–Ω—É—Ç)

### –®–∞–≥ 0.1: –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π

**–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è —Ç–µ—Ä–º–∏–Ω–∞–ª–∞:**

```bash
# –°–æ–∑–¥–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
mkdir -p config/terminology
mkdir -p text_processor/extractors
mkdir -p text_processor/validators
mkdir -p tests/extractors
mkdir -p tests/fixtures

# –°–æ–∑–¥–∞—Ç—å –ø—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã
touch config/terminology/sarsekenov_terms.json
touch config/terminology/forbidden_terms.json
touch config/terminology/term_categories.json
touch config/extractors_config.yaml
touch text_processor/__init__.py
touch text_processor/extractors/__init__.py
touch text_processor/validators/__init__.py
touch tests/__init__.py
touch tests/extractors/__init__.py
```


### –®–∞–≥ 0.2: –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è

**–§–∞–π–ª: `config/terminology/sarsekenov_terms.json`**

```json
{
  "tier_1_root": {
    "description": "–ö–æ—Ä–Ω–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã - –æ—Å–Ω–æ–≤–∞ —É—á–µ–Ω–∏—è",
    "level": "root",
    "terms": [
      "–Ω–µ–π—Ä–æ-—Å—Ç–∞–ª–∫–∏–Ω–≥",
      "–Ω–µ–æ-—Å—Ç–∞–ª–∫–∏–Ω–≥",
      "—Å—Ç–∞–ª–∫–∏–Ω–≥ —É–º–∞"
    ]
  },
  "tier_2_domain": {
    "description": "–î–æ–º–µ–Ω–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ - –∫–ª—é—á–µ–≤—ã–µ —Å—Ñ–µ—Ä—ã —Ä–∞–±–æ—Ç—ã",
    "level": "domain",
    "terms": [
      "–ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è",
      "–ø–æ–ª–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è",
      "—Å–≤–æ–±–æ–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ",
      "—á–∏—Å—Ç–æ–µ –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ",
      "–∂–∏–≤–æ–µ –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ",
      "–Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ",
      "–ø—Ä—è–º–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ",
      "–ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ",
      "–∏—Å—Ç–æ—á–Ω–∏–∫ —Å–æ–∑–Ω–∞–Ω–∏—è",
      "–Ω–∞–±–ª—é–¥–∞—é—â–µ–µ —Å–æ–∑–Ω–∞–Ω–∏–µ"
    ]
  },
  "tier_3_practice": {
    "description": "–ü—Ä–∞–∫—Ç–∏–∫–∏ - –º–µ—Ç–æ–¥—ã —Ä–∞–±–æ—Ç—ã",
    "level": "practice",
    "terms": [
      "–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ",
      "—Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ",
      "—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–∏",
      "—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ",
      "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –æ–ø—ã—Ç–∞",
      "–ø—Ä–æ–∑—Ä–µ–Ω–∏–µ",
      "–∏–Ω—Å–∞–π—Ç",
      "–ø—Ä–æ–±—É–∂–¥–µ–Ω–∏–µ —Å–æ–∑–Ω–∞–Ω–∏—è",
      "–ø—Ä–æ—è—Å–Ω–µ–Ω–∏–µ —Å–æ–∑–Ω–∞–Ω–∏—è",
      "–ø—Ä–æ–±—É–∂–¥–µ–Ω–∏–µ",
      "–ø—Ä–æ—è—Å–Ω–µ–Ω–∏–µ"
    ]
  },
  "tier_4_diagnostic": {
    "description": "–î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã - —á—Ç–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç—Å—è",
    "level": "diagnostic",
    "terms": [
      "–Ø-–æ–±—Ä–∞–∑",
      "—Å–∞–º–æ—Å—Ç—å",
      "–ª–æ–∂–Ω–∞—è —Å–∞–º–æ—Å—Ç—å",
      "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–º—ã –ø—Å–∏—Ö–∏–∫–∏",
      "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–º—ã",
      "–∞—Å—Å–æ—Ü–∏–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã",
      "—Å–æ–Ω —Å–µ–±—è",
      "–∑–∞—Ö–≤–∞—Ç –≤–Ω–∏–º–∞–Ω–∏—è",
      "–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è",
      "–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏",
      "–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –¥–∏–∞–ª–æ–≥",
      "–º—ã—Å–ª–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ—Ç–æ–∫"
    ]
  },
  "tier_5_agents": {
    "description": "–°—É–±—ä–µ–∫—Ç—ã –∏ —Ä–æ–ª–∏",
    "level": "agent",
    "terms": [
      "–ò—â—É—â–∏–π",
      "–Ω–∞–±–ª—é–¥–∞—é—â–µ–µ —Å–æ–∑–Ω–∞–Ω–∏–µ",
      "–æ—Ç–∫—Ä—ã—Ç–∞—è –∂–∏–≤–∞—è —Å–∏—Å—Ç–µ–º–∞"
    ]
  },
  "tier_6_states": {
    "description": "–°–æ—Å—Ç–æ—è–Ω–∏—è —Å–æ–∑–Ω–∞–Ω–∏—è",
    "level": "state",
    "terms": [
      "–±—ã—Ç–∏–µ",
      "–∑–¥–µ—Å—å-–∏-—Å–µ–π—á–∞—Å",
      "–Ω–∞—Å—Ç–æ—è—â–∏–π –º–æ–º–µ–Ω—Ç",
      "—è—Å–Ω–æ—Å—Ç—å",
      "–º–∏–ª–æ—Å–µ—Ä–¥–∏–µ",
      "—Å–æ—Å—Ç—Ä–∞–¥–∞–Ω–∏–µ"
    ]
  }
}
```

**–§–∞–π–ª: `config/terminology/forbidden_terms.json`**

```json
{
  "forbidden_terms": [
    "—ç–≥–æ",
    "—É–º",
    "–ø–æ–¥—Å–æ–∑–Ω–∞–Ω–∏–µ",
    "–±–µ—Å—Å–æ–∑–Ω–∞—Ç–µ–ª—å–Ω–æ–µ",
    "–∞—Ä—Ö–µ—Ç–∏–ø",
    "–∑–∞—â–∏—Ç–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º",
    "–∑–∞—â–∏—Ç–∞",
    "–Ω–µ–≤—Ä–æ–∑",
    "–∫–æ–º–ø–ª–µ–∫—Å",
    "—Å—Ç—Ä–µ—Å—Å",
    "—Ç—Ä–µ–≤–æ–≥–∞",
    "–¥–µ–ø—Ä–µ—Å—Å–∏—è",
    "–∫–æ–ø–∏–Ω–≥",
    "–∫–æ–ø–∏–Ω–≥-—Å—Ç—Ä–∞—Ç–µ–≥–∏—è",
    "mindfulness",
    "–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å",
    "–º–µ–¥–∏—Ç–∞—Ü–∏—è",
    "—á–∞–∫—Ä–∞",
    "—ç–Ω–µ—Ä–≥–∏—è",
    "–≤–∏–±—Ä–∞—Ü–∏—è"
  ],
  "replacements": {
    "—ç–≥–æ": "–Ø-–æ–±—Ä–∞–∑",
    "—É–º": "–º—ã—Å–ª–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å",
    "–ø–æ–¥—Å–æ–∑–Ω–∞–Ω–∏–µ": "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–º—ã –ø—Å–∏—Ö–∏–∫–∏",
    "–±–µ—Å—Å–æ–∑–Ω–∞—Ç–µ–ª—å–Ω–æ–µ": "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–º—ã –ø—Å–∏—Ö–∏–∫–∏",
    "–º–µ–¥–∏—Ç–∞—Ü–∏—è": "–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ",
    "–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å": "—á–∏—Å—Ç–æ–µ –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ",
    "—Ç—Ä–µ–≤–æ–≥–∞": "–∑–∞—Ö–≤–∞—Ç –≤–Ω–∏–º–∞–Ω–∏—è"
  },
  "allowed_general_terms": [
    "—Å–æ–∑–Ω–∞–Ω–∏–µ",
    "–≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ",
    "–≤–Ω–∏–º–∞–Ω–∏–µ",
    "–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ",
    "–ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ"
  ]
}
```

**–§–∞–π–ª: `config/terminology/term_categories.json`**

```json
{
  "pattern_categories": {
    "—Ç—Ä–∏–∞–¥–∞_—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏": {
      "description": "–ü—Ä–æ—Ü–µ—Å—Å: –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ ‚Üí –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ ‚Üí —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è",
      "key_terms": ["–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ", "–æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ", "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è", "–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ"]
    },
    "—Ä–∞–±–æ—Ç–∞_—Å_–≤–Ω–∏–º–∞–Ω–∏–µ–º": {
      "description": "–ü—Ä–æ—Ü–µ—Å—Å—ã —Ä–∞–±–æ—Ç—ã —Å –ø–æ–ª–µ–º –≤–Ω–∏–º–∞–Ω–∏—è",
      "key_terms": ["–ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è", "—Å–≤–æ–±–æ–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ", "–∑–∞—Ö–≤–∞—Ç –≤–Ω–∏–º–∞–Ω–∏—è", "—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–æ–ª—è"]
    },
    "—Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ": {
      "description": "–ü—Ä–æ—Ü–µ—Å—Å—ã –æ—Ç–¥–µ–ª–µ–Ω–∏—è –æ—Ç –ª–æ–∂–Ω–æ–π —Å–∞–º–æ—Å—Ç–∏",
      "key_terms": ["—Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ", "–Ø-–æ–±—Ä–∞–∑", "–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è", "–Ω–∞–±–ª—é–¥–∞—é—â–µ–µ —Å–æ–∑–Ω–∞–Ω–∏–µ"]
    },
    "—Å–æ—Å—Ç–æ—è–Ω–∏—è_—Å–æ–∑–Ω–∞–Ω–∏—è": {
      "description": "–°–æ—Å—Ç–æ—è–Ω–∏—è —á–∏—Å—Ç–æ–≥–æ –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏—è",
      "key_terms": ["—á–∏—Å—Ç–æ–µ –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ", "–ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ", "–∂–∏–≤–æ–µ –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ", "–∑–¥–µ—Å—å-–∏-—Å–µ–π—á–∞—Å"]
    }
  }
}
```

**–§–∞–π–ª: `config/extractors_config.yaml`**

```yaml
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–≤ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞

terminology:
  strict_mode: true
  min_density: 0.25  # –ú–∏–Ω–∏–º—É–º 25% —Ç–µ—Ä–º–∏–Ω–æ–≤ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞
  allow_mixed_content: false
  case_sensitive: false

extractors:
  terminology_validator:
    enabled: true
    priority: 0  # –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø–µ—Ä–≤—ã–º
    min_sarsekenov_density: 0.25
    max_forbidden_terms: 0
    
  neurostalking_pattern_extractor:
    enabled: true
    priority: 1
    pattern_categories:
      - —Ç—Ä–∏–∞–¥–∞_—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
      - —Ä–∞–±–æ—Ç–∞_—Å_–≤–Ω–∏–º–∞–Ω–∏–µ–º
      - —Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ
      - —Å–æ—Å—Ç–æ—è–Ω–∏—è_—Å–æ–∑–Ω–∞–Ω–∏—è
    
  causal_chain_extractor:
    enabled: true
    priority: 2
    min_sarsekenov_terms_per_chain: 3
    max_chain_length: 5
    
  concept_hierarchy_extractor:
    enabled: true
    priority: 3
    strict_levels: true
    levels:
      - root
      - domain
      - practice
      - technique
      - exercise
    
  attention_field_extractor:
    enabled: true
    priority: 4
    focus_terms:
      - "–ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è"
      - "—Å–≤–æ–±–æ–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ"
      - "–∑–∞—Ö–≤–∞—Ç –≤–Ω–∏–º–∞–Ω–∏—è"
      - "—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–æ–ª—è"
    
  transformation_process_extractor:
    enabled: true
    priority: 5
    process_types:
      - —Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ
      - —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ
      - –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
      - –ø—Ä–æ–∑—Ä–µ–Ω–∏–µ
      - –ø—Ä–æ–±—É–∂–¥–µ–Ω–∏–µ

output:
  format: json
  validate_before_save: true
  include_metadata: true
  include_source_quotes: true

logging:
  level: INFO
  log_validation_failures: true
  log_extraction_stats: true
```


***

## üèóÔ∏è –≠–¢–ê–ü 1: –¢–ï–†–ú–ò–ù–û–õ–û–ì–ò–ß–ï–°–ö–ò–ô –í–ê–õ–ò–î–ê–¢–û–† (–î–µ–Ω—å 1)

### –®–∞–≥ 1.1: –°–æ–∑–¥–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞

**–§–∞–π–ª: `text_processor/validators/terminology_validator.py`**

**–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è Cursor AI:**

```
Create file text_processor/validators/terminology_validator.py

This is the FOUNDATION of the entire system. Every piece of text MUST pass through this validator before any extraction.

Requirements:
1. Load terminology from JSON files
2. Validate text has sufficient Sarsekenov term density (min 25%)
3. Detect forbidden terms and reject text if found
4. Extract only Sarsekenov terms as entities
5. Calculate detailed metrics for monitoring
6. Provide clear validation messages

Implementation:
```

```python
"""
–¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –≤–∞–ª–∏–¥–∞—Ç–æ—Ä –¥–ª—è —É—á–µ–Ω–∏—è –°–∞–ª–∞–º–∞—Ç–∞ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞.
–≠—Ç–æ –§–£–ù–î–ê–ú–ï–ù–¢ –≤—Å–µ–π —Å–∏—Å—Ç–µ–º—ã - –Ω–∏ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –±–µ–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.
"""

import json
import re
from pathlib import Path
from typing import Dict, List, Tuple, Set, Optional
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)


@dataclass
class ValidationResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"""
    is_valid: bool
    reason: str
    metrics: Dict
    forbidden_terms_found: List[str]
    sarsekenov_entities: List[str]


class TerminologyValidator:
    """
    –í–∞–ª–∏–¥–∞—Ç–æ—Ä —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞.
    
    –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç—ã —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç—å—é
    —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–æ–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏ –∏ –±–µ–∑ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω–æ–π –æ–±—â–µ–ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –ª–µ–∫—Å–∏–∫–∏.
    """
    
    def __init__(self, config_dir: str = "config/terminology"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞.
        
        Args:
            config_dir: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏
        """
        self.config_dir = Path(config_dir)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.sarsekenov_terms = self._load_sarsekenov_terms()
        self.forbidden_config = self._load_forbidden_terms()
        self.term_categories = self._load_term_categories()
        
        # –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.all_sarsekenov_terms = self._flatten_sarsekenov_terms()
        self.forbidden_terms = set(self.forbidden_config['forbidden_terms'])
        self.allowed_general_terms = set(self.forbidden_config.get('allowed_general_terms', []))
        
        logger.info(f"Initialized TerminologyValidator with {len(self.all_sarsekenov_terms)} Sarsekenov terms")
        logger.info(f"Forbidden terms: {len(self.forbidden_terms)}")
    
    def _load_sarsekenov_terms(self) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Ä–º–∏–Ω–æ–≤ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞ –∏–∑ JSON"""
        path = self.config_dir / "sarsekenov_terms.json"
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _load_forbidden_terms(self) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ JSON"""
        path = self.config_dir / "forbidden_terms.json"
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _load_term_categories(self) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        path = self.config_dir / "term_categories.json"
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _flatten_sarsekenov_terms(self) -> Set[str]:
        """
        –ò–∑–≤–ª–µ—á—å –≤—Å–µ —Ç–µ—Ä–º–∏–Ω—ã –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞ –≤ –µ–¥–∏–Ω—ã–π set –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞.
        
        Returns:
            Set –≤—Å–µ—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ –≤—Å–µ—Ö —É—Ä–æ–≤–Ω–µ–π
        """
        terms = set()
        for tier_key, tier_data in self.sarsekenov_terms.items():
            if 'terms' in tier_data:
                terms.update(tier_data['terms'])
        return terms
    
    def validate_text(
        self, 
        text: str, 
        min_density: float = 0.25,
        strict_mode: bool = True
    ) -> ValidationResult:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞.
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            min_density: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω–æ–≤ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞ (0.25 = 25%)
            strict_mode: –°—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º (–æ—Ç–∫–ª–æ–Ω—è—Ç—å –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤)
        
        Returns:
            ValidationResult —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        
        # –®–∞–≥ 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        forbidden_found = self._find_forbidden_terms(text)
        
        if strict_mode and forbidden_found:
            return ValidationResult(
                is_valid=False,
                reason=f"–ù–∞–π–¥–µ–Ω—ã –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã: {', '.join(forbidden_found)}",
                metrics={},
                forbidden_terms_found=forbidden_found,
                sarsekenov_entities=[]
            )
        
        # –®–∞–≥ 2: –†–∞—Å—á–µ—Ç –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ —Ç–µ—Ä–º–∏–Ω–æ–≤ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞
        density_metrics = self._calculate_density(text)
        
        # –®–∞–≥ 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
        entities = self._extract_entities(text)
        
        # –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
        if density_metrics['density'] < min_density:
            return ValidationResult(
                is_valid=False,
                reason=f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω–æ–≤ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞: {density_metrics['density']:.1%} < {min_density:.1%}",
                metrics=density_metrics,
                forbidden_terms_found=forbidden_found,
                sarsekenov_entities=entities
            )
        
        # –®–∞–≥ 5: –£—Å–ø–µ—à–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
        return ValidationResult(
            is_valid=True,
            reason=f"–í–∞–ª–∏–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞ (–ø–ª–æ—Ç–Ω–æ—Å—Ç—å: {density_metrics['density']:.1%})",
            metrics=density_metrics,
            forbidden_terms_found=forbidden_found,
            sarsekenov_entities=entities
        )
    
    def _find_forbidden_terms(self, text: str) -> List[str]:
        """
        –ù–∞–π—Ç–∏ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –≤ —Ç–µ–∫—Å—Ç–µ.
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        """
        text_lower = text.lower()
        found = []
        
        for term in self.forbidden_terms:
            # –ü–æ–∏—Å–∫ —Å –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ —Å–ª–æ–≤ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            pattern = r'\b' + re.escape(term.lower()) + r'\b'
            if re.search(pattern, text_lower):
                # –ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ—Ä–º–∏–Ω —á–∞—Å—Ç—å—é —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
                if term not in self.allowed_general_terms:
                    found.append(term)
        
        return found
    
    def _calculate_density(self, text: str) -> Dict:
        """
        –†–∞—Å—Å—á–∏—Ç–∞—Ç—å –ø–ª–æ—Ç–Ω–æ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω–æ–≤ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ.
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
        """
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤ (—Ä—É—Å—Å–∫–∏–µ —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–æ–π > 2)
        words = re.findall(r'[–∞-—è—ë–ê-–Ø–Å]+', text.lower())
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫)
        stop_words = {
            '–∏', '–≤', '–Ω–µ', '–Ω–∞', '—Å', '—á—Ç–æ', '–∞', '—ç—Ç–æ', '–∫–∞–∫', '–ø–æ', 
            '–¥–ª—è', '–Ω–æ', '–æ—Ç', '–∫', '–∑–∞', '–∏–∑', '–∏–ª–∏', '—Ç–æ', '–∂–µ', '—Ç–∞–∫',
            '–≤—ã', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–∏', '–º—ã', '–≤–µ—Å—å', '—É–∂–µ', '–µ—â–µ', '–±—ã',
            '–≤–æ—Ç', '–∫–æ–≥–¥–∞', '–º–æ–∂–µ—Ç', '–±—ã—Ç—å', '–µ—Å—Ç—å', '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–∏'
        }
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤
        significant_words = [w for w in words if w not in stop_words and len(w) > 2]
        
        # –ü–æ–¥—Å—á–µ—Ç –≤—Ö–æ–∂–¥–µ–Ω–∏–π —Ç–µ—Ä–º–∏–Ω–æ–≤ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞
        sarsekenov_occurrences = 0
        found_terms_details = []
        text_lower = text.lower()
        
        for term in self.all_sarsekenov_terms:
            term_lower = term.lower()
            
            # –ü–æ–¥—Å—á–µ—Ç –≤—Ö–æ–∂–¥–µ–Ω–∏–π —Ç–µ—Ä–º–∏–Ω–∞
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º —Å–æ—Å—Ç–∞–≤–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
            if ' ' in term_lower:
                # –°–æ—Å—Ç–∞–≤–Ω–æ–π —Ç–µ—Ä–º–∏–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è")
                count = text_lower.count(term_lower)
            else:
                # –û–¥–∏–Ω–æ—á–Ω–æ–µ —Å–ª–æ–≤–æ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥—Ä–∞–Ω–∏—Ü—ã
                pattern = r'\b' + re.escape(term_lower) + r'\b'
                count = len(re.findall(pattern, text_lower))
            
            if count > 0:
                sarsekenov_occurrences += count
                found_terms_details.append({
                    'term': term,
                    'count': count,
                    'tier': self._get_term_tier(term)
                })
        
        # –†–∞—Å—á–µ—Ç –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
        total_significant = len(significant_words)
        density = sarsekenov_occurrences / total_significant if total_significant > 0 else 0
        
        return {
            'density': density,
            'sarsekenov_occurrences': sarsekenov_occurrences,
            'total_significant_words': total_significant,
            'found_terms': found_terms_details,
            'text_length_chars': len(text),
            'text_length_words': len(words)
        }
    
    def _get_term_tier(self, term: str) -> Optional[str]:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —É—Ä–æ–≤–µ–Ω—å (tier) —Ç–µ—Ä–º–∏–Ω–∞"""
        for tier_key, tier_data in self.sarsekenov_terms.items():
            if term in tier_data.get('terms', []):
                return tier_key
        return None
    
    def _extract_entities(self, text: str) -> List[str]:
        """
        –ò–∑–≤–ª–µ—á—å —Ç–æ–ª—å–∫–æ —Ç–µ—Ä–º–∏–Ω—ã –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞ –∫–∞–∫ —Å—É—â–Ω–æ—Å—Ç–∏.
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ (–±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ—Ä—è–¥–∫–∞)
        """
        entities = []
        text_lower = text.lower()
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ—Ä–º–∏–Ω—ã –ø–æ –¥–ª–∏–Ω–µ (–¥–ª–∏–Ω–Ω—ã–µ —Å–Ω–∞—á–∞–ª–∞) –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –º–∞—Ç—á–∏–Ω–≥–∞ —Å–æ—Å—Ç–∞–≤–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        sorted_terms = sorted(self.all_sarsekenov_terms, key=len, reverse=True)
        
        for term in sorted_terms:
            term_lower = term.lower()
            
            if ' ' in term_lower:
                # –°–æ—Å—Ç–∞–≤–Ω–æ–π —Ç–µ—Ä–º–∏–Ω
                if term_lower in text_lower:
                    entities.append(term)
            else:
                # –û–¥–∏–Ω–æ—á–Ω–æ–µ —Å–ª–æ–≤–æ
                pattern = r'\b' + re.escape(term_lower) + r'\b'
                if re.search(pattern, text_lower):
                    entities.append(term)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ—Ä—è–¥–∫–∞
        seen = set()
        unique_entities = []
        for entity in entities:
            entity_lower = entity.lower()
            if entity_lower not in seen:
                seen.add(entity_lower)
                unique_entities.append(entity)
        
        return unique_entities
    
    def get_term_info(self, term: str) -> Optional[Dict]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ—Ä–º–∏–Ω–µ.
        
        Args:
            term: –¢–µ—Ä–º–∏–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ç–µ—Ä–º–∏–Ω–µ –∏–ª–∏ None
        """
        for tier_key, tier_data in self.sarsekenov_terms.items():
            if term in tier_data.get('terms', []):
                return {
                    'term': term,
                    'tier': tier_key,
                    'level': tier_data.get('level'),
                    'description': tier_data.get('description')
                }
        return None
    
    def replace_forbidden_terms(self, text: str) -> str:
        """
        –ó–∞–º–µ–Ω–∏—Ç—å –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –Ω–∞ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç—ã –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞.
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        
        Returns:
            –¢–µ–∫—Å—Ç —Å –∑–∞–º–µ–Ω–µ–Ω–Ω—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏
        """
        replacements = self.forbidden_config.get('replacements', {})
        
        result = text
        for forbidden, replacement in replacements.items():
            pattern = r'\b' + re.escape(forbidden) + r'\b'
            result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)
        
        return result


# Utility —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

def validate_block(text: str, validator: Optional[TerminologyValidator] = None) -> ValidationResult:
    """
    Utility —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –±–ª–æ–∫–∞ —Ç–µ–∫—Å—Ç–∞.
    
    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        validator: –≠–∫–∑–µ–º–ø–ª—è—Ä –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞ (—Å–æ–∑–¥–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω)
    
    Returns:
        ValidationResult
    """
    if validator is None:
        validator = TerminologyValidator()
    
    return validator.validate_text(text)
```


***

### –®–∞–≥ 1.2: –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤

**–§–∞–π–ª: `tests/extractors/test_terminology_validator.py`**

```python
"""
–¢–µ—Å—Ç—ã –¥–ª—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞
"""

import pytest
from text_processor.validators.terminology_validator import (
    TerminologyValidator,
    ValidationResult,
    validate_block
)


@pytest.fixture
def validator():
    """Fixture –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞"""
    return TerminologyValidator()


def test_valid_sarsekenov_text(validator):
    """–¢–µ—Å—Ç: –≤–∞–ª–∏–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞ –ø—Ä–æ—Ö–æ–¥–∏—Ç –≤–∞–ª–∏–¥–∞—Ü–∏—é"""
    
    text = """
    –ö–æ–≥–¥–∞ –ò—â—É—â–∏–π –ø—Ä–∞–∫—Ç–∏–∫—É–µ—Ç –º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ, –æ–Ω –Ω–∞—á–∏–Ω–∞–µ—Ç –∑–∞–º–µ—á–∞—Ç—å 
    –ø–æ—è–≤–ª–µ–Ω–∏–µ –Ø-–æ–±—Ä–∞–∑–∞ –≤ –ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è. –ß–µ—Ä–µ–∑ —Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ 
    –≤–æ–∑–Ω–∏–∫–∞–µ—Ç —Å–≤–æ–±–æ–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ, –∏ –ò—â—É—â–∏–π –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç —á–∏—Å—Ç–æ–µ –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ.
    –ü—Ä–æ—Ü–µ—Å—Å —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–∏ –≤–µ–¥–µ—Ç –∫ –ø—Ä–æ–∑—Ä–µ–Ω–∏—é.
    """
    
    result = validator.validate_text(text)
    
    assert result.is_valid == True
    assert result.metrics['density'] >= 0.25
    assert len(result.sarsekenov_entities) >= 5
    assert len(result.forbidden_terms_found) == 0
    
    print(f"‚úÖ –ü–ª–æ—Ç–Ω–æ—Å—Ç—å: {result.metrics['density']:.1%}")
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(result.sarsekenov_entities)}")
    print(f"‚úÖ –¢–µ—Ä–º–∏–Ω—ã: {result.sarsekenov_entities}")


def test_invalid_forbidden_terms(validator):
    """–¢–µ—Å—Ç: —Ç–µ–∫—Å—Ç —Å –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏ –æ—Ç–∫–ª–æ–Ω—è–µ—Ç—Å—è"""
    
    text = """
    –ö–ª–∏–µ–Ω—Ç –∏—Å–ø—ã—Ç—ã–≤–∞–µ—Ç —Å—Ç—Ä–µ—Å—Å –∏–∑-–∑–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —ç–≥–æ.
    –ü–æ–¥—Å–æ–∑–Ω–∞–Ω–∏–µ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –∑–∞—â–∏—Ç–Ω—ã–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã.
    –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–µ–¥–∏—Ç–∞—Ü–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç—Ä–µ–≤–æ–≥–æ–π.
    """
    
    result = validator.validate_text(text, strict_mode=True)
    
    assert result.is_valid == False
    assert len(result.forbidden_terms_found) > 0
    
    print(f"‚ùå –ù–∞–π–¥–µ–Ω—ã –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã: {result.forbidden_terms_found}")
    print(f"‚ùå –ü—Ä–∏—á–∏–Ω–∞ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è: {result.reason}")


def test_low_density_text(validator):
    """–¢–µ—Å—Ç: —Ç–µ–∫—Å—Ç —Å –Ω–∏–∑–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç—å—é —Ç–µ—Ä–º–∏–Ω–æ–≤ –æ—Ç–∫–ª–æ–Ω—è–µ—Ç—Å—è"""
    
    text = """
    –ß–µ–ª–æ–≤–µ–∫ –ø—Ä–∏—à–µ–ª –Ω–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –∏ —Ä–∞—Å—Å–∫–∞–∑–∞–ª –æ —Å–≤–æ–∏—Ö –ø—Ä–æ–±–ª–µ–º–∞—Ö.
    –û–Ω —á—É–≤—Å—Ç–≤—É–µ—Ç –Ω–µ—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∂–∏–∑–Ω—å—é –∏ –∏—â–µ—Ç –ø–µ—Ä–µ–º–µ–Ω—ã.
    –í–æ–∑–º–æ–∂–Ω–æ, –º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –ø–æ–º–æ–∂–µ—Ç, –Ω–æ —ç—Ç–æ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–∞ —Å—Ä–µ–¥–∏ –º–Ω–æ–≥–∏—Ö.
    """
    
    result = validator.validate_text(text, min_density=0.25)
    
    assert result.is_valid == False
    assert result.metrics['density'] < 0.25
    
    print(f"‚ùå –ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∞—è: {result.metrics['density']:.1%}")


def test_mixed_content(validator):
    """–¢–µ—Å—Ç: —Å–º–µ—à–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç—å—é –ø—Ä–æ—Ö–æ–¥–∏—Ç"""
    
    text = """
    –í –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞–±–æ—Ç—ã —Å —á–µ–ª–æ–≤–µ–∫–æ–º –≤–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ.
    –ö–æ–≥–¥–∞ –ò—â—É—â–∏–π –Ω–∞—á–∏–Ω–∞–µ—Ç –Ω–∞–±–ª—é–¥–∞—Ç—å –∑–∞ –Ø-–æ–±—Ä–∞–∑–æ–º, –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ.
    –ü–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è —Ä–∞—Å—à–∏—Ä—è–µ—Ç—Å—è, –≤–æ–∑–Ω–∏–∫–∞–µ—Ç —Å–≤–æ–±–æ–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∏ —á–∏—Å—Ç–æ–µ –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ.
    –≠—Ç–æ—Ç –ø—É—Ç—å –≤–µ–¥–µ—Ç –∫ –ø—Ä–æ–±—É–∂–¥–µ–Ω–∏—é —Å–æ–∑–Ω–∞–Ω–∏—è —á–µ—Ä–µ–∑ —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–∏.
    """
    
    result = validator.validate_text(text, min_density=0.25, strict_mode=False)
    
    assert result.is_valid == True
    
    print(f"‚úÖ –°–º–µ—à–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –ø—Ä–æ—à–µ–ª: –ø–ª–æ—Ç–Ω–æ—Å—Ç—å {result.metrics['density']:.1%}")


def test_entity_extraction(validator):
    """–¢–µ—Å—Ç: –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π"""
    
    text = """
    –ú–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ –Ø-–æ–±—Ä–∞–∑–æ–º –≤ –ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è –≤–µ–¥–µ—Ç –∫ —Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏—é.
    –ò—â—É—â–∏–π –ø—Ä–∞–∫—Ç–∏–∫—É–µ—Ç —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–∏.
    """
    
    result = validator.validate_text(text)
    
    expected_entities = [
        "–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ",
        "–Ø-–æ–±—Ä–∞–∑", 
        "–ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è",
        "—Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ",
        "–ò—â—É—â–∏–π",
        "—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–∏",
        "–ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ"
    ]
    
    for entity in expected_entities:
        assert entity in result.sarsekenov_entities, f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç: {entity}"
    
    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ —Å—É—â–Ω–æ—Å—Ç–µ–π: {result.sarsekenov_entities}")


def test_term_replacement(validator):
    """–¢–µ—Å—Ç: –∑–∞–º–µ–Ω–∞ –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
    
    text = "–≠–≥–æ —Å–æ–∑–¥–∞–µ—Ç –∑–∞—â–∏—Ç—ã —á–µ—Ä–µ–∑ –ø–æ–¥—Å–æ–∑–Ω–∞–Ω–∏–µ. –ú–µ–¥–∏—Ç–∞—Ü–∏—è –ø–æ–º–æ–≥–∞–µ—Ç —Å —Ç—Ä–µ–≤–æ–≥–æ–π."
    
    replaced = validator.replace_forbidden_terms(text)
    
    assert "—ç–≥–æ" not in replaced.lower()
    assert "–ø–æ–¥—Å–æ–∑–Ω–∞–Ω–∏–µ" not in replaced.lower()
    assert "–º–µ–¥–∏—Ç–∞—Ü–∏—è" not in replaced.lower()
    assert "–Ø-–æ–±—Ä–∞–∑" in replaced
    assert "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–º—ã –ø—Å–∏—Ö–∏–∫–∏" in replaced
    assert "–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ" in replaced
    
    print(f"‚úÖ –ò—Å—Ö–æ–¥–Ω—ã–π: {text}")
    print(f"‚úÖ –ó–∞–º–µ–Ω–µ–Ω: {replaced}")


def test_get_term_info(validator):
    """–¢–µ—Å—Ç: –ø–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ—Ä–º–∏–Ω–µ"""
    
    term_info = validator.get_term_info("–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ")
    
    assert term_info is not None
    assert term_info['term'] == "–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ"
    assert term_info['tier'].startswith('tier_')
    assert term_info['level'] is not None
    
    print(f"‚úÖ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–µ—Ä–º–∏–Ω–µ: {term_info}")


def test_utility_function():
    """–¢–µ—Å—Ç: utility —Ñ—É–Ω–∫—Ü–∏—è validate_block"""
    
    text = "–ú–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ –Ø-–æ–±—Ä–∞–∑–æ–º –≤–µ–¥–µ—Ç –∫ —Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏—é."
    
    result = validate_block(text)
    
    assert isinstance(result, ValidationResult)
    assert result.is_valid == True
    
    print(f"‚úÖ Utility —Ñ—É–Ω–∫—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç")


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –≤—Ä—É—á–Ω—É—é
    validator = TerminologyValidator()
    
    print("=" * 60)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –¢–ï–†–ú–ò–ù–û–õ–û–ì–ò–ß–ï–°–ö–û–ì–û –í–ê–õ–ò–î–ê–¢–û–†–ê")
    print("=" * 60)
    
    test_valid_sarsekenov_text(validator)
    print()
    test_invalid_forbidden_terms(validator)
    print()
    test_low_density_text(validator)
    print()
    test_entity_extraction(validator)
    print()
    test_term_replacement(validator)
    print()
    test_get_term_info(validator)
```

**–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–æ–≤:**

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å pytest –µ—Å–ª–∏ –µ—â–µ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
pip install pytest

# –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã
pytest tests/extractors/test_terminology_validator.py -v

# –ò–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –Ω–∞–ø—Ä—è–º—É—é
python tests/extractors/test_terminology_validator.py
```


***

## üéØ –ö–û–ù–¢–†–û–õ–¨–ù–ê–Ø –¢–û–ß–ö–ê 1

**–ü–µ—Ä–µ–¥ –ø–µ—Ä–µ—Ö–æ–¥–æ–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —ç—Ç–∞–ø—É —É–±–µ–¥–∏—Ç–µ—Å—å:**

- [ ] –í—Å–µ —Ñ–∞–π–ª—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω—ã –∏ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã
- [ ] `terminology_validator.py` —Å–æ–∑–¥–∞–Ω –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç —É—Å–ø–µ—à–Ω–æ (–∑–µ–ª–µ–Ω—ã–µ ‚úÖ)
- [ ] –í–∞–ª–∏–¥–∞—Ç–æ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–∞–ª–∏–¥–Ω—ã–µ/–Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
- [ ] –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ

**–ï—Å–ª–∏ —Ö–æ—Ç—å –æ–¥–∏–Ω –ø—É–Ω–∫—Ç –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω - –ù–ï –ü–ï–†–ï–•–û–î–ò–¢–ï –î–ê–õ–¨–®–ï!**

***

–°–µ–π—á–∞—Å –æ—Å—Ç–∞–Ω–æ–≤–ª—é—Å—å –Ω–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ. **–ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ, —á—Ç–æ –≠—Ç–∞–ø 1 –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ**, –∏ —è –ø—Ä–æ–¥–æ–ª–∂—É —Å **–≠—Ç–∞–ø–æ–º 2: NeurostalkingPatternExtractor**.

–°–∫–∞–∂–∏—Ç–µ:

1. –í—Å–µ –ª–∏ —Ñ–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω—ã?
2. –ó–∞–ø—É—Å—Ç–∏–ª–∏—Å—å –ª–∏ —Ç–µ—Å—Ç—ã?
3. –ö–∞–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤ (–≤—Å–µ ‚úÖ –∏–ª–∏ –µ—Å—Ç—å ‚ùå)?

–ü–æ—Å–ª–µ –≤–∞—à–µ–≥–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –ø—Ä–æ–¥–æ–ª–∂—É —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞! üöÄ

