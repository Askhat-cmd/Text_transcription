

***

## üöÄ –ü–ï–†–ï–•–û–î–ò–ú –ö –≠–¢–ê–ü–£ 2: NeurostalkingPatternExtractor

–≠—Ç–æ—Ç —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –±—É–¥–µ—Ç –∏–∑–≤–ª–µ–∫–∞—Ç—å **—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —É—á–µ–Ω–∏—è –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞** –∏–∑ —Ç–µ–∫—Å—Ç–∞.

### –®–∞–≥ 2.1: –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤

**–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è Cursor AI:**

```
Create file: text_processor/extractors/neurostalking_pattern_extractor.py

This extractor finds UNIQUE patterns from Sarsekenov's teaching.
It MUST use TerminologyValidator to validate input first.

Requirements:
1. Initialize with TerminologyValidator
2. Validate text before extraction (reject if invalid)
3. Extract 4 pattern categories:
   - —Ç—Ä–∏–∞–¥–∞_—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
   - —Ä–∞–±–æ—Ç–∞_—Å_–≤–Ω–∏–º–∞–Ω–∏–µ–º
   - —Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ
   - —Å–æ—Å—Ç–æ—è–Ω–∏—è_—Å–æ–∑–Ω–∞–Ω–∏—è
4. Use Claude/GPT with specific prompts for each category
5. Return structured JSON with patterns

Implementation:
```

**–§–∞–π–ª: `text_processor/extractors/neurostalking_pattern_extractor.py`**

```python
"""
–≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–µ–π—Ä–æ-—Å—Ç–∞–ª–∫–∏–Ω–≥–∞ –°–∞–ª–∞–º–∞—Ç–∞ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞.

–ò–∑–≤–ª–µ–∫–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —É—á–µ–Ω–∏—è –∏–∑ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
"""

import json
import logging
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
from pathlib import Path

from text_processor.validators.terminology_validator import (
    TerminologyValidator,
    ValidationResult
)

logger = logging.getLogger(__name__)


@dataclass
class NeurostalkingPattern:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –Ω–µ–π—Ä–æ-—Å—Ç–∞–ª–∫–∏–Ω–≥–∞"""
    pattern_category: str  # —Ç—Ä–∏–∞–¥–∞_—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Ä–∞–±–æ—Ç–∞_—Å_–≤–Ω–∏–º–∞–Ω–∏–µ–º, –∏ —Ç.–¥.
    pattern_name: str
    description: str
    key_terms: List[str]
    typical_context: str
    recognition_markers: List[str]
    related_practices: List[str]
    source_quote: str
    confidence: float  # 0.0 - 1.0


class NeurostalkingPatternExtractor:
    """
    –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —É—á–µ–Ω–∏—è –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞.
    
    –ù–∞—Ö–æ–¥–∏—Ç –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã —á–µ—Ç—ã—Ä–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π:
    1. –¢—Ä–∏–∞–¥–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ (–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ ‚Üí –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ ‚Üí —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è)
    2. –†–∞–±–æ—Ç–∞ —Å –≤–Ω–∏–º–∞–Ω–∏–µ–º (–∑–∞—Ö–≤–∞—Ç, –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ, —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–æ–ª—è)
    3. –†–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ (–æ—Ç–¥–µ–ª–µ–Ω–∏–µ –æ—Ç –Ø-–æ–±—Ä–∞–∑–∞)
    4. –°–æ—Å—Ç–æ—è–Ω–∏—è —Å–æ–∑–Ω–∞–Ω–∏—è (—á–∏—Å—Ç–æ–µ –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ)
    """
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏
    PATTERN_CATEGORIES = {
        "—Ç—Ä–∏–∞–¥–∞_—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏": {
            "key_terms": ["–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ", "–æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ", "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è", "–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ"],
            "description": "–ü—Ä–æ—Ü–µ—Å—Å: –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ ‚Üí –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ ‚Üí —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è"
        },
        "—Ä–∞–±–æ—Ç–∞_—Å_–≤–Ω–∏–º–∞–Ω–∏–µ–º": {
            "key_terms": ["–ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è", "—Å–≤–æ–±–æ–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ", "–∑–∞—Ö–≤–∞—Ç –≤–Ω–∏–º–∞–Ω–∏—è", 
                         "—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–æ–ª—è", "–ø–æ–ª–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è"],
            "description": "–ü—Ä–æ—Ü–µ—Å—Å—ã —Ä–∞–±–æ—Ç—ã —Å –ø–æ–ª–µ–º –≤–Ω–∏–º–∞–Ω–∏—è"
        },
        "—Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ": {
            "key_terms": ["—Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ", "–Ø-–æ–±—Ä–∞–∑", "–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è", 
                         "–Ω–∞–±–ª—é–¥–∞—é—â–µ–µ —Å–æ–∑–Ω–∞–Ω–∏–µ", "–ª–æ–∂–Ω–∞—è —Å–∞–º–æ—Å—Ç—å"],
            "description": "–ü—Ä–æ—Ü–µ—Å—Å—ã –æ—Ç–¥–µ–ª–µ–Ω–∏—è –æ—Ç –ª–æ–∂–Ω–æ–π —Å–∞–º–æ—Å—Ç–∏"
        },
        "—Å–æ—Å—Ç–æ—è–Ω–∏—è_—Å–æ–∑–Ω–∞–Ω–∏—è": {
            "key_terms": ["—á–∏—Å—Ç–æ–µ –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ", "–ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ", "–∂–∏–≤–æ–µ –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ", 
                         "–∑–¥–µ—Å—å-–∏-—Å–µ–π—á–∞—Å", "–ø—Ä–æ–±—É–∂–¥–µ–Ω–∏–µ", "–ø—Ä–æ—è—Å–Ω–µ–Ω–∏–µ"],
            "description": "–°–æ—Å—Ç–æ—è–Ω–∏—è —á–∏—Å—Ç–æ–≥–æ –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏—è"
        }
    }
    
    def __init__(
        self,
        terminology_validator: Optional[TerminologyValidator] = None,
        use_llm: bool = False,
        llm_client = None
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞.
        
        Args:
            terminology_validator: –í–∞–ª–∏–¥–∞—Ç–æ—Ä —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏ (—Å–æ–∑–¥–∞–µ—Ç—Å—è –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω)
            use_llm: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (–∏–Ω–∞—á–µ rule-based)
            llm_client: –ö–ª–∏–µ–Ω—Ç LLM (OpenAI, Anthropic –∏ —Ç.–¥.)
        """
        self.validator = terminology_validator or TerminologyValidator()
        self.use_llm = use_llm
        self.llm_client = llm_client
        
        logger.info(f"Initialized NeurostalkingPatternExtractor (LLM: {use_llm})")
    
    def extract(
        self, 
        text: str,
        min_density: float = 0.25,
        categories: Optional[List[str]] = None
    ) -> Dict:
        """
        –ò–∑–≤–ª–µ—á—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–µ–π—Ä–æ-—Å—Ç–∞–ª–∫–∏–Ω–≥–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞.
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            min_density: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω–æ–≤ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞
            categories: –°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (None = –≤—Å–µ)
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
        """
        
        # –®–∞–≥ 1: –í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        validation = self.validator.validate_text(text, min_density=min_density)
        
        if not validation.is_valid:
            logger.warning(f"Text validation failed: {validation.reason}")
            return {
                "valid": False,
                "reason": validation.reason,
                "patterns": []
            }
        
        logger.info(f"Text validated successfully (density: {validation.metrics['density']:.1%})")
        
        # –®–∞–≥ 2: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        relevant_categories = self._identify_relevant_categories(
            validation.sarsekenov_entities,
            categories
        )
        
        if not relevant_categories:
            logger.info("No relevant pattern categories found")
            return {
                "valid": True,
                "patterns": [],
                "metrics": validation.metrics
            }
        
        # –®–∞–≥ 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        if self.use_llm and self.llm_client:
            patterns = self._extract_with_llm(text, relevant_categories, validation)
        else:
            patterns = self._extract_rule_based(text, relevant_categories, validation)
        
        return {
            "valid": True,
            "patterns": [asdict(p) for p in patterns],
            "metrics": validation.metrics,
            "categories_found": list(set(p.pattern_category for p in patterns))
        }
    
    def _identify_relevant_categories(
        self,
        entities: List[str],
        requested_categories: Optional[List[str]] = None
    ) -> List[str]:
        """
        –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π.
        
        Args:
            entities: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞
            requested_categories: –ó–∞–ø—Ä–æ—à–µ–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (None = –≤—Å–µ)
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        """
        relevant = []
        entities_lower = [e.lower() for e in entities]
        
        for category, info in self.PATTERN_CATEGORIES.items():
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            if requested_categories and category not in requested_categories:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            key_terms_lower = [t.lower() for t in info['key_terms']]
            
            if any(term in entities_lower for term in key_terms_lower):
                relevant.append(category)
                logger.debug(f"Category '{category}' is relevant")
        
        return relevant
    
    def _extract_rule_based(
        self,
        text: str,
        categories: List[str],
        validation: ValidationResult
    ) -> List[NeurostalkingPattern]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª (–±–µ–∑ LLM).
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.
        """
        patterns = []
        
        for category in categories:
            category_info = self.PATTERN_CATEGORIES[category]
            
            # –ü–æ–∏—Å–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            sentences = self._split_into_sentences(text)
            
            for sentence in sentences:
                sentence_lower = sentence.lower()
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏
                matching_terms = [
                    term for term in category_info['key_terms']
                    if term.lower() in sentence_lower
                ]
                
                if len(matching_terms) >= 2:  # –ú–∏–Ω–∏–º—É–º 2 —Ç–µ—Ä–º–∏–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
                    pattern = self._create_pattern_from_sentence(
                        sentence,
                        category,
                        matching_terms,
                        validation.sarsekenov_entities
                    )
                    
                    if pattern:
                        patterns.append(pattern)
        
        return patterns
    
    def _create_pattern_from_sentence(
        self,
        sentence: str,
        category: str,
        matching_terms: List[str],
        all_entities: List[str]
    ) -> Optional[NeurostalkingPattern]:
        """
        –°–æ–∑–¥–∞—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–≤—Ä–∏—Å—Ç–∏–∫.
        """
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –°–∞—Ä—Å–µ–∫–µ–Ω–æ–≤–∞ –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentence_entities = [
            e for e in all_entities
            if e.lower() in sentence.lower()
        ]
        
        if len(sentence_entities) < 2:
            return None
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∏—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        typical_contexts = {
            "—Ç—Ä–∏–∞–¥–∞_—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏": "–í –ø—Ä–æ—Ü–µ—Å—Å–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏—è",
            "—Ä–∞–±–æ—Ç–∞_—Å_–≤–Ω–∏–º–∞–Ω–∏–µ–º": "–ü—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –ø–æ–ª–µ–º –≤–Ω–∏–º–∞–Ω–∏—è",
            "—Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ": "–í –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏—è —Å –Ø-–æ–±—Ä–∞–∑–æ–º",
            "—Å–æ—Å—Ç–æ—è–Ω–∏—è_—Å–æ–∑–Ω–∞–Ω–∏—è": "–í —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —á–∏—Å—Ç–æ–≥–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏—è"
        }
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞—Ä–∫–µ—Ä–æ–≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
        recognition_markers = self._extract_recognition_markers(sentence, category)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫
        related_practices = self._identify_related_practices(sentence_entities)
        
        pattern = NeurostalkingPattern(
            pattern_category=category,
            pattern_name=self._generate_pattern_name(sentence, matching_terms),
            description=sentence.strip(),
            key_terms=sentence_entities,
            typical_context=typical_contexts.get(category, "–í –ø—Ä–∞–∫—Ç–∏–∫–µ –Ω–µ–π—Ä–æ-—Å—Ç–∞–ª–∫–∏–Ω–≥–∞"),
            recognition_markers=recognition_markers,
            related_practices=related_practices,
            source_quote=sentence.strip(),
            confidence=self._calculate_confidence(sentence_entities, matching_terms)
        )
        
        return pattern
    
    def _generate_pattern_name(self, sentence: str, key_terms: List[str]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–∞"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        if len(key_terms) >= 2:
            return f"{key_terms[0]} –∏ {key_terms[1]}"
        elif key_terms:
            return key_terms[0]
        else:
            return "–ü–∞—Ç—Ç–µ—Ä–Ω –Ω–µ–π—Ä–æ-—Å—Ç–∞–ª–∫–∏–Ω–≥–∞"
    
    def _extract_recognition_markers(self, sentence: str, category: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–∞—Ä–∫–µ—Ä–æ–≤ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–∞"""
        markers = []
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        marker_keywords = {
            "—Ç—Ä–∏–∞–¥–∞_—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏": ["–Ω–∞–±–ª—é–¥–∞—Ç—å", "–æ—Å–æ–∑–Ω–∞–≤–∞—Ç—å", "–∑–∞–º–µ—á–∞—Ç—å", "–≤–∏–¥–µ—Ç—å"],
            "—Ä–∞–±–æ—Ç–∞_—Å_–≤–Ω–∏–º–∞–Ω–∏–µ–º": ["–≤–Ω–∏–º–∞–Ω–∏–µ", "–ø–æ–ª–µ", "—Ä–∞—Å—à–∏—Ä—è–µ—Ç—Å—è", "—Å—É–∂–∞–µ—Ç—Å—è"],
            "—Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ": ["–æ—Ç–¥–µ–ª–µ–Ω–∏–µ", "–¥–∏—Å—Ç–∞–Ω—Ü–∏—è", "–Ω–∞–±–ª—é–¥–∞—Ç–µ–ª—å"],
            "—Å–æ—Å—Ç–æ—è–Ω–∏—è_—Å–æ–∑–Ω–∞–Ω–∏—è": ["–ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ", "—è—Å–Ω–æ—Å—Ç—å", "–ø—Ä–æ–±—É–∂–¥–µ–Ω–∏–µ"]
        }
        
        sentence_lower = sentence.lower()
        category_keywords = marker_keywords.get(category, [])
        
        for keyword in category_keywords:
            if keyword in sentence_lower:
                markers.append(f"–ü—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç '{keyword}'")
        
        return markers if markers else ["–ü—Ä—è–º–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞"]
    
    def _identify_related_practices(self, entities: List[str]) -> List[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫"""
        practices = []
        
        practice_terms = {
            "–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ", "—Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ", "—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ", 
            "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –æ–ø—ã—Ç–∞", "—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–∏"
        }
        
        for entity in entities:
            if entity.lower() in [p.lower() for p in practice_terms]:
                practices.append(entity)
        
        return practices if practices else ["–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ"]
    
    def _calculate_confidence(
        self,
        sentence_entities: List[str],
        matching_terms: List[str]
    ) -> float:
        """
        –†–∞—Å—Å—á–∏—Ç–∞—Ç—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –ø–∞—Ç—Ç–µ—Ä–Ω–µ.
        
        –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏ –∏—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏.
        """
        # –ë–∞–∑–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–µ—Ä–º–∏–Ω–æ–≤
        base_confidence = min(len(sentence_entities) * 0.15, 0.7)
        
        # –ë–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        category_bonus = min(len(matching_terms) * 0.1, 0.3)
        
        return min(base_confidence + category_bonus, 1.0)
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """–†–∞–∑–¥–µ–ª–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        import re
        
        # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–æ—á–∫–∞–º, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º –∑–Ω–∞–∫–∞–º
        sentences = re.split(r'[.!?]+', text)
        
        # –û—á–∏—Å—Ç–∫–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        sentences = [s.strip() for s in sentences if s.strip()]
        
        return sentences
    
    def _extract_with_llm(
        self,
        text: str,
        categories: List[str],
        validation: ValidationResult
    ) -> List[NeurostalkingPattern]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM.
        
        TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LLM –∫–ª–∏–µ–Ω—Ç–∞
        """
        logger.warning("LLM extraction not yet implemented, falling back to rule-based")
        return self._extract_rule_based(text, categories, validation)


# Utility —Ñ—É–Ω–∫—Ü–∏–∏

def extract_patterns(
    text: str,
    validator: Optional[TerminologyValidator] = None
) -> Dict:
    """
    Utility —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.
    
    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        validator: –í–∞–ª–∏–¥–∞—Ç–æ—Ä (—Å–æ–∑–¥–∞–µ—Ç—Å—è –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω)
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
    """
    extractor = NeurostalkingPatternExtractor(terminology_validator=validator)
    return extractor.extract(text)
```


***

### –®–∞–≥ 2.2: –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤

**–§–∞–π–ª: `tests/extractors/test_neurostalking_pattern_extractor.py`**

```python
"""
–¢–µ—Å—Ç—ã –¥–ª—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–µ–π—Ä–æ-—Å—Ç–∞–ª–∫–∏–Ω–≥–∞
"""

import pytest
from text_processor.extractors.neurostalking_pattern_extractor import (
    NeurostalkingPatternExtractor,
    NeurostalkingPattern,
    extract_patterns
)
from text_processor.validators.terminology_validator import TerminologyValidator


@pytest.fixture
def extractor():
    """Fixture –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞"""
    return NeurostalkingPatternExtractor()


def test_extract_triada_pattern(extractor):
    """–¢–µ—Å—Ç: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ —Ç—Ä–∏–∞–¥—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    
    text = """
    –ö–æ–≥–¥–∞ –ò—â—É—â–∏–π –ø—Ä–∞–∫—Ç–∏–∫—É–µ—Ç –º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ, –æ–Ω —Å–Ω–∞—á–∞–ª–∞ –Ω–∞–±–ª—é–¥–∞–µ—Ç –∑–∞ 
    –º—ã—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º –ø–æ—Ç–æ–∫–æ–º. –ó–∞—Ç–µ–º –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–º–æ–≤ –ø—Å–∏—Ö–∏–∫–∏.
    –≠—Ç–æ –≤–µ–¥–µ—Ç –∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ —Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ —Å –Ø-–æ–±—Ä–∞–∑–æ–º.
    """
    
    result = extractor.extract(text)
    
    assert result['valid'] == True
    assert len(result['patterns']) > 0
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –Ω–∞–π–¥–µ–Ω –ø–∞—Ç—Ç–µ—Ä–Ω –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç—Ä–∏–∞–¥–∞_—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    triada_patterns = [
        p for p in result['patterns']
        if p['pattern_category'] == '—Ç—Ä–∏–∞–¥–∞_—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏'
    ]
    
    assert len(triada_patterns) > 0
    
    pattern = triada_patterns[0]
    assert '–º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ' in [t.lower() for t in pattern['key_terms']]
    
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω –ø–∞—Ç—Ç–µ—Ä–Ω —Ç—Ä–∏–∞–¥—ã: {pattern['pattern_name']}")
    print(f"   –ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã: {pattern['key_terms']}")


def test_extract_attention_field_pattern(extractor):
    """–¢–µ—Å—Ç: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ —Ä–∞–±–æ—Ç—ã —Å –≤–Ω–∏–º–∞–Ω–∏–µ–º"""
    
    text = """
    –ü–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è –∑–∞—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç—Å—è –Ø-–æ–±—Ä–∞–∑–æ–º, –∫–æ–≥–¥–∞ –ò—â—É—â–∏–π –ø–æ–≥—Ä—É–∂–∞–µ—Ç—Å—è 
    –≤–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –¥–∏–∞–ª–æ–≥. –°–≤–æ–±–æ–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —Ç–µ—Ä—è–µ—Ç—Å—è, –ø–æ–ª–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è 
    —Å—É–∂–∞–µ—Ç—Å—è. –ß–µ—Ä–µ–∑ –º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–æ–ª—è –≤–Ω–∏–º–∞–Ω–∏—è.
    """
    
    result = extractor.extract(text)
    
    assert result['valid'] == True
    
    attention_patterns = [
        p for p in result['patterns']
        if p['pattern_category'] == '—Ä–∞–±–æ—Ç–∞_—Å_–≤–Ω–∏–º–∞–Ω–∏–µ–º'
    ]
    
    assert len(attention_patterns) > 0
    
    pattern = attention_patterns[0]
    attention_terms = ['–ø–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è', '—Å–≤–æ–±–æ–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ', '–∑–∞—Ö–≤–∞—Ç –≤–Ω–∏–º–∞–Ω–∏—è']
    
    found_attention_terms = [
        term for term in attention_terms
        if any(term.lower() in kt.lower() for kt in pattern['key_terms'])
    ]
    
    assert len(found_attention_terms) > 0
    
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω –ø–∞—Ç—Ç–µ—Ä–Ω –≤–Ω–∏–º–∞–Ω–∏—è: {pattern['pattern_name']}")


def test_extract_disidentification_pattern(extractor):
    """–¢–µ—Å—Ç: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ —Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏—è"""
    
    text = """
    –†–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ —Å –Ø-–æ–±—Ä–∞–∑–æ–º –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ –Ω–∞–±–ª—é–¥–∞—é—â–µ–µ —Å–æ–∑–Ω–∞–Ω–∏–µ.
    –ò—â—É—â–∏–π –∑–∞–º–µ—á–∞–µ—Ç –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é —Å –º—ã—Å–ª—è–º–∏ –∏ –Ω–∞—á–∏–Ω–∞–µ—Ç –≤–∏–¥–µ—Ç—å –ª–æ–∂–Ω—É—é —Å–∞–º–æ—Å—Ç—å.
    –í–æ–∑–Ω–∏–∫–∞–µ—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –º–µ–∂–¥—É –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª–µ–º –∏ –Ω–∞–±–ª—é–¥–∞–µ–º—ã–º.
    """
    
    result = extractor.extract(text)
    
    assert result['valid'] == True
    
    disid_patterns = [
        p for p in result['patterns']
        if p['pattern_category'] == '—Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ'
    ]
    
    assert len(disid_patterns) > 0
    
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω –ø–∞—Ç—Ç–µ—Ä–Ω —Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏—è")
    print(f"   –û–ø–∏—Å–∞–Ω–∏–µ: {disid_patterns[0]['description'][:100]}...")


def test_extract_consciousness_state_pattern(extractor):
    """–¢–µ—Å—Ç: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–æ–∑–Ω–∞–Ω–∏—è"""
    
    text = """
    –í —á–∏—Å—Ç–æ–º –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ –ò—â—É—â–∏–π –ø—Ä–µ–±—ã–≤–∞–µ—Ç –≤ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–∏ –∑–¥–µ—Å—å-–∏-—Å–µ–π—á–∞—Å.
    –ñ–∏–≤–æ–µ –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –º–æ–º–µ–Ω—Ç–∞ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –ø—Ä–æ–±—É–∂–¥–µ–Ω–∏–µ —Å–æ–∑–Ω–∞–Ω–∏—è.
    –í–æ–∑–Ω–∏–∫–∞–µ—Ç —è—Å–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ—è—Å–Ω–µ–Ω–∏–µ –ø—Ä–∏—Ä–æ–¥—ã —Å–æ–∑–Ω–∞–Ω–∏—è.
    """
    
    result = extractor.extract(text)
    
    assert result['valid'] == True
    
    state_patterns = [
        p for p in result['patterns']
        if p['pattern_category'] == '—Å–æ—Å—Ç–æ—è–Ω–∏—è_—Å–æ–∑–Ω–∞–Ω–∏—è'
    ]
    
    assert len(state_patterns) > 0
    
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω –ø–∞—Ç—Ç–µ—Ä–Ω —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–æ–∑–Ω–∞–Ω–∏—è")


def test_invalid_text_rejected(extractor):
    """–¢–µ—Å—Ç: –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç–∫–ª–æ–Ω—è–µ—Ç—Å—è"""
    
    text = """
    –ö–ª–∏–µ–Ω—Ç –∏—Å–ø—ã—Ç—ã–≤–∞–µ—Ç —Å—Ç—Ä–µ—Å—Å –∏ —Ç—Ä–µ–≤–æ–≥—É. –ï–≥–æ —ç–≥–æ –∑–∞—â–∏—â–∞–µ—Ç—Å—è.
    –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–µ–¥–∏—Ç–∞—Ü–∏—è –∏ —Ä–∞–±–æ—Ç–∞ —Å –ø–æ–¥—Å–æ–∑–Ω–∞–Ω–∏–µ–º.
    """
    
    result = extractor.extract(text)
    
    assert result['valid'] == False
    assert len(result['patterns']) == 0
    
    print(f"‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω: {result['reason']}")


def test_low_density_text_rejected(extractor):
    """–¢–µ—Å—Ç: —Ç–µ–∫—Å—Ç —Å –Ω–∏–∑–∫–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç—å—é –æ—Ç–∫–ª–æ–Ω—è–µ—Ç—Å—è"""
    
    text = """
    –ß–µ–ª–æ–≤–µ–∫ –ø—Ä–∏—à–µ–ª –Ω–∞ –≤—Å—Ç—Ä–µ—á—É –∏ —Ä–∞—Å—Å–∫–∞–∑–∞–ª –æ —Å–≤–æ–∏—Ö –ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏—è—Ö.
    –û–Ω —á—É–≤—Å—Ç–≤—É–µ—Ç –¥–∏—Å–∫–æ–º—Ñ–æ—Ä—Ç –∏ –∏—â–µ—Ç —Ä–µ—à–µ–Ω–∏–µ —Å–≤–æ–∏—Ö –ø—Ä–æ–±–ª–µ–º.
    –ú–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –º–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ.
    """
    
    result = extractor.extract(text, min_density=0.25)
    
    assert result['valid'] == False
    
    print(f"‚ùå –ù–∏–∑–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å: {result['reason']}")


def test_specific_category_extraction(extractor):
    """–¢–µ—Å—Ç: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
    
    text = """
    –ú–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ –Ø-–æ–±—Ä–∞–∑–æ–º –≤–µ–¥–µ—Ç –∫ —Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏—é.
    –ü–æ–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è —Ä–∞—Å—à–∏—Ä—è–µ—Ç—Å—è, –≤–æ–∑–Ω–∏–∫–∞–µ—Ç —Å–≤–æ–±–æ–¥–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ.
    –ß–∏—Å—Ç–æ–µ –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ.
    """
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Ä–∞–±–æ—Ç—ã —Å –≤–Ω–∏–º–∞–Ω–∏–µ–º
    result = extractor.extract(text, categories=['—Ä–∞–±–æ—Ç–∞_—Å_–≤–Ω–∏–º–∞–Ω–∏–µ–º'])
    
    assert result['valid'] == True
    
    # –í—Å–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ç–æ–ª—å–∫–æ —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    for pattern in result['patterns']:
        assert pattern['pattern_category'] == '—Ä–∞–±–æ—Ç–∞_—Å_–≤–Ω–∏–º–∞–Ω–∏–µ–º'
    
    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è '—Ä–∞–±–æ—Ç–∞_—Å_–≤–Ω–∏–º–∞–Ω–∏–µ–º'")


def test_confidence_calculation(extractor):
    """–¢–µ—Å—Ç: —Ä–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –ø–∞—Ç—Ç–µ—Ä–Ω–µ"""
    
    text = """
    –ú–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ –Ø-–æ–±—Ä–∞–∑–æ–º —á–µ—Ä–µ–∑ –Ω–∞–±–ª—é–¥–∞—é—â–µ–µ —Å–æ–∑–Ω–∞–Ω–∏–µ –≤–µ–¥–µ—Ç –∫ 
    —Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏—é, —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é –ø–æ–ª—è –≤–Ω–∏–º–∞–Ω–∏—è –∏ —á–∏—Å—Ç–æ–º—É –æ—Å–æ–∑–Ω–∞–≤–∞–Ω–∏—é.
    """
    
    result = extractor.extract(text)
    
    assert result['valid'] == True
    assert len(result['patterns']) > 0
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–µ—Ä–º–∏–Ω–æ–≤ –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –≤—ã—Å–æ–∫—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
    high_confidence_patterns = [
        p for p in result['patterns']
        if p['confidence'] > 0.5
    ]
    
    assert len(high_confidence_patterns) > 0
    
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(high_confidence_patterns)} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é")
    for p in high_confidence_patterns:
        print(f"   {p['pattern_name']}: {p['confidence']:.2f}")


def test_related_practices_identified(extractor):
    """–¢–µ—Å—Ç: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫"""
    
    text = """
    –ú–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∏ —Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞—é—Ç —Å–æ–≤–º–µ—Å—Ç–Ω–æ.
    –¶–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–∏ —É—Å–∏–ª–∏–≤–∞–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –æ–ø—ã—Ç–∞.
    """
    
    result = extractor.extract(text)
    
    assert result['valid'] == True
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã
    for pattern in result['patterns']:
        assert len(pattern['related_practices']) > 0
    
    print(f"‚úÖ –°–≤—è–∑–∞–Ω–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã")


def test_utility_function():
    """–¢–µ—Å—Ç: utility —Ñ—É–Ω–∫—Ü–∏—è extract_patterns"""
    
    text = """
    –ú–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ –Ø-–æ–±—Ä–∞–∑–æ–º –≤–µ–¥–µ—Ç –∫ —Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏—é 
    –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é –ø–æ–ª—è –≤–Ω–∏–º–∞–Ω–∏—è.
    """
    
    result = extract_patterns(text)
    
    assert result['valid'] == True
    assert len(result['patterns']) > 0
    
    print(f"‚úÖ Utility —Ñ—É–Ω–∫—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç")


def test_pattern_structure(extractor):
    """–¢–µ—Å—Ç: —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞"""
    
    text = """
    –ú–µ—Ç–∞–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ –Ø-–æ–±—Ä–∞–∑–æ–º –≤–µ–¥–µ—Ç –∫ —Ä–∞–∑–æ—Ç–æ–∂–¥–µ—Å—Ç–≤–ª–µ–Ω–∏—é.
    """
    
    result = extractor.extract(text)
    
    assert len(result['patterns']) > 0
    
    pattern = result['patterns'][0]
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
    required_fields = [
        'pattern_category',
        'pattern_name',
        'description',
        'key_terms',
        'typical_context',
        'recognition_markers',
        'related_practices',
        'source_quote',
        'confidence'
    ]
    
    for field in required_fields:
        assert field in pattern, f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ: {field}"
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤
    assert isinstance(pattern['key_terms'], list)
    assert isinstance(pattern['recognition_markers'], list)
    assert isinstance(pattern['related_practices'], list)
    assert isinstance(pattern['confidence'], (int, float))
    assert 0 <= pattern['confidence'] <= 1
    
    print(f"‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞")


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –≤—Ä—É—á–Ω—É—é
    extractor = NeurostalkingPatternExtractor()
    
    print("=" * 60)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï NEUROSTALKING PATTERN EXTRACTOR")
    print("=" * 60)
    print()
    
    test_extract_triada_pattern(extractor)
    print()
    test_extract_attention_field_pattern(extractor)
    print()
    test_extract_disidentification_pattern(extractor)
    print()
    test_extract_consciousness_state_pattern(extractor)
    print()
    test_invalid_text_rejected(extractor)
    print()
    test_confidence_calculation(extractor)
    print()
    test_pattern_structure(extractor)
```


***

## üìù –ö–û–ú–ê–ù–î–´ –î–õ–Ø CURSOR AI

**–°–∫–æ–ø–∏—Ä—É–π—Ç–µ —ç—Ç–æ –≤ Cursor:**

```
STEP 2: Create NeurostalkingPatternExtractor

Tasks:
1. Create text_processor/extractors/neurostalking_pattern_extractor.py
   - Copy the code provided above EXACTLY
   - Ensure it imports TerminologyValidator correctly

2. Create tests/extractors/test_neurostalking_pattern_extractor.py
   - Copy the test code provided above EXACTLY

3. Run tests:
   pytest tests/extractors/test_neurostalking_pattern_extractor.py -v

4. Show test results

Do NOT proceed to next extractor until these tests pass.
```


***

## ‚ö†Ô∏è –û–ñ–ò–î–ê–ï–ú–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´

–ü–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≠—Ç–∞–ø–∞ 2 –≤—ã –¥–æ–ª–∂–Ω—ã —É–≤–∏–¥–µ—Ç—å:

```
‚úÖ test_extract_triada_pattern PASSED
‚úÖ test_extract_attention_field_pattern PASSED
‚úÖ test_extract_disidentification_pattern PASSED
‚úÖ test_extract_consciousness_state_pattern PASSED
‚úÖ test_invalid_text_rejected PASSED
‚úÖ test_low_density_text_rejected PASSED
‚úÖ test_specific_category_extraction PASSED
‚úÖ test_confidence_calculation PASSED
‚úÖ test_related_practices_identified PASSED
‚úÖ test_utility_function PASSED
‚úÖ test_pattern_structure PASSED
```


***

–ó–∞–ø—É—Å—Ç–∏—Ç–µ –≠—Ç–∞–ø 2 –∏ –ø–æ–∫–∞–∂–∏—Ç–µ –º–Ω–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤! üöÄ

