
# üéØ –ü–æ–ª–Ω—ã–π –≥–∞–π–¥ –ø–æ –¥–æ—Ä–∞–±–æ—Ç–∫–µ —Å–∏—Å—Ç–µ–º—ã –¥–ª—è IDE AI Cursor

## üìã –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ

1. [–§–∞–∑–∞ 1: –£–ª—É—á—à–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)](#%D1%84%D0%B0%D0%B7%D0%B0-1)
2. [–§–∞–∑–∞ 2: –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –ø—Ä–∞–∫—Ç–∏–∫](#%D1%84%D0%B0%D0%B7%D0%B0-2)
3. [–§–∞–∑–∞ 3: –ú–æ–¥—É–ª—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (Safety)](#%D1%84%D0%B0%D0%B7%D0%B0-3)
4. [–§–∞–∑–∞ 4: –ò–µ—Ä–∞—Ä—Ö–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤](#%D1%84%D0%B0%D0%B7%D0%B0-4)
5. [–§–∞–∑–∞ 5: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è](#%D1%84%D0%B0%D0%B7%D0%B0-5)

***

```
# <a id="—Ñ–∞–∑–∞-1"></a>–§–ê–ó–ê 1: –£–ª—É—á—à–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
```


## –ó–∞–¥–∞—á–∞ –¥–ª—è Cursor AI

```markdown
# –ó–ê–î–ê–ß–ê 1.1: –î–æ–±–∞–≤–∏—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ —Å–≤—è–∑–µ–π –≤ –≥—Ä–∞—Ñ–µ –∑–Ω–∞–Ω–∏–π

## –ö–æ–Ω—Ç–µ–∫—Å—Ç
–°–µ–π—á–∞—Å –≤—Å–µ —Å–≤—è–∑–∏ –≤ –≥—Ä–∞—Ñ–µ –∏–º–µ—é—Ç strength=1.0, —á—Ç–æ –Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ.
–ù—É–∂–Ω–æ –≤—ã—á–∏—Å–ª—è—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ:
- Co-occurrence frequency (—á–∞—Å—Ç–æ—Ç–∞ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏)
- –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –∫–æ–Ω—Ü–µ–ø—Ç–∞–º–∏ –≤ —Ç–µ–∫—Å—Ç–µ
- PMI (Pointwise Mutual Information)

## –§–∞–π–ª –¥–ª—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
`orchestrator/knowledge_graph_builder.py`

## –ß—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å

### –®–∞–≥ 1: –°–æ–∑–¥–∞—Ç—å –∫–ª–∞—Å—Å GraphWeightCalculator
–î–æ–±–∞–≤—å –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å –≤ –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞ `knowledge_graph_builder.py`:

```

class GraphWeightCalculator:
"""
–í—ã—á–∏—Å–ª—è–µ—Ç –≤–µ—Å–∞ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –∫–æ–Ω—Ü–µ–ø—Ç–∞–º–∏ –≤ –≥—Ä–∞—Ñ–µ –∑–Ω–∞–Ω–∏–π.
"""

    def __init__(self):
        self.concept_positions = {}  # {concept: [positions_in_text]}
        self.cooccurrence_matrix = {}  # {(concept1, concept2): count}
        
    def analyze_block(self, block_content: str, entities: List[str], block_idx: int):
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–ª–æ–∫ —Ç–µ–∫—Å—Ç–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–∑–∏—Ü–∏–∏ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤.
        
        Args:
            block_content: –¢–µ–∫—Å—Ç –±–ª–æ–∫–∞
            entities: –°–ø–∏—Å–æ–∫ —Å—É—â–Ω–æ—Å—Ç–µ–π (–∫–æ–Ω—Ü–µ–ø—Ç–æ–≤) –≤ –±–ª–æ–∫–µ
            block_idx: –ò–Ω–¥–µ–∫—Å –±–ª–æ–∫–∞
        """
        words = block_content.lower().split()
        
        # –ù–∞–π—Ç–∏ –ø–æ–∑–∏—Ü–∏–∏ –∫–∞–∂–¥–æ–≥–æ –∫–æ–Ω—Ü–µ–ø—Ç–∞
        for entity in entities:
            entity_lower = entity.lower()
            positions = []
            
            for i, word in enumerate(words):
                if entity_lower in word:
                    positions.append((block_idx, i))
            
            if entity not in self.concept_positions:
                self.concept_positions[entity] = []
            self.concept_positions[entity].extend(positions)
        
        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å co-occurrence
        for i, entity1 in enumerate(entities):
            for entity2 in entities[i+1:]:
                pair = tuple(sorted([entity1, entity2]))
                self.cooccurrence_matrix[pair] = self.cooccurrence_matrix.get(pair, 0) + 1
    
    def calculate_pmi(self, entity1: str, entity2: str, total_blocks: int) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç Pointwise Mutual Information –º–µ–∂–¥—É –¥–≤—É–º—è –∫–æ–Ω—Ü–µ–ø—Ç–∞–º–∏.
        
        Args:
            entity1: –ü–µ—Ä–≤—ã–π –∫–æ–Ω—Ü–µ–ø—Ç
            entity2: –í—Ç–æ—Ä–æ–π –∫–æ–Ω—Ü–µ–ø—Ç
            total_blocks: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–æ–∫–æ–≤
            
        Returns:
            PMI score (0.0 - 1.0)
        """
        import math
        
        pair = tuple(sorted([entity1, entity2]))
        cooccur = self.cooccurrence_matrix.get(pair, 0)
        
        if cooccur == 0:
            return 0.0
        
        count1 = len([p for p in self.concept_positions.get(entity1, []) if p < total_blocks])
        count2 = len([p for p in self.concept_positions.get(entity2, []) if p < total_blocks])
        
        if count1 == 0 or count2 == 0:
            return 0.0
        
        p_xy = cooccur / total_blocks
        p_x = count1 / total_blocks
        p_y = count2 / total_blocks
        
        if p_x * p_y == 0:
            return 0.0
        
        pmi = math.log2(p_xy / (p_x * p_y))
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç 0 –¥–æ 1
        normalized_pmi = max(0, min(1, (pmi + 10) / 20))
        
        return normalized_pmi
    
    def calculate_distance_weight(self, entity1: str, entity2: str) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –≤–µ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –∫–æ–Ω—Ü–µ–ø—Ç–∞–º–∏.
        –ß–µ–º –±–ª–∏–∂–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ —Ç–µ–∫—Å—Ç–µ, —Ç–µ–º –≤—ã—à–µ –≤–µ—Å.
        
        Args:
            entity1: –ü–µ—Ä–≤—ã–π –∫–æ–Ω—Ü–µ–ø—Ç
            entity2: –í—Ç–æ—Ä–æ–π –∫–æ–Ω—Ü–µ–ø—Ç
            
        Returns:
            Weight (0.0 - 1.0)
        """
        positions1 = self.concept_positions.get(entity1, [])
        positions2 = self.concept_positions.get(entity2, [])
        
        if not positions1 or not positions2:
            return 0.0
        
        min_distances = []
        for pos1 in positions1:
            for pos2 in positions2:
                # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤ –æ–¥–Ω–æ–º –±–ª–æ–∫–µ
                if pos1 == pos2:
                    distance = abs(pos1 - pos2)[^1]
                    min_distances.append(distance)
        
        if not min_distances:
            return 0.3  # –ë–∞–∑–æ–≤—ã–π –≤–µ—Å –¥–ª—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –±–ª–æ–∫–æ–≤
        
        avg_distance = sum(min_distances) / len(min_distances)
        # –ß–µ–º –º–µ–Ω—å—à–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ, —Ç–µ–º –≤—ã—à–µ –≤–µ—Å
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ
        import math
        weight = math.exp(-avg_distance / 50)  # 50 —Å–ª–æ–≤ - —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω–∞—è –¥–ª–∏–Ω–∞
        
        return weight
    
    def calculate_combined_weight(self, entity1: str, entity2: str, total_blocks: int) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –≤–µ—Å —Å–≤—è–∑–∏ –∫–∞–∫ –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –º–µ—Ç—Ä–∏–∫.
        
        Args:
            entity1: –ü–µ—Ä–≤—ã–π –∫–æ–Ω—Ü–µ–ø—Ç
            entity2: –í—Ç–æ—Ä–æ–π –∫–æ–Ω—Ü–µ–ø—Ç
            total_blocks: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–æ–∫–æ–≤
            
        Returns:
            Combined weight (0.0 - 1.0)
        """
        pair = tuple(sorted([entity1, entity2]))
        cooccur_count = self.cooccurrence_matrix.get(pair, 0)
        
        if cooccur_count == 0:
            return 0.1  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤–µ—Å –¥–ª—è —Å–≤—è–∑–µ–π –±–µ–∑ co-occurrence
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ co-occurrence
        max_cooccur = max(self.cooccurrence_matrix.values()) if self.cooccurrence_matrix else 1
        freq_weight = cooccur_count / max_cooccur
        
        # PMI
        pmi_weight = self.calculate_pmi(entity1, entity2, total_blocks)
        
        # Distance weight
        dist_weight = self.calculate_distance_weight(entity1, entity2)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ—Å (–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞)
        combined = (
            0.4 * freq_weight +  # 40% - —á–∞—Å—Ç–æ—Ç–∞
            0.3 * pmi_weight +   # 30% - PMI
            0.3 * dist_weight    # 30% - —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
        )
        
        return round(combined, 3)
    ```

### –®–∞–≥ 2: –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –≤ KnowledgeGraphBuilder

–ù–∞–π–¥–∏ –º–µ—Ç–æ–¥ `_build_graph_entities` –≤ –∫–ª–∞—Å—Å–µ `KnowledgeGraphBuilder` –∏ –¥–æ–±–∞–≤—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ `GraphWeightCalculator`:

```

def _build_graph_entities(self, blocks: List[dict]) -> dict:
"""
–°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π –∏–∑ –±–ª–æ–∫–æ–≤.
"""
\# ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...

    # –î–û–ë–ê–í–ò–¢–¨ –≠–¢–û:
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –≤–µ—Å–æ–≤
    weight_calculator = GraphWeightCalculator()
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –±–ª–æ–∫–∏ –¥–ª—è —Å–±–æ—Ä–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    for idx, block in enumerate(blocks):
        entities = block.get('graph_entities', [])
        content = block.get('content', '')
        weight_calculator.analyze_block(content, entities, idx)
    
    total_blocks = len(blocks)
    
    # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è —É–∑–ª–æ–≤ ...
    
    # –ú–û–î–ò–§–ò–¶–ò–†–û–í–ê–¢–¨ —Å–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π:
    # –í–º–µ—Å—Ç–æ strength=1.0 –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ –≤–µ—Å–∞
    
    for link_type in ['conceptual_links', 'causal_links', 'practical_links']:
        for link in combined_relationships.get(link_type, []):
            source = link['source']
            target = link['target']
            
            # –ó–ê–ú–ï–ù–ò–¢–¨ –≠–¢–£ –°–¢–†–û–ö–£:
            # strength = link.get('strength', 1.0)
            
            # –ù–ê –≠–¢–û:
            strength = weight_calculator.calculate_combined_weight(
                source, target, total_blocks
            )
            
            # ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è edge ...
    ```

### –®–∞–≥ 3: –û–±–Ω–æ–≤–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≥—Ä–∞—Ñ–∞

–î–æ–±–∞–≤—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ –≤–µ—Å–∞—Ö –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:

```


# –í –∫–æ–Ω—Ü–µ –º–µ—Ç–æ–¥–∞ _build_graph_entities –¥–æ–±–∞–≤–∏—Ç—å:

edge_weights = [edge['confidence'] for edge in knowledge_graph['edges']]

knowledge_graph['metadata']['weight_statistics'] = {
'min_weight': min(edge_weights) if edge_weights else 0,
'max_weight': max(edge_weights) if edge_weights else 0,
'avg_weight': sum(edge_weights) / len(edge_weights) if edge_weights else 0,
'median_weight': sorted(edge_weights)[len(edge_weights)//2] if edge_weights else 0
}

```

## –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞
- ‚úÖ –í–µ—Å–∞ —Å–≤—è–∑–µ–π –≤–∞—Ä—å–∏—Ä—É—é—Ç—Å—è –æ—Ç 0.1 –¥–æ 1.0
- ‚úÖ –ß–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è –≤–º–µ—Å—Ç–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã –∏–º–µ—é—Ç –≤–µ—Å > 0.7
- ‚úÖ –†–µ–¥–∫–æ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã –∏–º–µ—é—Ç –≤–µ—Å < 0.3
- ‚úÖ –í –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≥—Ä–∞—Ñ–∞ –µ—Å—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–µ—Å–æ–≤

## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
–ó–∞–ø—É—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ –∏ –ø—Ä–æ–≤–µ—Ä—å:
```

python main.py --video_id 9BEpGP7L1_Q

```

–ü—Ä–æ–≤–µ—Ä—å –≤ —Ñ–∞–π–ª–µ `for_vector.json`:
- –ü–æ–ª–µ `knowledge_graph.edges[*].confidence` –¥–æ–ª–∂–Ω–æ –∏–º–µ—Ç—å —Ä–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
- –ü–æ–ª–µ `knowledge_graph.metadata.weight_statistics` –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∑–∞–ø–æ–ª–Ω–µ–Ω–æ
```


***

```
# <a id="—Ñ–∞–∑–∞-2"></a>–§–ê–ó–ê 2: –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –ø—Ä–∞–∫—Ç–∏–∫
```


## –ó–∞–¥–∞—á–∞ –¥–ª—è Cursor AI

```markdown
# –ó–ê–î–ê–ß–ê 2.1: –°–æ–∑–¥–∞—Ç—å —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π

## –ö–æ–Ω—Ç–µ–∫—Å—Ç
–ù—É–∂–Ω–æ –∏–∑–≤–ª–µ–∫–∞—Ç—å –∏–∑ —Ç–µ–∫—Å—Ç–∞ –ª–µ–∫—Ü–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ —Å –ø–æ—à–∞–≥–æ–≤—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏.

## –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π —Ñ–∞–π–ª
`orchestrator/extractors/practice_extractor.py`

## –ö–æ–¥

```

"""
–≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞ –ª–µ–∫—Ü–∏–π.
"""

import re
from typing import List, Dict, Optional
from dataclasses import dataclass

@dataclass
class PracticeStep:
"""–û–¥–∏–Ω —à–∞–≥ –ø—Ä–∞–∫—Ç–∏–∫–∏."""
step_number: int
instruction: str
duration: Optional[str] = None
notes: Optional[str] = None

@dataclass
class Practice:
"""–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ."""
title: str
description: str
steps: List[PracticeStep]
goal: str
prerequisites: List[str]
duration: Optional[str] = None
difficulty: str  \# beginner, intermediate, advanced
related_concepts: List[str]
when_to_use: List[str]
contraindications: List[str]

    def to_dict(self) -> dict:
        return {
            'title': self.title,
            'description': self.description,
            'steps': [
                {
                    'step_number': step.step_number,
                    'instruction': step.instruction,
                    'duration': step.duration,
                    'notes': step.notes
                }
                for step in self.steps
            ],
            'goal': self.goal,
            'prerequisites': self.prerequisites,
            'duration': self.duration,
            'difficulty': self.difficulty,
            'related_concepts': self.related_concepts,
            'when_to_use': self.when_to_use,
            'contraindications': self.contraindications
        }
    class PracticeExtractor:
"""
–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞ –ª–µ–∫—Ü–∏–π.
"""

    # –ú–∞—Ä–∫–µ—Ä—ã –Ω–∞—á–∞–ª–∞ –ø—Ä–∞–∫—Ç–∏–∫–∏
    PRACTICE_MARKERS = [
        r'–ø—Ä–∞–∫—Ç–∏–∫[–∞–∏]',
        r'—É–ø—Ä–∞–∂–Ω–µ–Ω–∏[–µ—è]',
        r'—Ç–µ—Ö–Ω–∏–∫[–∞–∏]',
        r'–º–µ—Ç–æ–¥',
        r'—Å–ø–æ—Å–æ–±',
        r'–¥–µ–ª–∞–µ–º —Ç–∞–∫',
        r'—Å–¥–µ–ª–∞–π—Ç–µ',
        r'–ø–æ–ø—Ä–æ–±—É–π—Ç–µ',
        r'–¥–∞–≤–∞–π—Ç–µ',
    ]
    
    # –ú–∞—Ä–∫–µ—Ä—ã —à–∞–≥–æ–≤
    STEP_MARKERS = [
        r'–ø–µ—Ä–≤–æ–µ',
        r'–≤—Ç–æ—Ä–æ–µ',
        r'—Ç—Ä–µ—Ç—å–µ',
        r'–¥–∞–ª–µ–µ',
        r'–∑–∞—Ç–µ–º',
        r'–ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ',
        r'–≤–æ-–ø–µ—Ä–≤—ã—Ö',
        r'–≤–æ-–≤—Ç–æ—Ä—ã—Ö',
        r'–≤-—Ç—Ä–µ—Ç—å–∏—Ö',
        r'—à–∞–≥ \d+',
        r'\d+\.',
    ]
    
    # –ò–º–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ —Ñ–æ—Ä–º—ã –≥–ª–∞–≥–æ–ª–æ–≤
    IMPERATIVE_PATTERNS = [
        r'\b(—Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Ç–µ—Å—å|–æ–±—Ä–∞—Ç–∏—Ç–µ|–∑–∞–º–µ—Ç—å—Ç–µ|–ø–æ—á—É–≤—Å—Ç–≤—É–π—Ç–µ|–æ—Å–æ–∑–Ω–∞–π—Ç–µ)',
        r'\b(–æ—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ—Å—å|–∑–∞–º—Ä–∏—Ç–µ|—Ä–∞—Å—Å–ª–∞–±—å—Ç–µ—Å—å|–¥—ã—à–∏—Ç–µ)',
        r'\b(–ø—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ|–≤–æ–æ–±—Ä–∞–∑–∏—Ç–µ|–≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ)',
        r'\b(–Ω–∞–±–ª—é–¥–∞–π—Ç–µ|—Å–ª–µ–¥–∏—Ç–µ|–æ—Ç—Å–ª–µ–∂–∏–≤–∞–π—Ç–µ)',
    ]
    
    def __init__(self):
        self.practice_pattern = re.compile(
            '|'.join(self.PRACTICE_MARKERS),
            re.IGNORECASE
        )
        self.step_pattern = re.compile(
            '|'.join(self.STEP_MARKERS),
            re.IGNORECASE
        )
        self.imperative_pattern = re.compile(
            '|'.join(self.IMPERATIVE_PATTERNS),
            re.IGNORECASE
        )
    
    def extract_practices(self, blocks: List[dict]) -> List[Practice]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏–∑ —Å–ø–∏—Å–∫–∞ –±–ª–æ–∫–æ–≤.
        
        Args:
            blocks: –°–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫
        """
        practices = []
        
        for block in blocks:
            content = block.get('content', '')
            
            # –ü–æ–∏—Å–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫
            if self._is_practice_block(content):
                practice = self._extract_practice_from_block(block)
                if practice:
                    practices.append(practice)
        
        return practices
    
    def _is_practice_block(self, content: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –±–ª–æ–∫ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ.
        """
        # –ï—Å—Ç—å –º–∞—Ä–∫–µ—Ä –ø—Ä–∞–∫—Ç–∏–∫–∏
        has_marker = bool(self.practice_pattern.search(content))
        
        # –ï—Å—Ç—å –∏–º–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ —Ñ–æ—Ä–º—ã
        has_imperatives = bool(self.imperative_pattern.search(content))
        
        # –ï—Å—Ç—å –º–∞—Ä–∫–µ—Ä—ã —à–∞–≥–æ–≤
        has_steps = bool(self.step_pattern.search(content))
        
        return has_marker and (has_imperatives or has_steps)
    
    def _extract_practice_from_block(self, block: dict) -> Optional[Practice]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ø—Ä–∞–∫—Ç–∏–∫—É –∏–∑ –±–ª–æ–∫–∞.
        """
        content = block.get('content', '')
        
        # –†–∞–∑–±–∏—Ç—å –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = self._split_sentences(content)
        
        # –ù–∞–π—Ç–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ (–ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –º–∞—Ä–∫–µ—Ä–æ–º)
        title = self._extract_title(sentences)
        if not title:
            title = "–ü—Ä–∞–∫—Ç–∏–∫–∞ –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
        
        # –ò–∑–≤–ª–µ—á—å –æ–ø–∏—Å–∞–Ω–∏–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ—Å–ª–µ –Ω–∞–∑–≤–∞–Ω–∏—è)
        description = self._extract_description(sentences)
        
        # –ò–∑–≤–ª–µ—á—å —à–∞–≥–∏
        steps = self._extract_steps(sentences)
        
        if len(steps) < 2:
            # –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —à–∞–≥–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–∏
            return None
        
        # –ò–∑–≤–ª–µ—á—å —Ü–µ–ª—å (–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å "—á—Ç–æ–±—ã", "–¥–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã")
        goal = self._extract_goal(sentences)
        
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        difficulty = self._estimate_difficulty(block, steps)
        
        # –°–≤—è–∑–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã
        related_concepts = block.get('graph_entities', [])
        
        practice = Practice(
            title=title,
            description=description,
            steps=steps,
            goal=goal,
            prerequisites=[],  # –ó–∞–ø–æ–ª–Ω–∏–º –ø–æ–∑–∂–µ
            duration=None,  # –ü–æ–ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å
            difficulty=difficulty,
            related_concepts=related_concepts,
            when_to_use=[],  # –ó–∞–ø–æ–ª–Ω–∏–º –ø–æ–∑–∂–µ
            contraindications=[]  # –ó–∞–ø–æ–ª–Ω–∏–º –ø–æ–∑–∂–µ
        )
        
        return practice
    
    def _split_sentences(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞
        sentences = re.split(r'[.!?]\s+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def _extract_title(self, sentences: List[str]) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏."""
        for sentence in sentences[:5]:  # –ò—â–µ–º –≤ –ø–µ—Ä–≤—ã—Ö 5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö
            if self.practice_pattern.search(sentence):
                # –ë–µ—Ä—ë–º –¥–æ –ø–µ—Ä–≤–æ–≥–æ –∑–Ω–∞–∫–∞ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏–ª–∏ –¥–æ –∫–æ–Ω—Ü–∞
                title = sentence.split(',').split('.')
                return title[:100]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        return ""
    
    def _extract_description(self, sentences: List[str]) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏."""
        # –ë–µ—Ä—ë–º 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ—Å–ª–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        description_sentences = []
        found_title = False
        
        for sentence in sentences:
            if self.practice_pattern.search(sentence):
                found_title = True
                continue
            
            if found_title:
                description_sentences.append(sentence)
                if len(description_sentences) >= 2:
                    break
        
        return ' '.join(description_sentences)
    
    def _extract_steps(self, sentences: List[str]) -> List[PracticeStep]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏."""
        steps = []
        step_number = 1
        
        for sentence in sentences:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —ç—Ç–æ —à–∞–≥?
            is_step = (
                self.step_pattern.search(sentence) or
                self.imperative_pattern.search(sentence)
            )
            
            if is_step and len(sentence) > 20:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —à–∞–≥–∞
                step = PracticeStep(
                    step_number=step_number,
                    instruction=sentence,
                    duration=None,
                    notes=None
                )
                steps.append(step)
                step_number += 1
        
        return steps
    
    def _extract_goal(self, sentences: List[str]) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ü–µ–ª—å –ø—Ä–∞–∫—Ç–∏–∫–∏."""
        goal_markers = ['—á—Ç–æ–±—ã', '–¥–ª—è —Ç–æ–≥–æ', '—Ü–µ–ª—å', '–∑–∞–¥–∞—á–∞']
        
        for sentence in sentences:
            for marker in goal_markers:
                if marker in sentence.lower():
                    return sentence
        
        return "–¶–µ–ª—å –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
    
    def _estimate_difficulty(self, block: dict, steps: List[PracticeStep]) -> str:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–∞–∫—Ç–∏–∫–∏."""
        # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ:
        # 1. –ö–æ–ª–∏—á–µ—Å—Ç–≤–∞ —à–∞–≥–æ–≤
        # 2. –°–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
        # 3. –î–ª–∏–Ω—ã –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
        
        num_steps = len(steps)
        complexity_score = block.get('complexity_score', 5.0)
        
        if num_steps <= 3 and complexity_score < 5:
            return 'beginner'
        elif num_steps <= 5 and complexity_score < 7:
            return 'intermediate'
        else:
            return 'advanced'
    
# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ pipeline

def extract_practices_from_blocks(blocks: List[dict]) -> List[dict]:
"""
–û–±—ë—Ä—Ç–∫–∞ –¥–ª—è –≤—ã–∑–æ–≤–∞ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ pipeline.

    Args:
        blocks: –°–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        
    Returns:
        –°–ø–∏—Å–æ–∫ –ø—Ä–∞–∫—Ç–∏–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ dict
    """
    extractor = PracticeExtractor()
    practices = extractor.extract_practices(blocks)
    return [p.to_dict() for p in practices]
    ```

## –®–∞–≥ 2: –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –≤ KnowledgeGraphBuilder

–í —Ñ–∞–π–ª–µ `orchestrator/knowledge_graph_builder.py` –¥–æ–±–∞–≤—å:

```


# –í –Ω–∞—á–∞–ª–µ —Ñ–∞–π–ª–∞

from orchestrator.extractors.practice_extractor import extract_practices_from_blocks

# –í –º–µ—Ç–æ–¥–µ build_knowledge_graph –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–ª–æ–∫–æ–≤:

def build_knowledge_graph(self, document_data: dict) -> dict:
\# ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...

    # –î–û–ë–ê–í–ò–¢–¨ –ü–û–°–õ–ï –û–ë–†–ê–ë–û–¢–ö–ò –ë–õ–û–ö–û–í:
    # –ò–∑–≤–ª–µ—á—å –ø—Ä–∞–∫—Ç–∏–∫–∏
    practices = extract_practices_from_blocks(blocks)
    
    # –î–æ–±–∞–≤–∏—Ç—å –≤ document_data
    document_data['practices'] = practices
    
    # –û–±–Ω–æ–≤–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    document_data['document_metadata']['practices_count'] = len(practices)
    
    # ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ ...
    ```

## –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞
- ‚úÖ –§–∞–π–ª `for_vector.json` —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–µ–∫—Ü–∏—é `practices`
- ‚úÖ –ö–∞–∂–¥–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –∏–º–µ–µ—Ç title, steps, goal
- ‚úÖ –®–∞–≥–∏ –ø—Ä–æ–Ω—É–º–µ—Ä–æ–≤–∞–Ω—ã –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
- ‚úÖ –û–ø—Ä–µ–¥–µ–ª—ë–Ω —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏

## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
```

python -c "
from orchestrator.extractors.practice_extractor import extract_practices_from_blocks

test_block = {
'content': '''
–ü—Ä–∞–∫—Ç–∏–∫–∞ –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–≥–æ –¥—ã—Ö–∞–Ω–∏—è. –≠—Ç–æ –±–∞–∑–æ–≤–æ–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö.
–ü–µ—Ä–≤–æ–µ, —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Ç–µ—Å—å –Ω–∞ —Å–≤–æ—ë–º –¥—ã—Ö–∞–Ω–∏–∏. –ù–∞–±–ª—é–¥–∞–π—Ç–µ –∫–∞–∫ –≤–æ–∑–¥—É—Ö –≤—Ö–æ–¥–∏—Ç –∏ –≤—ã—Ö–æ–¥–∏—Ç.
–í—Ç–æ—Ä–æ–µ, –∑–∞–º–µ—Ç—å—Ç–µ –æ—â—É—â–µ–Ω–∏—è –≤ —Ç–µ–ª–µ –ø—Ä–∏ –≤–¥–æ—Ö–µ –∏ –≤—ã–¥–æ—Ö–µ.
–¢—Ä–µ—Ç—å–µ, –∫–æ–≥–¥–∞ –æ—Ç–≤–ª–µ–∫–∞–µ—Ç–µ—Å—å, –º—è–≥–∫–æ –≤–æ–∑–≤—Ä–∞—â–∞–π—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∫ –¥—ã—Ö–∞–Ω–∏—é.
–≠—Ç–æ –ø–æ–º–æ–∂–µ—Ç —Ä–∞–∑–≤–∏—Ç—å –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å –∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ.
''',
'graph_entities': ['–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å', '–¥—ã—Ö–∞–Ω–∏–µ', '–ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ'],
'complexity_score': 3.0
}

practices = extract_practices_from_blocks([test_block])
print(f'–ù–∞–π–¥–µ–Ω–æ –ø—Ä–∞–∫—Ç–∏–∫: {len(practices)}')
if practices:
print(f'–ù–∞–∑–≤–∞–Ω–∏–µ: {practices[\"title\"]}')
print(f'–®–∞–≥–æ–≤: {len(practices[\"steps\"])}')
print(f'–°–ª–æ–∂–Ω–æ—Å—Ç—å: {practices[\"difficulty\"]}')
"

```
```


***

```
# <a id="—Ñ–∞–∑–∞-3"></a>–§–ê–ó–ê 3: –ú–æ–¥—É–ª—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (Safety)
```


## –ó–∞–¥–∞—á–∞ –¥–ª—è Cursor AI

```markdown
# –ó–ê–î–ê–ß–ê 3.1: –°–æ–∑–¥–∞—Ç—å –º–æ–¥—É–ª—å Safety –¥–ª—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏

## –ö–æ–Ω—Ç–µ–∫—Å—Ç
–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û –¥–ª—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞!
–ù—É–∂–Ω–æ –∏–∑–≤–ª–µ–∫–∞—Ç—å –∏ –¥–æ–±–∞–≤–ª—è—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ:
- –ü—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è—Ö
- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è—Ö –º–µ—Ç–æ–¥–∞
- –°–∏—Ç—É–∞—Ü–∏—è—Ö –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É
- –ö—Ä–∞—Å–Ω—ã—Ö —Ñ–ª–∞–≥–∞—Ö

## –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π —Ñ–∞–π–ª
`orchestrator/extractors/safety_extractor.py`

## –ö–æ–¥

```

"""
–≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø—Ä–∞–∫—Ç–∏–∫.
"""

import re
from typing import List, Dict
from dataclasses import dataclass

@dataclass
class SafetyInfo:
"""–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø—Ä–∞–∫—Ç–∏–∫–∏."""
contraindications: List[str]  \# –ü—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è
limitations: List[str]  \# –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
when_to_stop: List[str]  \# –ö–æ–≥–¥–∞ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è
when_to_seek_help: List[str]  \# –ö–æ–≥–¥–∞ –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É
red_flags: List[str]  \# –ö—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏
notes: List[str]  \# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏

    def to_dict(self) -> dict:
        return {
            'contraindications': self.contraindications,
            'limitations': self.limitations,
            'when_to_stop': self.when_to_stop,
            'when_to_seek_professional_help': self.when_to_seek_help,
            'red_flags': self.red_flags,
            'notes': self.notes
        }
    class SafetyExtractor:
"""
–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞.
"""

    # –ú–∞—Ä–∫–µ—Ä—ã –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏–π
    CONTRAINDICATION_MARKERS = [
        r'–ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω',
        r'–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è',
        r'–Ω–µ–ª—å–∑—è',
        r'–∑–∞–ø—Ä–µ—â–µ–Ω–æ',
        r'–∏–∑–±–µ–≥–∞–π—Ç–µ',
        r'–Ω–µ —Å—Ç–æ–∏—Ç',
        r'–æ–ø–∞—Å–Ω–æ –¥–ª—è',
    ]
    
    # –ú–∞—Ä–∫–µ—Ä—ã –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
    LIMITATION_MARKERS = [
        r'–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏',
        r'—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏',
        r'–Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è',
        r'—Ç—Ä–µ–±—É–µ—Ç –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç–∏',
        r'—Å –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å—é',
    ]
    
    # –ú–∞—Ä–∫–µ—Ä—ã –∫–æ–≥–¥–∞ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è
    STOP_MARKERS = [
        r'–µ—Å–ª–∏ –ø–æ—è–≤–ª—è–µ—Ç—Å—è',
        r'–ø—Ä–∏ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–∏',
        r'–ø—Ä–µ–∫—Ä–∞—Ç–∏—Ç–µ –µ—Å–ª–∏',
        r'–æ—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ—Å—å –∫–æ–≥–¥–∞',
        r'–Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ –ø—Ä–µ–∫—Ä–∞—Ç–∏—Ç–µ',
    ]
    
    # –ú–∞—Ä–∫–µ—Ä—ã –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É
    HELP_MARKERS = [
        r'–æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫',
        r'–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è',
        r'—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç',
        r'–ø—Å–∏—Ö–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç',
        r'–≤—Ä–∞—á',
        r'–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –ø–æ–º–æ—â—å',
    ]
    
    # –ö—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏ (—Å–µ—Ä—å—ë–∑–Ω—ã–µ —Å–∏–º–ø—Ç–æ–º—ã)
    RED_FLAGS = [
        r'—Å—É–∏—Ü–∏–¥–∞–ª—å–Ω',
        r'—Å–∞–º–æ–ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏',
        r'–ø–∞–Ω–∏—á–µ—Å–∫–∏–µ –∞—Ç–∞–∫–∏',
        r'–≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏',
        r'–±—Ä–µ–¥',
        r'–ø–æ—Ç–µ—Ä—è —Å–æ–∑–Ω–∞–Ω–∏—è',
        r'—Å–µ—Ä–¥—Ü–µ–±–∏–µ–Ω–∏–µ',
        r'—É–¥—É—à—å–µ',
    ]
    
    def __init__(self):
        self.contraindication_pattern = re.compile(
            '|'.join(self.CONTRAINDICATION_MARKERS),
            re.IGNORECASE
        )
        self.limitation_pattern = re.compile(
            '|'.join(self.LIMITATION_MARKERS),
            re.IGNORECASE
        )
        self.stop_pattern = re.compile(
            '|'.join(self.STOP_MARKERS),
            re.IGNORECASE
        )
        self.help_pattern = re.compile(
            '|'.join(self.HELP_MARKERS),
            re.IGNORECASE
        )
        self.red_flag_pattern = re.compile(
            '|'.join(self.RED_FLAGS),
            re.IGNORECASE
        )
    
    def extract_safety_info(self, blocks: List[dict]) -> SafetyInfo:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏–∑ –±–ª–æ–∫–æ–≤.
        
        Args:
            blocks: –°–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            
        Returns:
            SafetyInfo —Å –∏–∑–≤–ª–µ—á—ë–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        contraindications = []
        limitations = []
        when_to_stop = []
        when_to_seek_help = []
        red_flags = []
        notes = []
        
        for block in blocks:
            content = block.get('content', '')
            sentences = self._split_sentences(content)
            
            for sentence in sentences:
                # –ü—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è
                if self.contraindication_pattern.search(sentence):
                    contraindications.append(sentence)
                
                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
                if self.limitation_pattern.search(sentence):
                    limitations.append(sentence)
                
                # –ö–æ–≥–¥–∞ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è
                if self.stop_pattern.search(sentence):
                    when_to_stop.append(sentence)
                
                # –ö–æ–≥–¥–∞ –æ–±—Ä–∞—â–∞—Ç—å—Å—è –∑–∞ –ø–æ–º–æ—â—å—é
                if self.help_pattern.search(sentence):
                    when_to_seek_help.append(sentence)
                
                # –ö—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏
                if self.red_flag_pattern.search(sentence):
                    red_flags.append(sentence)
        
        # –î–æ–±–∞–≤–∏—Ç—å –±–∞–∑–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        safety_info = SafetyInfo(
            contraindications=self._deduplicate(contraindications),
            limitations=self._deduplicate(limitations),
            when_to_stop=self._deduplicate(when_to_stop),
            when_to_seek_help=self._add_default_help_recommendations(
                self._deduplicate(when_to_seek_help)
            ),
            red_flags=self._deduplicate(red_flags),
            notes=self._add_general_notes()
        )
        
        return safety_info
    
    def _split_sentences(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è."""
        sentences = re.split(r'[.!?]\s+', text)
        return [s.strip() for s in sentences if len(s.strip()) > 20]
    
    def _deduplicate(self, items: List[str]) -> List[str]:
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã —Å —É—á—ë—Ç–æ–º —Å—Ö–æ–∂–µ—Å—Ç–∏."""
        seen = set()
        unique = []
        for item in items:
            normalized = item.lower().strip()
            if normalized not in seen:
                seen.add(normalized)
                unique.append(item)
        return unique
    
    def _add_default_help_recommendations(self, existing: List[str]) -> List[str]:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –±–∞–∑–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ–±—Ä–∞—â–µ–Ω–∏—è –∑–∞ –ø–æ–º–æ—â—å—é."""
        defaults = [
            "–ü—Ä–∏ —Å—É–∏—Ü–∏–¥–∞–ª—å–Ω—ã—Ö –º—ã—Å–ª—è—Ö –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –ø—Å–∏—Ö–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç—É –∏–ª–∏ –ø–æ–∑–≤–æ–Ω–∏—Ç–µ –Ω–∞ –≥–æ—Ä—è—á—É—é –ª–∏–Ω–∏—é",
            "–ï—Å–ª–∏ –ø—Ä–∞–∫—Ç–∏–∫–∞ –≤—ã–∑—ã–≤–∞–µ—Ç —Å–∏–ª—å–Ω—ã–π –¥–∏—Å–∫–æ–º—Ñ–æ—Ä—Ç –±–æ–ª–µ–µ 3 –¥–Ω–µ–π, –ø—Ä–æ–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É–π—Ç–µ—Å—å —Å–æ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–º",
            "–ü—Ä–∏ –æ–±–æ—Å—Ç—Ä–µ–Ω–∏–∏ –ø—Å–∏—Ö–∏—á–µ—Å–∫–∏—Ö —Å–∏–º–ø—Ç–æ–º–æ–≤ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –≤—Ä–∞—á—É-–ø—Å–∏—Ö–∏–∞—Ç—Ä—É",
        ]
        
        return existing + defaults
    
    def _add_general_notes(self) -> List[str]:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –æ–±—â–∏–µ –ø—Ä–∏–º–µ—á–∞–Ω–∏—è –æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏."""
        return [
            "–î–∞–Ω–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –Ω–µ –∑–∞–º–µ–Ω—è—é—Ç –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é –ø—Å–∏—Ö–æ—Ç–µ—Ä–∞–ø–∏—é",
            "–ü—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –ø—Å–∏—Ö–∏—á–µ—Å–∫–∏—Ö –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è –≤—Ä–∞—á–∞",
            "–ü—Ä–∞–∫—Ç–∏–∫–∏ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω—ã –¥–ª—è –∑–¥–æ—Ä–æ–≤—ã—Ö –ª—é–¥–µ–π, –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–∏—Ö—Å—è —Å–∞–º–æ—Ä–∞–∑–≤–∏—Ç–∏–µ–º",
            "–†–µ–≥—É–ª—è—Ä–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏ - –Ω–∞—á–∏–Ω–∞–π—Ç–µ —Å –º–∞–ª–æ–≥–æ",
        ]
    def extract_safety_from_blocks(blocks: List[dict]) -> dict:
"""
–û–±—ë—Ä—Ç–∫–∞ –¥–ª—è –≤—ã–∑–æ–≤–∞ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ pipeline.

    Args:
        blocks: –°–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        
    Returns:
        Safety –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ dict
    """
    extractor = SafetyExtractor()
    safety_info = extractor.extract_safety_info(blocks)
    return safety_info.to_dict()
    ```

## –®–∞–≥ 2: –î–æ–±–∞–≤–∏—Ç—å safety –≤ –∫–∞–∂–¥—ã–π –±–ª–æ–∫

–í —Ñ–∞–π–ª–µ `orchestrator/knowledge_graph_builder.py` –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–π –æ–±—Ä–∞–±–æ—Ç–∫—É –±–ª–æ–∫–æ–≤:

```

from orchestrator.extractors.safety_extractor import SafetyExtractor

def build_knowledge_graph(self, document_data: dict) -> dict:
\# ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å safety extractor
    safety_extractor = SafetyExtractor()
    
    # –ò–∑–≤–ª–µ—á—å –æ–±—â—É—é safety –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –≤—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    global_safety = safety_extractor.extract_safety_info(blocks)
    document_data['global_safety'] = global_safety.to_dict()
    
    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ –¥–æ–±–∞–≤–∏—Ç—å safety
    for block in blocks:
        # –ï—Å–ª–∏ –±–ª–æ–∫ –Ω–µ –∏–º–µ–µ—Ç safety, –¥–æ–±–∞–≤–∏—Ç—å –ø—É—Å—Ç–æ–π
        if 'safety' not in block or not any(block['safety'].values()):
            block['safety'] = {
                'contraindications': [],
                'limitations': [],
                'when_to_stop': [],
                'when_to_seek_professional_help': [],
                'notes': []
            }
    
    # ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ ...
    ```

## –®–∞–≥ 3: –û–±–æ–≥–∞—Ç–∏—Ç—å safety –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –ø—Ä–∞–∫—Ç–∏–∫

–í —Ñ–∞–π–ª–µ `orchestrator/extractors/practice_extractor.py` –¥–æ–±–∞–≤—å:

```

from orchestrator.extractors.safety_extractor import SafetyExtractor

class PracticeExtractor:
def __init__(self):
\# ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...
self.safety_extractor = SafetyExtractor()

    def _extract_practice_from_block(self, block: dict) -> Optional[Practice]:
        # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...
        
        # –î–û–ë–ê–í–ò–¢–¨ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ safety –¥–ª—è –ø—Ä–∞–∫—Ç–∏–∫–∏
        practice_sentences = self._split_sentences(content)
        practice_safety = self.safety_extractor.extract_safety_info([{
            'content': content
        }])
        
        practice = Practice(
            # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—è ...
            contraindications=practice_safety.contraindications
        )
        
        return practice
    ```

## –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞
- ‚úÖ –ö–∞–∂–¥–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –∏–º–µ–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ contraindications
- ‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –∏–º–µ–µ—Ç global_safety —Å –±–∞–∑–æ–≤—ã–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
- ‚úÖ –ö—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å) –∏–∑–≤–ª–µ—á–µ–Ω—ã
- ‚úÖ –ú–∏–Ω–∏–º—É–º 3 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ "–∫–æ–≥–¥–∞ –æ–±—Ä–∞—â–∞—Ç—å—Å—è –∑–∞ –ø–æ–º–æ—â—å—é"

## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
```

from orchestrator.extractors.safety_extractor import extract_safety_from_blocks

test_blocks = [{
'content': '''
–≠—Ç–∞ –ø—Ä–∞–∫—Ç–∏–∫–∞ –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ª—é–¥—è–º —Å –ø–∞–Ω–∏—á–µ—Å–∫–∏–º–∏ –∞—Ç–∞–∫–∞–º–∏.
–ï—Å–ª–∏ –ø–æ—è–≤–ª—è–µ—Ç—Å—è –≥–æ–ª–æ–≤–æ–∫—Ä—É–∂–µ–Ω–∏–µ, –ø—Ä–µ–∫—Ä–∞—Ç–∏—Ç–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ.
–ü—Ä–∏ —É—Å–∏–ª–µ–Ω–∏–∏ —Ç—Ä–µ–≤–æ–≥–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –ø—Å–∏—Ö–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç—É.
'''
}]

safety = extract_safety_from_blocks(test_blocks)
print(f"–ü—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è: {len(safety['contraindications'])}")
print(f"–ö–æ–≥–¥–∞ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è: {len(safety['when_to_stop'])}")
print(f"–ö–æ–≥–¥–∞ –∑–∞ –ø–æ–º–æ—â—å—é: {len(safety['when_to_seek_professional_help'])}")

```
```


***

```
# <a id="—Ñ–∞–∑–∞-4"></a>–§–ê–ó–ê 4: –ò–µ—Ä–∞—Ä—Ö–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –∏ Prerequisites
```


## –ó–∞–¥–∞—á–∞ –¥–ª—è Cursor AI

```markdown
# –ó–ê–î–ê–ß–ê 4.1: –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∏–µ—Ä–∞—Ä—Ö–∏—é –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤

## –ö–æ–Ω—Ç–µ–∫—Å—Ç
–ù—É–∂–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞–∫–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã –±–∞–∑–æ–≤—ã–µ, –∫–∞–∫–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ, –∏ –∫–∞–∫–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã —è–≤–ª—è—é—Ç—Å—è prerequisites –¥–ª—è –¥—Ä—É–≥–∏—Ö.

## –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π —Ñ–∞–π–ª
`orchestrator/extractors/hierarchy_builder.py`

## –ö–æ–¥

```

"""
–ü–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å –∏–µ—Ä–∞—Ä—Ö–∏–∏ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π.
"""

from typing import List, Dict, Set, Tuple
from collections import defaultdict
import networkx as nx

class ConceptHierarchyBuilder:
"""
–°—Ç—Ä–æ–∏—Ç –∏–µ—Ä–∞—Ä—Ö–∏—é –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Ä—è–¥–∫–∞ –∏—Ö –ø–æ—è–≤–ª–µ–Ω–∏—è
–∏ —á–∞—Å—Ç–æ—Ç—ã —É–ø–æ–º–∏–Ω–∞–Ω–∏–π.
"""

    def __init__(self):
        self.concept_first_appearance = {}  # {concept: block_index}
        self.concept_frequency = defaultdict(int)  # {concept: count}
        self.concept_cooccurrence = defaultdict(int)  # {(concept1, concept2): count}
        self.dependency_graph = nx.DiGraph()
    
    def build_hierarchy(self, blocks: List[dict]) -> dict:
        """
        –°—Ç—Ä–æ–∏—Ç –∏–µ—Ä–∞—Ä—Ö–∏—é –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤.
        
        Args:
            blocks: –°–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            
        Returns:
            Dict —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± –∏–µ—Ä–∞—Ä—Ö–∏–∏
        """
        # –ê–Ω–∞–ª–∏–∑ –±–ª–æ–∫–æ–≤
        self._analyze_blocks(blocks)
        
        # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        self._build_dependency_graph()
        
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —É—Ä–æ–≤–Ω–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        levels = self._assign_levels()
        
        # –ù–∞–π—Ç–∏ prerequisites
        prerequisites_map = self._find_prerequisites()
        
        # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        learning_sequence = self._generate_learning_sequence()
        
        return {
            'concept_levels': levels,
            'prerequisites': prerequisites_map,
            'learning_sequence': learning_sequence,
            'fundamental_concepts': self._get_fundamental_concepts(levels),
            'advanced_concepts': self._get_advanced_concepts(levels)
        }
    
    def _analyze_blocks(self, blocks: List[dict]):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–ª–æ–∫–∏ –¥–ª—è —Å–±–æ—Ä–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏."""
        for idx, block in enumerate(blocks):
            entities = block.get('graph_entities', [])
            
            for entity in entities:
                # –ü–µ—Ä–≤–æ–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ
                if entity not in self.concept_first_appearance:
                    self.concept_first_appearance[entity] = idx
                
                # –ß–∞—Å—Ç–æ—Ç–∞
                self.concept_frequency[entity] += 1
            
            # Co-occurrence
            for i, entity1 in enumerate(entities):
                for entity2 in entities[i+1:]:
                    pair = tuple(sorted([entity1, entity2]))
                    self.concept_cooccurrence[pair] += 1
    
    def _build_dependency_graph(self):
        """–°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –º–µ–∂–¥—É –∫–æ–Ω—Ü–µ–ø—Ç–∞–º–∏."""
        # –ü—Ä–∞–≤–∏–ª–æ: –µ—Å–ª–∏ –∫–æ–Ω—Ü–µ–ø—Ç A –ø–æ—è–≤–ª—è–µ—Ç—Å—è —Ä–∞–Ω—å—à–µ B –∏ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —Å –Ω–∏–º,
        # —Ç–æ A -> B (A —è–≤–ª—è–µ—Ç—Å—è prerequisite –¥–ª—è B)
        
        for (concept1, concept2), cooccur_count in self.concept_cooccurrence.items():
            if cooccur_count < 2:  # –°–ª–∏—à–∫–æ–º —Ä–µ–¥–∫–æ
                continue
            
            idx1 = self.concept_first_appearance[concept1]
            idx2 = self.concept_first_appearance[concept2]
            
            # –¢–æ—Ç, —á—Ç–æ –ø–æ—è–≤–∏–ª—Å—è —Ä–∞–Ω—å—à–µ, —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è prerequisite
            if idx1 < idx2:
                self.dependency_graph.add_edge(concept1, concept2, weight=cooccur_count)
            else:
                self.dependency_graph.add_edge(concept2, concept1, weight=cooccur_count)
    
    def _assign_levels(self) -> Dict[str, str]:
        """
        –ù–∞–∑–Ω–∞—á–∞–µ—Ç —É—Ä–æ–≤–Ω–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ü–µ–ø—Ç–∞–º.
        
        Returns:
            {concept: level} –≥–¥–µ level in ['fundamental', 'intermediate', 'advanced']
        """
        levels = {}
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ü–µ–ø—Ç—ã –ø–æ –ø–µ—Ä–≤–æ–º—É –ø–æ—è–≤–ª–µ–Ω–∏—é
        sorted_concepts = sorted(
            self.concept_first_appearance.items(),
            key=lambda x: x[^1]
        )
        
        total = len(sorted_concepts)
        
        for i, (concept, _) in enumerate(sorted_concepts):
            # –ü–µ—Ä–≤—ã–µ 30% - fundamental
            if i < total * 0.3:
                level = 'fundamental'
            # –°–ª–µ–¥—É—é—â–∏–µ 50% - intermediate
            elif i < total * 0.8:
                level = 'intermediate'
            # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 20% - advanced
            else:
                level = 'advanced'
            
            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–æ—Ç—ã
            freq = self.concept_frequency[concept]
            avg_freq = sum(self.concept_frequency.values()) / len(self.concept_frequency)
            
            # –ß–∞—Å—Ç–æ —É–ø–æ–º–∏–Ω–∞–µ–º—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã —Å–∫–æ—Ä–µ–µ fundamental
            if freq > avg_freq * 2:
                if level == 'advanced':
                    level = 'intermediate'
                elif level == 'intermediate' and i < total * 0.5:
                    level = 'fundamental'
            
            levels[concept] = level
        
        return levels
    
    def _find_prerequisites(self) -> Dict[str, List[str]]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç prerequisites –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–Ω—Ü–µ–ø—Ç–∞.
        
        Returns:
            {concept: [list of prerequisite concepts]}
        """
        prerequisites_map = {}
        
        for concept in self.dependency_graph.nodes():
            # –ù–∞–π—Ç–∏ –≤—Å–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã, –æ—Ç –∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–≤–∏—Å–∏—Ç –¥–∞–Ω–Ω—ã–π
            predecessors = list(self.dependency_graph.predecessors(concept))
            
            # –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ (–≤–µ—Å —Ä—ë–±–µ—Ä)
            weighted_prereqs = [
                (pred, self.dependency_graph[pred][concept]['weight'])
                for pred in predecessors
            ]
            weighted_prereqs.sort(key=lambda x: x, reverse=True)[^1]
            
            # –í–∑—è—Ç—å —Ç–æ–ø-3 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö prerequisites
            prerequisites_map[concept] = [pred for pred, _ in weighted_prereqs[:3]]
        
        return prerequisites_map
    
    def _generate_learning_sequence(self) -> List[List[str]]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑—É—á–µ–Ω–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤.
        
        Returns:
            –°–ø–∏—Å–æ–∫ —É—Ä–æ–≤–Ω–µ–π, –∫–∞–∂–¥—ã–π —É—Ä–æ–≤–µ–Ω—å - —Å–ø–∏—Å–æ–∫ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
        """
        try:
            # –¢–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –≥—Ä–∞—Ñ–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
            sorted_concepts = list(nx.topological_sort(self.dependency_graph))
            
            # –†–∞–∑–±–∏—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–∏ (–≥—Ä—É–ø–ø—ã –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –±–µ–∑ –≤–∑–∞–∏–º–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π)
            levels = []
            remaining = set(sorted_concepts)
            
            while remaining:
                # –ù–∞–π—Ç–∏ –∫–æ–Ω—Ü–µ–ø—Ç—ã –±–µ–∑ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è prerequisites
                current_level = []
                for concept in list(remaining):
                    prereqs = set(self.dependency_graph.predecessors(concept))
                    if not prereqs.intersection(remaining):
                        current_level.append(concept)
                
                if not current_level:
                    # –¶–∏–∫–ª –≤ –≥—Ä–∞—Ñ–µ - –¥–æ–±–∞–≤–∏—Ç—å –≤—Å–µ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è
                    current_level = list(remaining)
                
                levels.append(current_level)
                remaining -= set(current_level)
            
            return levels
            
        except nx.NetworkXError:
            # –ì—Ä–∞—Ñ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ü–∏–∫–ª—ã - fallback –Ω–∞ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫—É –ø–æ –ø–æ—è–≤–ª–µ–Ω–∏—é
            sorted_concepts = sorted(
                self.concept_first_appearance.items(),
                key=lambda x: x[^1]
            )
            # –†–∞–∑–±–∏—Ç—å –Ω–∞ –≥—Ä—É–ø–ø—ã –ø–æ 5
            concepts_only = [c for c, _ in sorted_concepts]
            return [concepts_only[i:i+5] for i in range(0, len(concepts_only), 5)]
    
    def _get_fundamental_concepts(self, levels: Dict[str, str]) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –±–∞–∑–æ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤."""
        return [c for c, level in levels.items() if level == 'fundamental']
    
    def _get_advanced_concepts(self, levels: Dict[str, str]) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤."""
        return [c for c, level in levels.items() if level == 'advanced']
    def build_concept_hierarchy(blocks: List[dict]) -> dict:
"""
–û–±—ë—Ä—Ç–∫–∞ –¥–ª—è –≤—ã–∑–æ–≤–∞ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ pipeline.

    Args:
        blocks: –°–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        
    Returns:
        –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏–µ—Ä–∞—Ä—Ö–∏–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ dict
    """
    builder = ConceptHierarchyBuilder()
    return builder.build_hierarchy(blocks)
    ```

## –®–∞–≥ 2: –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –≤ pipeline

–í `orchestrator/knowledge_graph_builder.py`:

```

from orchestrator.extractors.hierarchy_builder import build_concept_hierarchy

def build_knowledge_graph(self, document_data: dict) -> dict:
\# ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...

    # –î–û–ë–ê–í–ò–¢–¨ –ü–û–°–õ–ï –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–ª–æ–∫–æ–≤:
    
    # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∏–µ—Ä–∞—Ä—Ö–∏—é –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
    hierarchy = build_concept_hierarchy(blocks)
    document_data['concept_hierarchy'] = hierarchy
    
    # –û–±–Ω–æ–≤–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    document_data['document_metadata']['fundamental_concepts_count'] = len(
        hierarchy['fundamental_concepts']
    )
    document_data['document_metadata']['advanced_concepts_count'] = len(
        hierarchy['advanced_concepts']
    )
    
    # –î–æ–±–∞–≤–∏—Ç—å prerequisites –≤ –±–ª–æ–∫–∏
    for block in blocks:
        block_entities = block.get('graph_entities', [])
        block_prerequisites = []
        
        for entity in block_entities:
            prereqs = hierarchy['prerequisites'].get(entity, [])
            block_prerequisites.extend(prereqs)
        
        # –£–±—Ä–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã
        block['prerequisites']['prerequisites'] = list(set(block_prerequisites))
        block['prerequisites']['recommended_sequence'] = hierarchy['learning_sequence']
    
    # ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ ...
    ```

## –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞
- ‚úÖ –î–æ–∫—É–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç `concept_hierarchy`
- ‚úÖ –ö–æ–Ω—Ü–µ–ø—Ç—ã —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –Ω–∞ fundamental/intermediate/advanced
- ‚úÖ –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–Ω—Ü–µ–ø—Ç–∞ —É–∫–∞–∑–∞–Ω—ã prerequisites
- ‚úÖ –ï—Å—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑—É—á–µ–Ω–∏—è

## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
```

from orchestrator.extractors.hierarchy_builder import build_concept_hierarchy

test_blocks = [
{'graph_entities': ['–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å', '–≤–Ω–∏–º–∞–Ω–∏–µ', '–¥—ã—Ö–∞–Ω–∏–µ']},
{'graph_entities': ['–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å', '–º–µ–¥–∏—Ç–∞—Ü–∏—è', '–ø—Ä–∞–∫—Ç–∏–∫–∞']},
{'graph_entities': ['–º–µ–¥–∏—Ç–∞—Ü–∏—è', '—Å–∞–º–∞–¥—Ö–∏', '–ø—Ä–æ—Å–≤–µ—Ç–ª–µ–Ω–∏–µ']},
]

hierarchy = build_concept_hierarchy(test_blocks)
print(f"Fundamental: {hierarchy['fundamental_concepts']}")
print(f"Advanced: {hierarchy['advanced_concepts']}")
print(f"Learning sequence: {hierarchy['learning_sequence']}")

```
```


***

```
# <a id="—Ñ–∞–∑–∞-5"></a>–§–ê–ó–ê 5: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è
```


## –ó–∞–¥–∞—á–∞ –¥–ª—è Cursor AI

```markdown
# –ó–ê–î–ê–ß–ê 5.1: –°–æ–∑–¥–∞—Ç—å comprehensive —Ç–µ—Å—Ç—ã

## –°–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª —Ç–µ—Å—Ç–æ–≤
`tests/test_knowledge_graph_enhancements.py`

## –ö–æ–¥

```

"""
–¢–µ—Å—Ç—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏–π –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π.
"""

import pytest
import json
from pathlib import Path

class TestGraphWeights:
"""–¢–µ—Å—Ç—ã –≤–µ—Å–æ–≤ —Å–≤—è–∑–µ–π –≤ –≥—Ä–∞—Ñ–µ."""

    def test_weights_variance(self, processed_video_data):
        """–í–µ—Å–∞ –¥–æ–ª–∂–Ω—ã –≤–∞—Ä—å–∏—Ä–æ–≤–∞—Ç—å—Å—è."""
        kg = processed_video_data['knowledge_graph']
        weights = [edge['confidence'] for edge in kg['edges']]
        
        assert len(set(weights)) > 1, "–í—Å–µ –≤–µ—Å–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ!"
        assert min(weights) >= 0.1, f"–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤–µ—Å —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π: {min(weights)}"
        assert max(weights) <= 1.0, f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–µ—Å —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π: {max(weights)}"
    
    def test_weight_statistics(self, processed_video_data):
        """–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–µ—Å–æ–≤."""
        kg = processed_video_data['knowledge_graph']
        stats = kg['metadata'].get('weight_statistics')
        
        assert stats is not None, "–ù–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤–µ—Å–æ–≤!"
        assert 'min_weight' in stats
        assert 'max_weight' in stats
        assert 'avg_weight' in stats
        assert stats['avg_weight'] > 0
    class TestPracticeExtractor:
"""–¢–µ—Å—Ç—ã —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ –ø—Ä–∞–∫—Ç–∏–∫."""

    def test_practices_extracted(self, processed_video_data):
        """–ü—Ä–∞–∫—Ç–∏–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω—ã."""
        practices = processed_video_data.get('practices', [])
        
        # –û–∂–∏–¥–∞–µ–º —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –ø—Ä–∞–∫—Ç–∏–∫—É –≤ 20-–º–∏–Ω—É—Ç–Ω–æ–º –≤–∏–¥–µ–æ
        assert len(practices) > 0, "–ù–µ –∏–∑–≤–ª–µ—á–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–∏!"
    
    def test_practice_structure(self, processed_video_data):
        """–ö–∞–∂–¥–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –¥–æ–ª–∂–Ω–∞ –∏–º–µ—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É."""
        practices = processed_video_data.get('practices', [])
        
        for practice in practices:
            assert 'title' in practice
            assert 'steps' in practice
            assert 'goal' in practice
            assert 'difficulty' in practice
            
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —à–∞–≥–∏
            assert len(practice['steps']) >= 2, "–°–ª–∏—à–∫–æ–º –º–∞–ª–æ —à–∞–≥–æ–≤!"
            
            for step in practice['steps']:
                assert 'step_number' in step
                assert 'instruction' in step
                assert len(step['instruction']) > 10, "–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è!"
    
    def test_practice_difficulty(self, processed_video_data):
        """–°–ª–æ–∂–Ω–æ—Å—Ç—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ."""
        practices = processed_video_data.get('practices', [])
        
        valid_difficulties = ['beginner', 'intermediate', 'advanced']
        
        for practice in practices:
            assert practice['difficulty'] in valid_difficulties, \
                f"–ù–µ–≤–µ—Ä–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å: {practice['difficulty']}"
    class TestSafetyExtractor:
"""–¢–µ—Å—Ç—ã —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ safety –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."""

    def test_global_safety_exists(self, processed_video_data):
        """–î–æ–∫—É–º–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å global_safety."""
        assert 'global_safety' in processed_video_data, "–ù–µ—Ç global_safety!"
    
    def test_safety_structure(self, processed_video_data):
        """Safety –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–æ–ª—è."""
        safety = processed_video_data['global_safety']
        
        required_fields = [
            'contraindications',
            'limitations',
            'when_to_stop',
            'when_to_seek_professional_help',
            'notes'
        ]
        
        for field in required_fields:
            assert field in safety, f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ: {field}"
    
    def test_default_help_recommendations(self, processed_video_data):
        """–î–æ–ª–∂–Ω—ã –±—ã—Ç—å –±–∞–∑–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ–±—Ä–∞—â–µ–Ω–∏—è –∑–∞ –ø–æ–º–æ—â—å—é."""
        safety = processed_video_data['global_safety']
        help_recommendations = safety['when_to_seek_professional_help']
        
        assert len(help_recommendations) >= 3, \
            "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –æ–±—Ä–∞—â–µ–Ω–∏—è –∑–∞ –ø–æ–º–æ—â—å—é!"
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        help_text = ' '.join(help_recommendations).lower()
        assert '—Å—É–∏—Ü–∏–¥' in help_text or '–ø—Å–∏—Ö–æ—Ç–µ—Ä–∞–ø–µ–≤—Ç' in help_text
    class TestConceptHierarchy:
"""–¢–µ—Å—Ç—ã –∏–µ—Ä–∞—Ä—Ö–∏–∏ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤."""

    def test_hierarchy_exists(self, processed_video_data):
        """–î–æ–∫—É–º–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å concept_hierarchy."""
        assert 'concept_hierarchy' in processed_video_data, "–ù–µ—Ç concept_hierarchy!"
    
    def test_hierarchy_structure(self, processed_video_data):
        """–ò–µ—Ä–∞—Ä—Ö–∏—è –¥–æ–ª–∂–Ω–∞ –∏–º–µ—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É."""
        hierarchy = processed_video_data['concept_hierarchy']
        
        required_fields = [
            'concept_levels',
            'prerequisites',
            'learning_sequence',
            'fundamental_concepts',
            'advanced_concepts'
        ]
        
        for field in required_fields:
            assert field in hierarchy, f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ: {field}"
    
    def test_concept_levels(self, processed_video_data):
        """–ö–æ–Ω—Ü–µ–ø—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –ø–æ —É—Ä–æ–≤–Ω—è–º."""
        hierarchy = processed_video_data['concept_hierarchy']
        levels = hierarchy['concept_levels']
        
        valid_levels = ['fundamental', 'intermediate', 'advanced']
        
        assert len(levels) > 0, "–ù–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ —Å —É—Ä–æ–≤–Ω—è–º–∏!"
        
        for concept, level in levels.items():
            assert level in valid_levels, f"–ù–µ–≤–µ—Ä–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å {level} –¥–ª—è {concept}"
    
    def test_learning_sequence(self, processed_video_data):
        """–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑—É—á–µ–Ω–∏—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏—á–Ω–æ–π."""
        hierarchy = processed_video_data['concept_hierarchy']
        sequence = hierarchy['learning_sequence']
        
        assert len(sequence) > 0, "–ü—É—Å—Ç–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑—É—á–µ–Ω–∏—è!"
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ —ç—Ç–æ —Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤
        assert isinstance(sequence, list), "–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏!"
    class TestIntegration:
"""–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã."""

    def test_full_pipeline(self):
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ."""
        # –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ
        import subprocess
        result = subprocess.run(
            ['python', 'main.py', '--video_id', '9BEpGP7L1_Q'],
            capture_output=True,
            text=True
        )
        
        assert result.returncode == 0, f"Pipeline failed: {result.stderr}"
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω
        output_file = Path('data/sag_final/2025/05') / \
                     '2025-05-15_9BEpGP7L1_Q_–ö–∞–∫_–±—ã—Ç—å_–≤_–ü—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–∏.for_vector.json'
        
        assert output_file.exists(), "–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª –Ω–µ —Å–æ–∑–¥–∞–Ω!"
        
        # –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å
        with open(output_file) as f:
            data = json.load(f)
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Å–µ –Ω–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        assert 'practices' in data
        assert 'global_safety' in data
        assert 'concept_hierarchy' in data
        assert data['knowledge_graph']['metadata'].get('weight_statistics')
    
# –§–∏–∫—Å—Ç—É—Ä–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏

@pytest.fixture
def processed_video_data():
"""–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ."""
output_file = Path('data/sag_final/2025/05') / \
'2025-05-15_9BEpGP7L1_Q_–ö–∞–∫_–±—ã—Ç—å_–≤_–ü—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–∏.for_vector.json'

    if not output_file.exists():
        pytest.skip("–¢–µ—Å—Ç–æ–≤–æ–µ –≤–∏–¥–µ–æ –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ")
    
    with open(output_file) as f:
        return json.load(f)
    ```

## –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤

```


# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å pytest –µ—Å–ª–∏ –µ—â—ë –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω

pip install pytest

# –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã

pytest tests/test_knowledge_graph_enhancements.py -v

# –ò–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –≤—ã–≤–æ–¥–æ–º

pytest tests/test_knowledge_graph_enhancements.py -vv

```

## –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞ –≤—Å–µ—Ö —Ñ–∞–∑
- ‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç
- ‚úÖ –§–∞–π–ª for_vector.json —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –Ω–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
- ‚úÖ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –≤—ã—Ä–æ—Å (–±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö)
- ‚úÖ –ù–µ—Ç –æ—à–∏–±–æ–∫ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ

## –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞

```


# –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–µ—Å—Ç–æ–≤–æ–µ –≤–∏–¥–µ–æ

python main.py --video_id 9BEpGP7L1_Q

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –±–æ–ª—å—à–µ ~250KB)

ls -lh data/sag_final/2025/05/*.for_vector.json

# –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç—ã

pytest tests/test_knowledge_graph_enhancements.py

# –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ —Ñ–∞–π–ª–µ

python -c "
import json
with open('data/sag_final/2025/05/2025-05-15_9BEpGP7L1_Q_–ö–∞–∫_–±—ã—Ç—å_–≤_–ü—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–∏.for_vector.json') as f:
data = json.load(f)
print(f'–ü—Ä–∞–∫—Ç–∏–∫: {len(data.get(\"practices\", []))}')
print(f'Safety —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {len(data[\"global_safety\"][\"when_to_seek_professional_help\"])}')
print(f'Fundamental –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤: {len(data[\"concept_hierarchy\"][\"fundamental_concepts\"])}')
print(f'Learning —É—Ä–æ–≤–Ω–µ–π: {len(data[\"concept_hierarchy\"][\"learning_sequence\"])}')
print(f'–í–µ—Å —Å–≤—è–∑–µ–π: min={data[\"knowledge_graph\"][\"metadata\"][\"weight_statistics\"][\"min_weight\"]}, max={data[\"knowledge_graph\"][\"metadata\"][\"weight_statistics\"][\"max_weight\"]}')
"

```
```


***

## üéâ –§–∏–Ω–∞–ª—å–Ω—ã–π —á–µ–∫–ª–∏—Å—Ç

–ü–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á, —É –≤–∞—Å –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å:

- [x] ‚úÖ –í–µ—Å–∞ —Å–≤—è–∑–µ–π –≤ –≥—Ä–∞—Ñ–µ –≤–∞—Ä—å–∏—Ä—É—é—Ç—Å—è –æ—Ç 0.1 –¥–æ 1.0
- [x] ‚úÖ –ò–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è —Å —à–∞–≥–∞–º–∏
- [x] ‚úÖ –ö–∞–∂–¥–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –∏–º–µ–µ—Ç safety –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
- [x] ‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –∏–º–µ–µ—Ç global_safety —Å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
- [x] ‚úÖ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∞ –∏–µ—Ä–∞—Ä—Ö–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ (fundamental/intermediate/advanced)
- [x] ‚úÖ –î–ª—è –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã prerequisites
- [x] ‚úÖ –ï—Å—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑—É—á–µ–Ω–∏—è
- [x] ‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç

**–ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é –Ω–∞ 500 –≤–∏–¥–µ–æ: 85-90%!** üöÄ

<div align="center">‚ÅÇ</div>

[^1]: image.jpg

